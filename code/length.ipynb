{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fccdd2f-009b-403c-a55e-4cd573675633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 09:55:33.191035: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np, pandas as pd, torch\n",
    "import torch.utils.data as torchdata\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel, BertModel, AutoConfig\n",
    "\n",
    "from IsoScore import IsoScore\n",
    "from dadapy import Data\n",
    "from skdim.id import MLE, MOM, TLE, CorrInt, FisherS, lPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c33ee21-7b40-4aa9-b856-fac4c832c604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ corpus ready — 194,916 tokens across length classes ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
      "Sample sizes per length (raw cap):\n",
      "{'1': 29626, '10': 7492, '2': 29918, '3': 36097, '4': 33008, '5': 18892, '6': 14274, '7': 12188, '8': 7895, '9': 5526}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-base-uncased (embed subset): 100%|██| 10067/10067 [01:21<00:00, 123.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ embedded 194,916 tokens  • layers=13\n",
      "\n",
      "→ Computing metric: gride …\n",
      "  ✓ saved: CSV= tables_LENGTH/length_bootstrap/length_raw_gride_bert-base-uncased.csv  plot= results_LENGTH/length_raw_gride_bert-base-uncased.png\n",
      "\n",
      "✓ done (incremental outputs produced per metric).\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ============== Optional deps (gracefully skipped if not installed) ==============\n",
    "HAS_DADAPY = False\n",
    "try:\n",
    "    from dadapy import Data  # DADApy ID estimators (TwoNN, GRIDE)\n",
    "    HAS_DADAPY = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "HAS_SKDIM = False\n",
    "try:\n",
    "    from skdim.id import (\n",
    "        MOM, TLE, CorrInt, FisherS, lPCA,\n",
    "        MLE, DANCo, ESS, MiND_ML, MADA, KNN\n",
    "    )\n",
    "    HAS_SKDIM = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# IsoScore: library if available, else a simple monotone fallback\n",
    "try:\n",
    "    from isoscore import IsoScore\n",
    "    _HAS_ISOSCORE = True\n",
    "except Exception:\n",
    "    _HAS_ISOSCORE = False\n",
    "    class _IsoScoreFallback:\n",
    "        @staticmethod\n",
    "        def IsoScore(X: np.ndarray) -> float:\n",
    "            C = np.cov(X.T, ddof=0)\n",
    "            ev = np.linalg.eigvalsh(C)\n",
    "            if ev.mean() <= 0 or ev[-1] <= 0:\n",
    "                return 0.0\n",
    "            return float(np.clip(ev.mean() / (ev[-1] + 1e-9), 0.0, 1.0))\n",
    "    IsoScore = _IsoScoreFallback()\n",
    "\n",
    "# =============================== CONFIG ===============================\n",
    "CSV_PATH      = \"en_ewt-ud-train_sentences.csv\"\n",
    "LENGTH_COL    = \"length\"                # your per-sentence list[int] column\n",
    "\n",
    "# - BERT:  BASELINE=\"bert-base-uncased\", WORD_REP_MODE=\"first\" (or \"mean\"/\"last\")\n",
    "# - GPT-2: BASELINE=\"gpt2\",             WORD_REP_MODE=\"last\"  (or \"mean\")\n",
    "BASELINE      = \"bert-base-uncased\"\n",
    "WORD_REP_MODE = \"first\"\n",
    "\n",
    "RAW_MAX_PER_CLASS = 253_700\n",
    "N_BOOTSTRAP_FAST   = 50\n",
    "N_BOOTSTRAP_HEAVY  = 200\n",
    "FAST_BS_MAX_SAMP_PER_CLASS  = 253_700  # M ~ N for classic bootstrap\n",
    "HEAVY_BS_MAX_SAMP_PER_CLASS = 5000\n",
    "\n",
    "\n",
    "\n",
    "LENGTH_MAX_CLASS = 10\n",
    "EXCLUDE_ZERO_LENGTH = True\n",
    "\n",
    "# Repro / device\n",
    "RAND_SEED=42\n",
    "PLOT_DIR = Path(\"results_LENGTH\"); PLOT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CSV_DIR  = Path(\"tables_LENGTH\") / \"length_bootstrap\"; CSV_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": torch.backends.cudnn.benchmark = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "EPS = 1e-12\n",
    "\n",
    "# =============================== HELPERS ===============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _center(X: np.ndarray) -> np.ndarray:\n",
    "    return X - X.mean(0, keepdims=True)\n",
    "\n",
    "def _eigvals_from_X(X: np.ndarray) -> np.ndarray:\n",
    "    Xc = _center(X.astype(np.float32, copy=False))\n",
    "    try:\n",
    "        _, S, _ = np.linalg.svd(Xc, full_matrices=False)\n",
    "        lam = (S**2).astype(np.float64)\n",
    "        lam.sort()\n",
    "        return lam[::-1]\n",
    "    except Exception:\n",
    "        return np.array([], dtype=np.float64)\n",
    "\n",
    "def _jitter_unique(X: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    try:\n",
    "        if np.unique(X, axis=0).shape[0] < X.shape[0]:\n",
    "            X = X + np.random.normal(scale=eps, size=X.shape).astype(X.dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return X\n",
    "\n",
    "def _num_hidden_layers(model) -> int:\n",
    "    n = getattr(model.config, \"num_hidden_layers\", None)\n",
    "    if n is None: n = getattr(model.config, \"n_layer\", None)\n",
    "    if n is None: raise ValueError(\"Cannot determine number of hidden layers\")\n",
    "    return int(n)\n",
    "\n",
    "def _hidden_size(model) -> int:\n",
    "    d = getattr(model.config, \"hidden_size\", None)\n",
    "    if d is None: d = getattr(model.config, \"n_embd\", None)\n",
    "    if d is None: raise ValueError(\"Cannot determine hidden size\")\n",
    "    return int(d)\n",
    "\n",
    "# ========= Per-subsample single-value compute functions (your original set) =========\n",
    "# --- Isotropy (fast) ---\n",
    "def _iso_once(X: np.ndarray) -> float:\n",
    "    return float(IsoScore.IsoScore(X))\n",
    "\n",
    "def _spect_once(X: np.ndarray) -> float:\n",
    "    ev = np.linalg.eigvalsh(np.cov(X.T, ddof=0))\n",
    "    return float(ev[-1] / (ev.mean() + 1e-9))\n",
    "\n",
    "def _rand_once(X: np.ndarray, K: int = 2000) -> float:\n",
    "    n = X.shape[0]\n",
    "    if n < 2: return np.nan\n",
    "    rng = np.random.default_rng()\n",
    "    K_eff = min(K, (n*(n-1))//2)\n",
    "    i = rng.integers(0, n, size=K_eff)\n",
    "    j = rng.integers(0, n, size=K_eff)\n",
    "    same = i == j\n",
    "    if same.any():\n",
    "        j[same] = rng.integers(0, n, size=same.sum())\n",
    "    A, B = X[i], X[j]\n",
    "    num = np.sum(A*B, axis=1)\n",
    "    den = (np.linalg.norm(A, axis=1)*np.linalg.norm(B, axis=1) + 1e-9)\n",
    "    return float(np.mean(np.abs(num/den)))\n",
    "\n",
    "def _sf_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    gm = np.exp(np.mean(np.log(lam + EPS)))\n",
    "    am = float(lam.mean() + EPS)\n",
    "    return float(gm / am)\n",
    "\n",
    "def _pfI_once(X: np.ndarray) -> float:\n",
    "    n, d = X.shape\n",
    "    if n < 2: return np.nan\n",
    "    rng = np.random.default_rng()\n",
    "    U = rng.standard_normal((PFI_DIRS, d)).astype(np.float32)\n",
    "    U /= np.linalg.norm(U, axis=1, keepdims=True) + 1e-9\n",
    "    S = U @ X.T\n",
    "    m = np.max(S, axis=1, keepdims=True)\n",
    "    logZ = (m + np.log(np.sum(np.exp(S - m), axis=1, keepdims=True))).ravel()\n",
    "    lo = np.percentile(logZ, PFI_Q_LO)\n",
    "    hi = np.percentile(logZ, PFI_Q_HI)\n",
    "    return float(np.exp(lo - hi))  # ~ min Z / max Z (robust)\n",
    "\n",
    "def _vmf_kappa_once(X: np.ndarray) -> float:\n",
    "    if X.shape[0] < 2: return np.nan\n",
    "    Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
    "    R = np.linalg.norm(Xn.mean(axis=0))\n",
    "    d = Xn.shape[1]\n",
    "    if R < 1e-9: return 0.0\n",
    "    return float(max(R * (d - R**2) / (1.0 - R**2 + 1e-9), 0.0))\n",
    "\n",
    "# --- Linear ID (fast, spectral) ---\n",
    "def _pcaXX_once(X: np.ndarray, var_ratio: float) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    c = np.cumsum(lam); thr = c[-1] * var_ratio\n",
    "    return float(np.searchsorted(c, thr) + 1)\n",
    "\n",
    "def _pca95_once(X: np.ndarray) -> float:\n",
    "    return _pcaXX_once(X, 0.95)\n",
    "\n",
    "def _pca99_once(X: np.ndarray) -> float:\n",
    "    return _pcaXX_once(X, 0.99)\n",
    "\n",
    "def _erank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    p = lam / (lam.sum() + EPS)\n",
    "    H = -(p * np.log(p + EPS)).sum()\n",
    "    return float(np.exp(H))\n",
    "\n",
    "def _pr_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    s1 = lam.sum(); s2 = (lam**2).sum()\n",
    "    return float((s1**2) / (s2 + EPS))\n",
    "\n",
    "def _stable_rank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    return float(lam.sum() / (lam.max() + EPS))\n",
    "\n",
    "# --- Non-linear (heavy) ---\n",
    "def _dadapy_twonn_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    id_est, _, _ = d.compute_id_2NN()\n",
    "    return float(id_est)\n",
    "\n",
    "def _dadapy_gride_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    d.compute_distances(maxk=64)\n",
    "    ids, _, _ = d.return_id_scaling_gride(range_max=64)\n",
    "    return float(ids[-1])\n",
    "\n",
    "def _skdim_factory(name: str):\n",
    "    if not HAS_SKDIM: return None\n",
    "    mapping = {\n",
    "        \"mom\": MOM, \"tle\": TLE, \"corrint\": CorrInt, \"fishers\": FisherS,\n",
    "        \"lpca\": lPCA, \"lpca95\": lPCA, \"lpca99\": lPCA,\n",
    "        \"mle\": MLE, \"danco\": DANCo, \"ess\": ESS, \"mind_ml\": MiND_ML,\n",
    "        \"mada\": MADA, \"knn\": KNN,\n",
    "    }\n",
    "    cls = mapping.get(name)\n",
    "    if cls is None: return None\n",
    "    def _builder():\n",
    "        if name == \"lpca\":   return cls(ver=\"FO\")\n",
    "        if name == \"lpca95\": return cls(ver=\"ratio\", alphaRatio=0.95)\n",
    "        if name == \"lpca99\": return cls(ver=\"ratio\", alphaRatio=0.99)\n",
    "        return cls()\n",
    "    return _builder\n",
    "\n",
    "def _skdim_once_builder(name: str) -> Callable[[np.ndarray], float] | None:\n",
    "    build = _skdim_factory(name)\n",
    "    if build is None: return None\n",
    "    def _once(X: np.ndarray) -> float:\n",
    "        est = build()\n",
    "        est.fit(_jitter_unique(X))\n",
    "        return float(getattr(est, \"dimension_\", np.nan))\n",
    "    return _once\n",
    "\n",
    "# Registries (FULL set preserved)\n",
    "FAST_ONCE: Dict[str, Callable[[np.ndarray], float]] = {\n",
    "    # Isotropy\n",
    "    \"iso\": _iso_once, \"spect\": _spect_once, \"rand\": _rand_once,\n",
    "    \"sf\": _sf_once, \"pfI\": _pfI_once, \"vmf_kappa\": _vmf_kappa_once,\n",
    "    # Linear ID\n",
    "    \"pca95\": _pca95_once, \"pca99\": _pca99_once,\n",
    "    \"erank\": _erank_once, \"pr\": _pr_once, \"stable_rank\": _stable_rank_once,\n",
    "}\n",
    "HEAVY_ONCE: Dict[str, Callable[[np.ndarray], float] | None] = {\n",
    "    # Non-linear / local\n",
    "    \"twonn\": _dadapy_twonn_once, \"gride\": _dadapy_gride_once,\n",
    "    \"mom\": _skdim_once_builder(\"mom\"), \"tle\": _skdim_once_builder(\"tle\"),\n",
    "    \"corrint\": _skdim_once_builder(\"corrint\"), \"fishers\": _skdim_once_builder(\"fishers\"),\n",
    "    \"lpca\": _skdim_once_builder(\"lpca\"), \"lpca95\": _skdim_once_builder(\"lpca95\"),\n",
    "    \"lpca99\": _skdim_once_builder(\"lpca99\"),\n",
    "    \"mle\": _skdim_once_builder(\"mle\"), \"danco\": _skdim_once_builder(\"danco\"),\n",
    "    \"ess\": _skdim_once_builder(\"ess\"), \"mind_ml\": _skdim_once_builder(\"mind_ml\"),\n",
    "    \"mada\": _skdim_once_builder(\"mada\"), \"knn\": _skdim_once_builder(\"knn\"),\n",
    "}\n",
    "LABELS = {\n",
    "    # Isotropy\n",
    "    \"iso\":\"IsoScore\",\"spect\":\"Spectral Ratio\",\"rand\":\"RandCos |μ|\",\n",
    "    \"sf\":\"Spectral Flatness\",\"pfI\":\"Partition Isotropy I\",\"vmf_kappa\":\"vMF κ\",\n",
    "    # Linear ID\n",
    "    \"pca95\":\"PCs@95%\",\"pca99\":\"PCs@99%\",\"erank\":\"Effective Rank\",\"pr\":\"Participation Ratio\",\"stable_rank\":\"Stable Rank\",\n",
    "    # Non-linear / local\n",
    "    \"twonn\":\"TwoNN ID\",\"gride\":\"GRIDE\",\"mom\":\"MOM\",\"tle\":\"TLE\",\"corrint\":\"CorrInt\",\"fishers\":\"FisherS\",\n",
    "    \"lpca\":\"lPCA FO\",\"lpca95\":\"lPCA 0.95\",\"lpca99\":\"lPCA 0.99\",\n",
    "    \"mle\":\"MLE\",\"danco\":\"DANCo\",\"ess\":\"ESS\",\"mind_ml\":\"MiND-ML\",\"mada\":\"MADA\",\"knn\":\"KNN\",\n",
    "}\n",
    "ALL_METRICS=[ \"gride\"]\n",
    "# =============================== DATA: use existing length column ===============================\n",
    "def _pick_length_col(df: pd.DataFrame) -> str:\n",
    "    for cand in [LENGTH_COL, \"token_length\", \"lengths\", \"len\", \"LEN\", \"Length\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    raise ValueError(f\"No length column found. Tried: {LENGTH_COL}, token_length, lengths, len, LEN, Length.\")\n",
    "\n",
    "def load_length_from_column(csv_path: str,\n",
    "                            length_max: int = LENGTH_MAX_CLASS,\n",
    "                            exclude_zero: bool = EXCLUDE_ZERO_LENGTH):\n",
    "    \"\"\"\n",
    "    CSV expects:\n",
    "      sentence_id (str), tokens (list[str]), length (list[int]) per sentence.\n",
    "    Emits token-level rows with 'length_class' in {1..length_max} (length_max means 10+ by default).\n",
    "    \"\"\"\n",
    "    df_all = pd.read_csv(csv_path)\n",
    "    len_col = _pick_length_col(df_all)\n",
    "    df = df_all[[\"sentence_id\",\"tokens\", len_col]].copy()\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df.tokens  = df.tokens.apply(_to_list)\n",
    "    df[len_col] = df[len_col].apply(_to_list)\n",
    "\n",
    "    rows = []\n",
    "    for sid, toks, lens in df[[\"sentence_id\",\"tokens\", len_col]].itertuples(index=False):\n",
    "        L = min(len(toks), len(lens))\n",
    "        for wid in range(L):\n",
    "            try:\n",
    "                k = int(lens[wid])\n",
    "            except Exception:\n",
    "                continue\n",
    "            if k <= 0 and exclude_zero:\n",
    "                continue\n",
    "            if k < 0:\n",
    "                continue\n",
    "            k = min(max(k, 0), length_max)  # bucket upper tail into 'length_max'\n",
    "            if k == 0 and exclude_zero:\n",
    "                continue\n",
    "            rows.append((sid, wid, str(k), toks[wid]))\n",
    "    df_tok = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\",\"length_class\",\"word\"])\n",
    "    df_sent = df[[\"sentence_id\",\"tokens\"]].drop_duplicates(\"sentence_id\")\n",
    "    if df_tok.empty:\n",
    "        raise ValueError(\"No token rows constructed—check that your length column contains integer lists.\")\n",
    "    return df_sent, df_tok\n",
    "\n",
    "# =============================== Tokenizer/Model loader (robust for GPT‑2) ===============================\n",
    "def _load_tok_and_model(baseline: str):\n",
    "    \"\"\"\n",
    "    Robust loader:\n",
    "    - Prefer fast tokenizers (needed for .word_ids()).\n",
    "    - For GPT-2: set pad_token to eos_token and right-padding.\n",
    "    - Fallback to 'openai-community/gpt2' if the plain 'gpt2' entry is unavailable.\n",
    "    \"\"\"\n",
    "    model_id = baseline\n",
    "    tok = None; mdl = None\n",
    "    tried = []\n",
    "\n",
    "    def _try_load(mid: str):\n",
    "        tok = AutoTokenizer.from_pretrained(mid, use_fast=True, add_prefix_space=True)\n",
    "        # Ensure right padding (GPT-2 tips)\n",
    "        if getattr(tok, \"padding_side\", None) != \"right\":\n",
    "            tok.padding_side = \"right\"\n",
    "        # For GPT-2 and similar: add pad token if missing\n",
    "        if tok.pad_token is None and getattr(tok, \"eos_token\", None) is not None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "        mdl = AutoModel.from_pretrained(mid, output_hidden_states=True)\n",
    "        # Propagate pad id to model if absent\n",
    "        if getattr(mdl.config, \"pad_token_id\", None) is None and tok.pad_token_id is not None:\n",
    "            mdl.config.pad_token_id = tok.pad_token_id\n",
    "        return tok, mdl\n",
    "\n",
    "    # try main id\n",
    "    try:\n",
    "        tok, mdl = _try_load(model_id)\n",
    "    except Exception as e1:\n",
    "        tried.append((model_id, str(e1)))\n",
    "        # GPT-2 robust fallback namespace\n",
    "        if model_id.lower() in {\"gpt2\", \"gpt-2\"}:\n",
    "            for alt in [\"openai-community/gpt2\", \"gpt2\"]:\n",
    "                try:\n",
    "                    tok, mdl = _try_load(alt)\n",
    "                    model_id = alt\n",
    "                    break\n",
    "                except Exception as e2:\n",
    "                    tried.append((alt, str(e2)))\n",
    "        if tok is None or mdl is None:\n",
    "            # surface useful diagnostics\n",
    "            raise RuntimeError(\n",
    "                \"Failed to load tokenizer/model. Attempts:\\n\" +\n",
    "                \"\\n\".join([f\" - {mid}: {err}\" for mid, err in tried])\n",
    "            )\n",
    "\n",
    "    mdl = mdl.eval().to(device)\n",
    "    if device == \"cuda\":\n",
    "        mdl.half()\n",
    "    return tok, mdl, model_id\n",
    "\n",
    "# =============================== EMBEDDING (BERT & GPT‑2) ===============================\n",
    "def embed_subset(df_sent: pd.DataFrame,\n",
    "                 subset_df: pd.DataFrame,\n",
    "                 baseline: str = BASELINE,\n",
    "                 word_rep_mode: str = WORD_REP_MODE,\n",
    "                 batch_size: int = BATCH_SIZE) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df_sent[\"sentence_id\"]   = df_sent[\"sentence_id\"].astype(str)\n",
    "    subset_df[\"sentence_id\"] = subset_df[\"sentence_id\"].astype(str)\n",
    "\n",
    "    # sid -> list[(global_idx, word_id)]\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(subset_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(str(sid), []).append((gidx, int(wid)))\n",
    "\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_sent[df_sent.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    tokzr, model, model_id = _load_tok_and_model(baseline)\n",
    "\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
    "    # Use add_prefix_space when supported (GPT‑2-friendly)\n",
    "    if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "        enc_kwargs[\"add_prefix_space\"] = True\n",
    "\n",
    "    L = _num_hidden_layers(model) + 1   # include embedding layer\n",
    "    D = _hidden_size(model)\n",
    "    N = len(subset_df)\n",
    "\n",
    "    reps   = np.zeros((L, N, D), np.float16)\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{model_id} (embed subset)\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L,B,T,D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                mp = {}\n",
    "                # Fast tokenizers expose word_ids(); map wordpiece positions back to word indices\n",
    "                for tidx, wid in enumerate(enc_be.word_ids(b)):\n",
    "                    if wid is not None:\n",
    "                        mp.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = mp.get(wid)\n",
    "                    if not toks: continue\n",
    "                    if word_rep_mode == \"first\":\n",
    "                        vec = h[:, b, toks[0], :]\n",
    "                    elif word_rep_mode == \"last\":\n",
    "                        vec = h[:, b, toks[-1], :]\n",
    "                    elif word_rep_mode == \"mean\":\n",
    "                        vec = h[:, b, toks, :].mean(axis=1)\n",
    "                    else:\n",
    "                        raise ValueError(\"WORD_REP_MODE must be {'first','last','mean'} (GPT‑2: 'last' or 'mean').\")\n",
    "                    reps[:, gidx, :] = vec.astype(np.float16, copy=False)\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            # free batch buffers\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    missing = int((~filled).sum())\n",
    "    if missing: print(f\"⚠ Missing vectors for {missing} of {N} tokens\")\n",
    "    del model; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    return reps, filled\n",
    "\n",
    "# =============================== BOOTSTRAP CORE ===============================\n",
    "def _bs_layer_loop(rep_sub: np.ndarray, M: int, n_reps: int, compute_once: Callable[[np.ndarray], float]):\n",
    "    L, N, D = rep_sub.shape\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    A = np.full((n_reps, L), np.nan, np.float32)\n",
    "    for r in range(n_reps):\n",
    "        idx = rng.integers(0, N, size=M)\n",
    "        for l in range(L):\n",
    "            X = rep_sub[l, idx].astype(np.float32, copy=False)\n",
    "            try:\n",
    "                A[r, l] = float(compute_once(X))\n",
    "            except Exception:\n",
    "                A[r, l] = np.nan\n",
    "    mu = np.nanmean(A, axis=0).astype(np.float32)\n",
    "    lo = np.nanpercentile(A, 2.5, axis=0).astype(np.float32)\n",
    "    hi = np.nanpercentile(A, 97.5, axis=0).astype(np.float32)\n",
    "    return mu, lo, hi\n",
    "\n",
    "# =============================== SAVE / PLOT ===============================\n",
    "def save_metric_csv_all_classes(metric: str,\n",
    "                                class_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                                layers: np.ndarray,\n",
    "                                baseline: str,\n",
    "                                subset_name: str = \"raw\"):\n",
    "    rows = []\n",
    "    for c, stats in class_to_stats.items():\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        for l, val in enumerate(mu):\n",
    "            rows.append({\n",
    "                \"subset\": subset_name, \"model\": baseline, \"feature\": \"length\",\n",
    "                \"class\": c, \"metric\": metric, \"layer\": int(layers[l]),\n",
    "                \"mean\": float(val) if np.isfinite(val) else np.nan,\n",
    "                \"ci_low\": float(lo[l]) if isinstance(lo, np.ndarray) and np.isfinite(lo[l]) else np.nan,\n",
    "                \"ci_high\": float(hi[l]) if isinstance(hi, np.ndarray) and np.isfinite(hi[l]) else np.nan,\n",
    "                \"n_tokens\": int(stats.get(\"n\", 0)), \"word_rep_mode\": WORD_REP_MODE,\n",
    "                \"source_csv\": Path(CSV_PATH).name,\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    out = CSV_DIR / f\"length_{subset_name}_{metric}_{baseline}.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "\n",
    "def plot_metric_with_ci(class_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                        layers: np.ndarray, metric: str, title: str, out_path: Path,\n",
    "                        palette: Dict[str, Tuple[float, float, float]] | None = None):\n",
    "    plt.figure(figsize=(10.5, 5.5))\n",
    "    for c in sorted(class_to_stats.keys(), key=lambda s: int(s)):\n",
    "        stats = class_to_stats[c]\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        if mu is None or np.all(np.isnan(mu)): continue\n",
    "        color = palette.get(c) if isinstance(palette, dict) else None\n",
    "        plt.plot(layers, mu, label=c, lw=1.8, color=color)\n",
    "        if isinstance(lo, np.ndarray) and isinstance(hi, np.ndarray) and not np.all(np.isnan(lo)):\n",
    "            plt.fill_between(layers, lo, hi, alpha=0.15, color=color)\n",
    "    plt.xlabel(\"Layer\"); plt.ylabel(LABELS.get(metric, metric.upper())); plt.title(title)\n",
    "    plt.legend(ncol=6, fontsize=\"small\", title=f\"Length ( {LENGTH_MAX_CLASS} = {LENGTH_MAX_CLASS}+ )\", frameon=False)\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=220); plt.close()\n",
    "\n",
    "# =============================== DRIVER ===============================\n",
    "def sample_raw(df_tok: pd.DataFrame, per_class_cap: int = RAW_MAX_PER_CLASS) -> pd.DataFrame:\n",
    "    picks = []\n",
    "    for c, sub in df_tok.groupby(\"length_class\", sort=False):\n",
    "        n = min(len(sub), per_class_cap)\n",
    "        picks.append(sub.sample(n, random_state=RAND_SEED, replace=False))\n",
    "    return pd.concat(picks, ignore_index=True)\n",
    "\n",
    "def make_length_palette(classes: List[str]) -> Dict[str, Tuple[float, float, float]]:\n",
    "    vals = sorted([int(c) for c in classes])\n",
    "    cmap = sns.color_palette(\"rocket\", len(vals))\n",
    "    return {str(v): cmap[i] for i, v in enumerate(vals)}\n",
    "\n",
    "def run_length_from_col_pipeline():\n",
    "    # 1) Load token lists + length classes from existing column\n",
    "    df_sent, len_df = load_length_from_column(\n",
    "        CSV_PATH, length_max=LENGTH_MAX_CLASS, exclude_zero=EXCLUDE_ZERO_LENGTH\n",
    "    )\n",
    "    classes = sorted(len_df.length_class.unique(), key=lambda s: int(s))\n",
    "    palette = make_length_palette(classes)\n",
    "    print(f\"✓ corpus ready — {len(len_df):,} tokens across length classes {classes}\")\n",
    "\n",
    "    # 2) Optional per-class cap (currently unlimited for fast metrics)\n",
    "    raw_df = sample_raw(len_df, RAW_MAX_PER_CLASS)\n",
    "    print(\"Sample sizes per length (raw cap):\")\n",
    "    counts = raw_df.length_class.value_counts().sort_index()\n",
    "    print(counts.to_dict())\n",
    "\n",
    "    # 3) Embed once\n",
    "    reps, filled = embed_subset(df_sent, raw_df, BASELINE, WORD_REP_MODE, BATCH_SIZE)\n",
    "    raw_df = raw_df.reset_index(drop=True).loc[filled].reset_index(drop=True)\n",
    "    cls_arr = raw_df.length_class.values\n",
    "    L = reps.shape[0]; layers = np.arange(L)\n",
    "    print(f\"✓ embedded {len(raw_df):,} tokens  • layers={L}\")\n",
    "\n",
    "    # 4) Metric loop (FULL set)\n",
    "    for metric in ALL_METRICS:\n",
    "        print(f\"\\n→ Computing metric: {metric} …\")\n",
    "        compute_once = FAST_ONCE.get(metric) or HEAVY_ONCE.get(metric)\n",
    "        if compute_once is None:\n",
    "            print(f\"  (skipping {metric}: estimator unavailable)\")\n",
    "            continue\n",
    "\n",
    "        n_bs = N_BOOTSTRAP_FAST if metric in FAST_ONCE else N_BOOTSTRAP_HEAVY\n",
    "        Mcap = FAST_BS_MAX_SAMP_PER_CLASS if metric in FAST_ONCE else HEAVY_BS_MAX_SAMP_PER_CLASS\n",
    "\n",
    "        class_results: Dict[str, Dict[str, np.ndarray]] = {}\n",
    "        for c in classes:\n",
    "            idx = np.where(cls_arr == c)[0]\n",
    "            if idx.size < 3:\n",
    "                continue\n",
    "            sub = reps[:, idx]  # (L, n_c, D)\n",
    "            Nc = sub.shape[1]\n",
    "            M = min(Mcap, Nc)\n",
    "            mu, lo, hi = _bs_layer_loop(sub, M, n_bs, compute_once)\n",
    "            class_results[c] = {\"mean\": mu, \"lo\": lo, \"hi\": hi, \"n\": int(Nc)}\n",
    "\n",
    "        # Save + plot immediately\n",
    "        save_metric_csv_all_classes(metric, class_results, layers, BASELINE, subset_name=\"raw\")\n",
    "        plot_metric_with_ci(class_results, layers, metric,\n",
    "                            title=f\"{LABELS.get(metric, metric.upper())} • {BASELINE}\",\n",
    "                            out_path=PLOT_DIR / f\"length_raw_{metric}_{BASELINE}.png\",\n",
    "                            palette=palette)\n",
    "        print(f\"  ✓ saved: CSV= {CSV_DIR}/length_raw_{metric}_{BASELINE}.csv  \"\n",
    "              f\"plot= {PLOT_DIR}/length_raw_{metric}_{BASELINE}.png\")\n",
    "\n",
    "        del class_results; gc.collect()\n",
    "        if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    del reps; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    print(\"\\n✓ done (incremental outputs produced per metric).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_length_from_col_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a95d03-69dd-4263-824a-2ed7873a3098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ corpus ready — 184,849 tokens across length classes ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
      "Sample sizes per length (raw cap):\n",
      "{'1': 27854, '10': 7320, '2': 27939, '3': 33973, '4': 31213, '5': 18028, '6': 13729, '7': 11768, '8': 7673, '9': 5352}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openai-community/gpt2 (embed subset): 100%|█| 10067/10067 [01:34<00:00, 107.09it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ embedded 184,849 tokens  • layers=13\n",
      "\n",
      "→ Computing metric: gride …\n",
      "  ✓ saved: CSV= tables_LENGTH_no_index/length_bootstrap/length_raw_gride_gpt2.csv  plot= results_LENGTH_no_index/length_raw_gride_gpt2.png\n",
      "\n",
      "✓ done (incremental outputs produced per metric).\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "sns.set_style(\"darkgrid\")\n",
    "# ============== Optional deps (gracefully skipped if not installed) ==============\n",
    "HAS_DADAPY = False\n",
    "try:\n",
    "    from dadapy import Data  # DADApy ID estimators (TwoNN, GRIDE)\n",
    "    HAS_DADAPY = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "HAS_SKDIM = False\n",
    "try:\n",
    "    from skdim.id import (\n",
    "        MOM, TLE, CorrInt, FisherS, lPCA,\n",
    "        MLE, DANCo, ESS, MiND_ML, MADA, KNN\n",
    "    )\n",
    "    HAS_SKDIM = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# IsoScore: library if available, else a simple monotone fallback\n",
    "try:\n",
    "    from isoscore import IsoScore\n",
    "    _HAS_ISOSCORE = True\n",
    "except Exception:\n",
    "    _HAS_ISOSCORE = False\n",
    "    class _IsoScoreFallback:\n",
    "        @staticmethod\n",
    "        def IsoScore(X: np.ndarray) -> float:\n",
    "            C = np.cov(X.T, ddof=0)\n",
    "            ev = np.linalg.eigvalsh(C)\n",
    "            if ev.mean() <= 0 or ev[-1] <= 0:\n",
    "                return 0.0\n",
    "            return float(np.clip(ev.mean() / (ev[-1] + 1e-9), 0.0, 1.0))\n",
    "    IsoScore = _IsoScoreFallback()\n",
    "\n",
    "# =============================== CONFIG ===============================\n",
    "CSV_PATH      = \"en_ewt-ud-train_sentences.csv\"\n",
    "LENGTH_COL    = \"length\"\n",
    "\n",
    "BASELINE      = \"gpt2\"\n",
    "WORD_REP_MODE = \"last\"\n",
    "\n",
    "RAW_MAX_PER_CLASS = 253_771\n",
    "N_BOOTSTRAP_FAST   = 50\n",
    "N_BOOTSTRAP_HEAVY  = 200\n",
    "FAST_BS_MAX_SAMP_PER_CLASS  = 184_870\n",
    "HEAVY_BS_MAX_SAMP_PER_CLASS = 5000\n",
    "\n",
    "LENGTH_MAX_CLASS   = 10\n",
    "EXCLUDE_ZERO_LENGTH = True\n",
    "\n",
    "# NEW: drop tokens at 1-based sentence index = 1 (the first word)\n",
    "# UD CoNLL-U IDs start at 1; our 'wid' below is 0-based, so index==1 <=> wid==0\n",
    "EXCLUDE_INDEX_1 = True\n",
    "\n",
    "RAND_SEED=42\n",
    "PLOT_DIR = Path(\"results_LENGTH_no_index\"); PLOT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CSV_DIR  = Path(\"tables_LENGTH_no_index\") / \"length_bootstrap\"; CSV_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# =============================== HELPERS ===============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _center(X: np.ndarray) -> np.ndarray:\n",
    "    return X - X.mean(0, keepdims=True)\n",
    "\n",
    "def _eigvals_from_X(X: np.ndarray) -> np.ndarray:\n",
    "    Xc = _center(X.astype(np.float32, copy=False))\n",
    "    try:\n",
    "        _, S, _ = np.linalg.svd(Xc, full_matrices=False)\n",
    "        lam = (S**2).astype(np.float64)\n",
    "        lam.sort()\n",
    "        return lam[::-1]\n",
    "    except Exception:\n",
    "        return np.array([], dtype=np.float64)\n",
    "\n",
    "def _jitter_unique(X: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    try:\n",
    "        if np.unique(X, axis=0).shape[0] < X.shape[0]:\n",
    "            X = X + np.random.normal(scale=eps, size=X.shape).astype(X.dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return X\n",
    "\n",
    "def _num_hidden_layers(model) -> int:\n",
    "    n = getattr(model.config, \"num_hidden_layers\", None)\n",
    "    if n is None: n = getattr(model.config, \"n_layer\", None)\n",
    "    if n is None: raise ValueError(\"Cannot determine number of hidden layers\")\n",
    "    return int(n)\n",
    "\n",
    "def _hidden_size(model) -> int:\n",
    "    d = getattr(model.config, \"hidden_size\", None)\n",
    "    if d is None: d = getattr(model.config, \"n_embd\", None)\n",
    "    if d is None: raise ValueError(\"Cannot determine hidden size\")\n",
    "    return int(d)\n",
    "\n",
    "# ========= Per-subsample single-value compute functions (your original set) =========\n",
    "# --- Isotropy (fast) ---\n",
    "def _iso_once(X: np.ndarray) -> float:\n",
    "    return float(IsoScore.IsoScore(X))\n",
    "\n",
    "def _spect_once(X: np.ndarray) -> float:\n",
    "    ev = np.linalg.eigvalsh(np.cov(X.T, ddof=0))\n",
    "    return float(ev[-1] / (ev.mean() + 1e-9))\n",
    "\n",
    "def _rand_once(X: np.ndarray, K: int = 2000) -> float:\n",
    "    n = X.shape[0]\n",
    "    if n < 2: return np.nan\n",
    "    rng = np.random.default_rng()\n",
    "    K_eff = min(K, (n*(n-1))//2)\n",
    "    i = rng.integers(0, n, size=K_eff)\n",
    "    j = rng.integers(0, n, size=K_eff)\n",
    "    same = i == j\n",
    "    if same.any():\n",
    "        j[same] = rng.integers(0, n, size=same.sum())\n",
    "    A, B = X[i], X[j]\n",
    "    num = np.sum(A*B, axis=1)\n",
    "    den = (np.linalg.norm(A, axis=1)*np.linalg.norm(B, axis=1) + 1e-9)\n",
    "    return float(np.mean(np.abs(num/den)))\n",
    "\n",
    "def _sf_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    gm = np.exp(np.mean(np.log(lam + EPS)))\n",
    "    am = float(lam.mean() + EPS)\n",
    "    return float(gm / am)\n",
    "\n",
    "def _pfI_once(X: np.ndarray) -> float:\n",
    "    n, d = X.shape\n",
    "    if n < 2: return np.nan\n",
    "    rng = np.random.default_rng()\n",
    "    U = rng.standard_normal((PFI_DIRS, d)).astype(np.float32)\n",
    "    U /= np.linalg.norm(U, axis=1, keepdims=True) + 1e-9\n",
    "    S = U @ X.T\n",
    "    m = np.max(S, axis=1, keepdims=True)\n",
    "    logZ = (m + np.log(np.sum(np.exp(S - m), axis=1, keepdims=True))).ravel()\n",
    "    lo = np.percentile(logZ, PFI_Q_LO)\n",
    "    hi = np.percentile(logZ, PFI_Q_HI)\n",
    "    return float(np.exp(lo - hi))  # ~ min Z / max Z (robust)\n",
    "\n",
    "def _vmf_kappa_once(X: np.ndarray) -> float:\n",
    "    if X.shape[0] < 2: return np.nan\n",
    "    Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
    "    R = np.linalg.norm(Xn.mean(axis=0))\n",
    "    d = Xn.shape[1]\n",
    "    if R < 1e-9: return 0.0\n",
    "    return float(max(R * (d - R**2) / (1.0 - R**2 + 1e-9), 0.0))\n",
    "\n",
    "# --- Linear ID (fast, spectral) ---\n",
    "def _pcaXX_once(X: np.ndarray, var_ratio: float) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    c = np.cumsum(lam); thr = c[-1] * var_ratio\n",
    "    return float(np.searchsorted(c, thr) + 1)\n",
    "\n",
    "def _pca95_once(X: np.ndarray) -> float:\n",
    "    return _pcaXX_once(X, 0.95)\n",
    "\n",
    "def _pca99_once(X: np.ndarray) -> float:\n",
    "    return _pcaXX_once(X, 0.99)\n",
    "\n",
    "def _erank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    p = lam / (lam.sum() + EPS)\n",
    "    H = -(p * np.log(p + EPS)).sum()\n",
    "    return float(np.exp(H))\n",
    "\n",
    "def _pr_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    s1 = lam.sum(); s2 = (lam**2).sum()\n",
    "    return float((s1**2) / (s2 + EPS))\n",
    "\n",
    "def _stable_rank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    return float(lam.sum() / (lam.max() + EPS))\n",
    "\n",
    "# --- Non-linear (heavy) ---\n",
    "def _dadapy_twonn_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    id_est, _, _ = d.compute_id_2NN()\n",
    "    return float(id_est)\n",
    "\n",
    "def _dadapy_gride_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    d.compute_distances(maxk=64)\n",
    "    ids, _, _ = d.return_id_scaling_gride(range_max=64)\n",
    "    return float(ids[-1])\n",
    "\n",
    "def _skdim_factory(name: str):\n",
    "    if not HAS_SKDIM: return None\n",
    "    mapping = {\n",
    "        \"mom\": MOM, \"tle\": TLE, \"corrint\": CorrInt, \"fishers\": FisherS,\n",
    "        \"lpca\": lPCA, \"lpca95\": lPCA, \"lpca99\": lPCA,\n",
    "        \"mle\": MLE, \"danco\": DANCo, \"ess\": ESS, \"mind_ml\": MiND_ML,\n",
    "        \"mada\": MADA, \"knn\": KNN,\n",
    "    }\n",
    "    cls = mapping.get(name)\n",
    "    if cls is None: return None\n",
    "    def _builder():\n",
    "        if name == \"lpca\":   return cls(ver=\"FO\")\n",
    "        if name == \"lpca95\": return cls(ver=\"ratio\", alphaRatio=0.95)\n",
    "        if name == \"lpca99\": return cls(ver=\"ratio\", alphaRatio=0.99)\n",
    "        return cls()\n",
    "    return _builder\n",
    "\n",
    "def _skdim_once_builder(name: str) -> Callable[[np.ndarray], float] | None:\n",
    "    build = _skdim_factory(name)\n",
    "    if build is None: return None\n",
    "    def _once(X: np.ndarray) -> float:\n",
    "        est = build()\n",
    "        est.fit(_jitter_unique(X))\n",
    "        return float(getattr(est, \"dimension_\", np.nan))\n",
    "    return _once\n",
    "\n",
    "# Registries (FULL set preserved)\n",
    "FAST_ONCE: Dict[str, Callable[[np.ndarray], float]] = {\n",
    "    # Isotropy\n",
    "    \"iso\": _iso_once, \"spect\": _spect_once, \"rand\": _rand_once,\n",
    "    \"sf\": _sf_once, \"pfI\": _pfI_once, \"vmf_kappa\": _vmf_kappa_once,\n",
    "    # Linear ID\n",
    "    \"pca95\": _pca95_once, \"pca99\": _pca99_once,\n",
    "    \"erank\": _erank_once, \"pr\": _pr_once, \"stable_rank\": _stable_rank_once,\n",
    "}\n",
    "HEAVY_ONCE: Dict[str, Callable[[np.ndarray], float] | None] = {\n",
    "    # Non-linear / local\n",
    "    \"twonn\": _dadapy_twonn_once, \"gride\": _dadapy_gride_once,\n",
    "    \"mom\": _skdim_once_builder(\"mom\"), \"tle\": _skdim_once_builder(\"tle\"),\n",
    "    \"corrint\": _skdim_once_builder(\"corrint\"), \"fishers\": _skdim_once_builder(\"fishers\"),\n",
    "    \"lpca\": _skdim_once_builder(\"lpca\"), \"lpca95\": _skdim_once_builder(\"lpca95\"),\n",
    "    \"lpca99\": _skdim_once_builder(\"lpca99\"),\n",
    "    \"mle\": _skdim_once_builder(\"mle\"), \"danco\": _skdim_once_builder(\"danco\"),\n",
    "    \"ess\": _skdim_once_builder(\"ess\"), \"mind_ml\": _skdim_once_builder(\"mind_ml\"),\n",
    "    \"mada\": _skdim_once_builder(\"mada\"), \"knn\": _skdim_once_builder(\"knn\"),\n",
    "}\n",
    "LABELS = {\n",
    "    # Isotropy\n",
    "    \"iso\":\"IsoScore\",\"spect\":\"Spectral Ratio\",\"rand\":\"RandCos |μ|\",\n",
    "    \"sf\":\"Spectral Flatness\",\"pfI\":\"Partition Isotropy I\",\"vmf_kappa\":\"vMF κ\",\n",
    "    # Linear ID\n",
    "    \"pca95\":\"PCs@95%\",\"pca99\":\"PCs@99%\",\"erank\":\"Effective Rank\",\"pr\":\"Participation Ratio\",\"stable_rank\":\"Stable Rank\",\n",
    "    # Non-linear / local\n",
    "    \"twonn\":\"TwoNN ID\",\"gride\":\"GRIDE\",\"mom\":\"MOM\",\"tle\":\"TLE\",\"corrint\":\"CorrInt\",\"fishers\":\"FisherS\",\n",
    "    \"lpca\":\"lPCA FO\",\"lpca95\":\"lPCA 0.95\",\"lpca99\":\"lPCA 0.99\",\n",
    "    \"mle\":\"MLE\",\"danco\":\"DANCo\",\"ess\":\"ESS\",\"mind_ml\":\"MiND-ML\",\"mada\":\"MADA\",\"knn\":\"KNN\",\n",
    "}\n",
    "ALL_METRICS=[  \"gride\"]\n",
    "# =============================== DATA: use existing length column ===============================\n",
    "def _pick_length_col(df: pd.DataFrame) -> str:\n",
    "    for cand in [LENGTH_COL, \"token_length\", \"lengths\", \"len\", \"LEN\", \"Length\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    raise ValueError(f\"No length column found. Tried: {LENGTH_COL}, token_length, lengths, len, LEN, Length.\")\n",
    "\n",
    "def load_length_from_column(csv_path: str,\n",
    "                            length_max: int = LENGTH_MAX_CLASS,\n",
    "                            exclude_zero: bool = EXCLUDE_ZERO_LENGTH):\n",
    "    \"\"\"\n",
    "    CSV expects:\n",
    "      sentence_id (str), tokens (list[str]), length (list[int]) per sentence.\n",
    "    Emits token-level rows with 'length_class' in {1..length_max} (length_max means 10+ by default).\n",
    "    Drops tokens whose per-sentence position is 1 (first word) if EXCLUDE_INDEX_1=True.\n",
    "    \"\"\"\n",
    "    df_all = pd.read_csv(csv_path)\n",
    "    len_col = _pick_length_col(df_all)\n",
    "    df = df_all[[\"sentence_id\",\"tokens\", len_col]].copy()\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df.tokens  = df.tokens.apply(_to_list)\n",
    "    df[len_col] = df[len_col].apply(_to_list)\n",
    "\n",
    "    rows = []\n",
    "    for sid, toks, lens in df[[\"sentence_id\",\"tokens\", len_col]].itertuples(index=False):\n",
    "        L = min(len(toks), len(lens))\n",
    "        for wid in range(L):\n",
    "            # -----------------------------------------------\n",
    "            # IMPORTANT: drop 1-based index == 1 (first word)\n",
    "            # If you meant 0-based index==1 (2nd word), change wid==0 -> wid==1 below.\n",
    "            if EXCLUDE_INDEX_1 and wid == 0:\n",
    "                continue\n",
    "            # -----------------------------------------------\n",
    "            try:\n",
    "                k = int(lens[wid])\n",
    "            except Exception:\n",
    "                continue\n",
    "            if k <= 0 and exclude_zero:\n",
    "                continue\n",
    "            if k < 0:\n",
    "                continue\n",
    "            k = min(max(k, 0), length_max)  # bucket upper tail into 'length_max'\n",
    "            if k == 0 and exclude_zero:\n",
    "                continue\n",
    "            rows.append((sid, wid, str(k), toks[wid]))\n",
    "\n",
    "    df_tok = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\",\"length_class\",\"word\"])\n",
    "\n",
    "    # Extra safety (redundant with the loop guard but future-proof if code moves):\n",
    "    if EXCLUDE_INDEX_1 and not df_tok.empty:\n",
    "        df_tok = df_tok[df_tok.word_id != 0].reset_index(drop=True)\n",
    "\n",
    "    df_sent = df[[\"sentence_id\",\"tokens\"]].drop_duplicates(\"sentence_id\")\n",
    "    if df_tok.empty:\n",
    "        raise ValueError(\"No token rows constructed—check that your length column contains integer lists.\")\n",
    "    return df_sent, df_tok\n",
    "\n",
    "# =============================== Tokenizer/Model loader (robust for GPT‑2) ===============================\n",
    "def _load_tok_and_model(baseline: str):\n",
    "    \"\"\"\n",
    "    Robust loader:\n",
    "    - Prefer fast tokenizers (needed for .word_ids()).\n",
    "    - For GPT-2: set pad_token to eos_token and right-padding.\n",
    "    - Fallback to 'openai-community/gpt2' if the plain 'gpt2' entry is unavailable.\n",
    "    \"\"\"\n",
    "    model_id = baseline\n",
    "    tok = None; mdl = None\n",
    "    tried = []\n",
    "\n",
    "    def _try_load(mid: str):\n",
    "        tok = AutoTokenizer.from_pretrained(mid, use_fast=True, add_prefix_space=True)\n",
    "        # Ensure right padding (GPT-2 tips)\n",
    "        if getattr(tok, \"padding_side\", None) != \"right\":\n",
    "            tok.padding_side = \"right\"\n",
    "        # For GPT-2 and similar: add pad token if missing\n",
    "        if tok.pad_token is None and getattr(tok, \"eos_token\", None) is not None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "        mdl = AutoModel.from_pretrained(mid, output_hidden_states=True)\n",
    "        # Propagate pad id to model if absent\n",
    "        if getattr(mdl.config, \"pad_token_id\", None) is None and tok.pad_token_id is not None:\n",
    "            mdl.config.pad_token_id = tok.pad_token_id\n",
    "        return tok, mdl\n",
    "\n",
    "    # try main id\n",
    "    try:\n",
    "        tok, mdl = _try_load(model_id)\n",
    "    except Exception as e1:\n",
    "        tried.append((model_id, str(e1)))\n",
    "        # GPT-2 robust fallback namespace\n",
    "        if model_id.lower() in {\"gpt2\", \"gpt-2\"}:\n",
    "            for alt in [\"openai-community/gpt2\", \"gpt2\"]:\n",
    "                try:\n",
    "                    tok, mdl = _try_load(alt)\n",
    "                    model_id = alt\n",
    "                    break\n",
    "                except Exception as e2:\n",
    "                    tried.append((alt, str(e2)))\n",
    "        if tok is None or mdl is None:\n",
    "            # surface useful diagnostics\n",
    "            raise RuntimeError(\n",
    "                \"Failed to load tokenizer/model. Attempts:\\n\" +\n",
    "                \"\\n\".join([f\" - {mid}: {err}\" for mid, err in tried])\n",
    "            )\n",
    "\n",
    "    mdl = mdl.eval().to(device)\n",
    "    if device == \"cuda\":\n",
    "        mdl.half()\n",
    "    return tok, mdl, model_id\n",
    "\n",
    "# =============================== EMBEDDING (BERT & GPT‑2) ===============================\n",
    "def embed_subset(df_sent: pd.DataFrame,\n",
    "                 subset_df: pd.DataFrame,\n",
    "                 baseline: str = BASELINE,\n",
    "                 word_rep_mode: str = WORD_REP_MODE,\n",
    "                 batch_size: int = BATCH_SIZE) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df_sent[\"sentence_id\"]   = df_sent[\"sentence_id\"].astype(str)\n",
    "    subset_df[\"sentence_id\"] = subset_df[\"sentence_id\"].astype(str)\n",
    "\n",
    "    # sid -> list[(global_idx, word_id)]\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(subset_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(str(sid), []).append((gidx, int(wid)))\n",
    "\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_sent[df_sent.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    tokzr, model, model_id = _load_tok_and_model(baseline)\n",
    "\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
    "    # Use add_prefix_space when supported (GPT‑2-friendly)\n",
    "    if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "        enc_kwargs[\"add_prefix_space\"] = True\n",
    "\n",
    "    L = _num_hidden_layers(model) + 1   # include embedding layer\n",
    "    D = _hidden_size(model)\n",
    "    N = len(subset_df)\n",
    "\n",
    "    reps   = np.zeros((L, N, D), np.float16)\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{model_id} (embed subset)\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L,B,T,D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                mp = {}\n",
    "                # Fast tokenizers expose word_ids(); map wordpiece positions back to word indices\n",
    "                for tidx, wid in enumerate(enc_be.word_ids(b)):\n",
    "                    if wid is not None:\n",
    "                        mp.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = mp.get(wid)\n",
    "                    if not toks: continue\n",
    "                    if word_rep_mode == \"first\":\n",
    "                        vec = h[:, b, toks[0], :]\n",
    "                    elif word_rep_mode == \"last\":\n",
    "                        vec = h[:, b, toks[-1], :]\n",
    "                    elif word_rep_mode == \"mean\":\n",
    "                        vec = h[:, b, toks, :].mean(axis=1)\n",
    "                    else:\n",
    "                        raise ValueError(\"WORD_REP_MODE must be {'first','last','mean'} (GPT‑2: 'last' or 'mean').\")\n",
    "                    reps[:, gidx, :] = vec.astype(np.float16, copy=False)\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            # free batch buffers\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    missing = int((~filled).sum())\n",
    "    if missing: print(f\"⚠ Missing vectors for {missing} of {N} tokens\")\n",
    "    del model; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    return reps, filled\n",
    "\n",
    "# =============================== BOOTSTRAP CORE ===============================\n",
    "def _bs_layer_loop(rep_sub: np.ndarray, M: int, n_reps: int, compute_once: Callable[[np.ndarray], float]):\n",
    "    L, N, D = rep_sub.shape\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    A = np.full((n_reps, L), np.nan, np.float32)\n",
    "    for r in range(n_reps):\n",
    "        idx = rng.integers(0, N, size=M)\n",
    "        for l in range(L):\n",
    "            X = rep_sub[l, idx].astype(np.float32, copy=False)\n",
    "            try:\n",
    "                A[r, l] = float(compute_once(X))\n",
    "            except Exception:\n",
    "                A[r, l] = np.nan\n",
    "    mu = np.nanmean(A, axis=0).astype(np.float32)\n",
    "    lo = np.nanpercentile(A, 2.5, axis=0).astype(np.float32)\n",
    "    hi = np.nanpercentile(A, 97.5, axis=0).astype(np.float32)\n",
    "    return mu, lo, hi\n",
    "\n",
    "# =============================== SAVE / PLOT ===============================\n",
    "def save_metric_csv_all_classes(metric: str,\n",
    "                                class_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                                layers: np.ndarray,\n",
    "                                baseline: str,\n",
    "                                subset_name: str = \"raw\"):\n",
    "    rows = []\n",
    "    for c, stats in class_to_stats.items():\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        for l, val in enumerate(mu):\n",
    "            rows.append({\n",
    "                \"subset\": subset_name, \"model\": baseline, \"feature\": \"length\",\n",
    "                \"class\": c, \"metric\": metric, \"layer\": int(layers[l]),\n",
    "                \"mean\": float(val) if np.isfinite(val) else np.nan,\n",
    "                \"ci_low\": float(lo[l]) if isinstance(lo, np.ndarray) and np.isfinite(lo[l]) else np.nan,\n",
    "                \"ci_high\": float(hi[l]) if isinstance(hi, np.ndarray) and np.isfinite(hi[l]) else np.nan,\n",
    "                \"n_tokens\": int(stats.get(\"n\", 0)), \"word_rep_mode\": WORD_REP_MODE,\n",
    "                \"source_csv\": Path(CSV_PATH).name,\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    out = CSV_DIR / f\"length_{subset_name}_{metric}_{baseline}.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "\n",
    "def plot_metric_with_ci(class_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                        layers: np.ndarray, metric: str, title: str, out_path: Path,\n",
    "                        palette: Dict[str, Tuple[float, float, float]] | None = None):\n",
    "    plt.figure(figsize=(10.5, 5.5))\n",
    "    for c in sorted(class_to_stats.keys(), key=lambda s: int(s)):\n",
    "        stats = class_to_stats[c]\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        if mu is None or np.all(np.isnan(mu)): continue\n",
    "        color = palette.get(c) if isinstance(palette, dict) else None\n",
    "        plt.plot(layers, mu, label=c, lw=1.8, color=color)\n",
    "        if isinstance(lo, np.ndarray) and isinstance(hi, np.ndarray) and not np.all(np.isnan(lo)):\n",
    "            plt.fill_between(layers, lo, hi, alpha=0.15, color=color)\n",
    "    plt.xlabel(\"Layer\"); plt.ylabel(LABELS.get(metric, metric.upper())); plt.title(title)\n",
    "    plt.legend(ncol=6, fontsize=\"small\", title=f\"Length ( {LENGTH_MAX_CLASS} = {LENGTH_MAX_CLASS}+ )\", frameon=False)\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=220); plt.close()\n",
    "\n",
    "# =============================== DRIVER ===============================\n",
    "def sample_raw(df_tok: pd.DataFrame, per_class_cap: int = RAW_MAX_PER_CLASS) -> pd.DataFrame:\n",
    "    picks = []\n",
    "    for c, sub in df_tok.groupby(\"length_class\", sort=False):\n",
    "        n = min(len(sub), per_class_cap)\n",
    "        picks.append(sub.sample(n, random_state=RAND_SEED, replace=False))\n",
    "    return pd.concat(picks, ignore_index=True)\n",
    "\n",
    "def make_length_palette(classes: List[str]) -> Dict[str, Tuple[float, float, float]]:\n",
    "    vals = sorted([int(c) for c in classes])\n",
    "    cmap = sns.color_palette(\"rocket\", len(vals))\n",
    "    return {str(v): cmap[i] for i, v in enumerate(vals)}\n",
    "\n",
    "def run_length_from_col_pipeline():\n",
    "    # 1) Load token lists + length classes from existing column\n",
    "    df_sent, len_df = load_length_from_column(\n",
    "        CSV_PATH, length_max=LENGTH_MAX_CLASS, exclude_zero=EXCLUDE_ZERO_LENGTH\n",
    "    )\n",
    "    classes = sorted(len_df.length_class.unique(), key=lambda s: int(s))\n",
    "    palette = make_length_palette(classes)\n",
    "    print(f\"✓ corpus ready — {len(len_df):,} tokens across length classes {classes}\")\n",
    "\n",
    "    # 2) Optional per-class cap (currently unlimited for fast metrics)\n",
    "    raw_df = sample_raw(len_df, RAW_MAX_PER_CLASS)\n",
    "    print(\"Sample sizes per length (raw cap):\")\n",
    "    counts = raw_df.length_class.value_counts().sort_index()\n",
    "    print(counts.to_dict())\n",
    "\n",
    "    # 3) Embed once\n",
    "    reps, filled = embed_subset(df_sent, raw_df, BASELINE, WORD_REP_MODE, BATCH_SIZE)\n",
    "    raw_df = raw_df.reset_index(drop=True).loc[filled].reset_index(drop=True)\n",
    "    cls_arr = raw_df.length_class.values\n",
    "    L = reps.shape[0]; layers = np.arange(L)\n",
    "    print(f\"✓ embedded {len(raw_df):,} tokens  • layers={L}\")\n",
    "\n",
    "    # 4) Metric loop (FULL set)\n",
    "    for metric in ALL_METRICS:\n",
    "        print(f\"\\n→ Computing metric: {metric} …\")\n",
    "        compute_once = FAST_ONCE.get(metric) or HEAVY_ONCE.get(metric)\n",
    "        if compute_once is None:\n",
    "            print(f\"  (skipping {metric}: estimator unavailable)\")\n",
    "            continue\n",
    "\n",
    "        n_bs = N_BOOTSTRAP_FAST if metric in FAST_ONCE else N_BOOTSTRAP_HEAVY\n",
    "        Mcap = FAST_BS_MAX_SAMP_PER_CLASS if metric in FAST_ONCE else HEAVY_BS_MAX_SAMP_PER_CLASS\n",
    "\n",
    "        class_results: Dict[str, Dict[str, np.ndarray]] = {}\n",
    "        for c in classes:\n",
    "            idx = np.where(cls_arr == c)[0]\n",
    "            if idx.size < 3:\n",
    "                continue\n",
    "            sub = reps[:, idx]  # (L, n_c, D)\n",
    "            Nc = sub.shape[1]\n",
    "            M = min(Mcap, Nc)\n",
    "            mu, lo, hi = _bs_layer_loop(sub, M, n_bs, compute_once)\n",
    "            class_results[c] = {\"mean\": mu, \"lo\": lo, \"hi\": hi, \"n\": int(Nc)}\n",
    "\n",
    "        # Save + plot immediately\n",
    "        save_metric_csv_all_classes(metric, class_results, layers, BASELINE, subset_name=\"raw\")\n",
    "        plot_metric_with_ci(class_results, layers, metric,\n",
    "                            title=f\"{LABELS.get(metric, metric.upper())} • {BASELINE}\",\n",
    "                            out_path=PLOT_DIR / f\"length_raw_{metric}_{BASELINE}.png\",\n",
    "                            palette=palette)\n",
    "        print(f\"  ✓ saved: CSV= {CSV_DIR}/length_raw_{metric}_{BASELINE}.csv  \"\n",
    "              f\"plot= {PLOT_DIR}/length_raw_{metric}_{BASELINE}.png\")\n",
    "\n",
    "        del class_results; gc.collect()\n",
    "        if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    del reps; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    print(\"\\n✓ done (incremental outputs produced per metric).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_length_from_col_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f02068-1f44-4037-8826-ec0906c4c8cb",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c62102-4fde-46b8-ba92-611f2b7ea8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.graph_objects as go\n",
    "from plotly import colors as pc\n",
    "\n",
    "CSV_PATH      = \"en_ewt-ud-train_sentences.csv\"   # must include: sentence_id (str), tokens (list[str]), length (list[int])\n",
    "LENGTH_COL    = \"length\"                          # name of the per-sentence list[int] column\n",
    "BASELINE      = \"bert-base-uncased\"                            # \"gpt2\" or \"bert-base-uncased\" etc.\n",
    "WORD_REP_MODE = \"first\"                            # GPT-2: 'last' (or 'mean'); BERT: 'first' (or 'mean')\n",
    "\n",
    "# Plotting subsample (for browser smoothness)\n",
    "PLOT_MAX_PER_CLASS = None                         # max points per class used in PCA/plot (None = all)\n",
    "\n",
    "# Output\n",
    "OUT_DIR  = Path(\"pca3d_by_classes\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "HTML_OUT = OUT_DIR / f\"{BASELINE.replace('/','_')}_pca3d_by_length_classes.html\"\n",
    "\n",
    "# Repro + device\n",
    "RAND_SEED = 42\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Length classes 1..10 (with 10 = 10+). 0 or negative can be excluded.\n",
    "LENGTH_MAX_CLASS      = 10\n",
    "EXCLUDE_ZERO_LENGTH   = True\n",
    "\n",
    "# =============================== HELPERS ===============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _num_hidden_layers(model) -> int:\n",
    "    n = getattr(model.config, \"num_hidden_layers\", None)\n",
    "    if n is None: n = getattr(model.config, \"n_layer\", None)\n",
    "    if n is None: raise ValueError(\"Cannot determine num_hidden_layers from model.config\")\n",
    "    return int(n)\n",
    "\n",
    "def _hidden_size(model) -> int:\n",
    "    d = getattr(model.config, \"hidden_size\", None)\n",
    "    if d is None: d = getattr(model.config, \"n_embd\", None)\n",
    "    if d is None: raise ValueError(\"Cannot determine hidden_size from model.config\")\n",
    "    return int(d)\n",
    "\n",
    "def _load_tok_and_model(model_id: str):\n",
    "    \"\"\"\n",
    "    Robust loader (fast tokenizer for word_ids; pad-right; GPT-2 pad_token=eos when missing).\n",
    "    Tries 'openai-community/gpt2' if 'gpt2' fails.\n",
    "    \"\"\"\n",
    "    cands = [model_id]\n",
    "    if model_id.lower() in {\"gpt2\", \"gpt-2\"}:\n",
    "        cands += [\"openai-community/gpt2\"]\n",
    "\n",
    "    last_err = None\n",
    "    for mid in cands:\n",
    "        try:\n",
    "            tok = AutoTokenizer.from_pretrained(mid, use_fast=True, add_prefix_space=True)\n",
    "            if getattr(tok, \"padding_side\", None) != \"right\":\n",
    "                tok.padding_side = \"right\"\n",
    "            if tok.pad_token is None and getattr(tok, \"eos_token\", None) is not None:\n",
    "                tok.pad_token = tok.eos_token\n",
    "            mdl = AutoModel.from_pretrained(mid, output_hidden_states=True)\n",
    "            if getattr(mdl.config, \"pad_token_id\", None) is None and tok.pad_token_id is not None:\n",
    "                mdl.config.pad_token_id = tok.pad_token_id\n",
    "            if device == \"cuda\": mdl = mdl.half()\n",
    "            return tok, mdl.eval().to(device), mid\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    raise RuntimeError(f\"Failed to load model/tokenizer for {cands}: {last_err}\")\n",
    "\n",
    "def _pick_length_col(df: pd.DataFrame) -> str:\n",
    "    for cand in [LENGTH_COL, \"token_length\", \"lengths\", \"len\", \"LEN\", \"Length\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    raise ValueError(\"No length column found; set LENGTH_COL appropriately.\")\n",
    "\n",
    "def load_length_from_column(csv_path: str,\n",
    "                            length_max: int = LENGTH_MAX_CLASS,\n",
    "                            exclude_zero: bool = EXCLUDE_ZERO_LENGTH):\n",
    "    \"\"\"\n",
    "    CSV must have: sentence_id (str), tokens (list[str]), length (list[int]).\n",
    "    Emits token-level rows with 'length_class' in {1..length_max} (length_max means 10+ by default).\n",
    "    \"\"\"\n",
    "    df_all = pd.read_csv(csv_path)\n",
    "    len_col = _pick_length_col(df_all)\n",
    "    df = df_all[[\"sentence_id\",\"tokens\", len_col]].copy()\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df.tokens  = df.tokens.apply(_to_list)\n",
    "    df[len_col] = df[len_col].apply(_to_list)\n",
    "\n",
    "    rows = []\n",
    "    for sid, toks, lens in df[[\"sentence_id\",\"tokens\", len_col]].itertuples(index=False):\n",
    "        Ls = min(len(toks), len(lens))\n",
    "        for wid in range(Ls):\n",
    "            try:\n",
    "                k = int(lens[wid])\n",
    "            except Exception:\n",
    "                continue\n",
    "            if k <= 0 and exclude_zero:  # skip non-positive if requested\n",
    "                continue\n",
    "            if k < 0:\n",
    "                continue\n",
    "            k = min(max(k, 1), length_max)  # bucket upper tail into 'length_max'\n",
    "            rows.append((sid, wid, str(k), toks[wid]))\n",
    "    df_tok = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\",\"length_class\",\"word\"])\n",
    "    df_sent = df[[\"sentence_id\",\"tokens\"]].drop_duplicates(\"sentence_id\")\n",
    "    if df_tok.empty:\n",
    "        raise ValueError(\"No token rows constructed—check that your length column contains integer lists.\")\n",
    "    return df_sent, df_tok\n",
    "\n",
    "def sample_for_plot(df_tok: pd.DataFrame, per_class_cap: int | None = PLOT_MAX_PER_CLASS) -> pd.DataFrame:\n",
    "    \"\"\"Cap per class for visualization to keep Plotly responsive.\"\"\"\n",
    "    if per_class_cap is None:\n",
    "        return df_tok.sample(frac=1.0, random_state=RAND_SEED).reset_index(drop=True)\n",
    "    picks = []\n",
    "    for c, sub in df_tok.groupby(\"length_class\", sort=True):\n",
    "        n = min(len(sub), per_class_cap)\n",
    "        picks.append(sub.sample(n, random_state=RAND_SEED, replace=False))\n",
    "    return pd.concat(picks, ignore_index=True).sample(frac=1.0, random_state=RAND_SEED).reset_index(drop=True)\n",
    "\n",
    "def embed_subset(df_sent: pd.DataFrame,\n",
    "                 subset_df: pd.DataFrame,\n",
    "                 baseline: str = BASELINE,\n",
    "                 word_rep_mode: str = WORD_REP_MODE,\n",
    "                 batch_size: int = 4) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      reps   (L, N, D)\n",
    "      filled (N,) mask\n",
    "      words  list[str] length N (hover text)\n",
    "    \"\"\"\n",
    "    df_sent[\"sentence_id\"]   = df_sent[\"sentence_id\"].astype(str)\n",
    "    subset_df[\"sentence_id\"] = subset_df[\"sentence_id\"].astype(str)\n",
    "\n",
    "    # sid -> list[(global_idx, word_id)]\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(subset_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(str(sid), []).append((gidx, int(wid)))\n",
    "\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_sent[df_sent.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    tokzr, model, model_tag = _load_tok_and_model(baseline)\n",
    "\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "        enc_kwargs[\"add_prefix_space\"] = True\n",
    "\n",
    "    L = _num_hidden_layers(model) + 1\n",
    "    D = _hidden_size(model)\n",
    "    N = len(subset_df)\n",
    "\n",
    "    reps   = np.zeros((L, N, D), np.float16)\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "    words  = [\"\"] * N\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{model_tag} (embed subset)\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L,B,T,D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                mp: Dict[int, List[int]] = {}\n",
    "                wids = enc_be.word_ids(b)\n",
    "                if wids is None:\n",
    "                    raise RuntimeError(\"Fast tokenizer required (word_ids() unavailable).\")\n",
    "                for tidx, wid in enumerate(wids):\n",
    "                    if wid is not None:\n",
    "                        mp.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                toks_for_sent = df_sel.loc[sid, \"tokens\"]\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = mp.get(wid)\n",
    "                    if not toks: continue\n",
    "                    if word_rep_mode == \"first\":\n",
    "                        vec = h[:, b, toks[0], :]\n",
    "                    elif word_rep_mode == \"last\":\n",
    "                        vec = h[:, b, toks[-1], :]\n",
    "                    elif word_rep_mode == \"mean\":\n",
    "                        vec = h[:, b, toks, :].mean(axis=1)\n",
    "                    else:\n",
    "                        raise ValueError(\"WORD_REP_MODE must be {'first','last','mean'}\")\n",
    "                    reps[:, gidx, :] = vec.astype(np.float16, copy=False)\n",
    "                    words[gidx] = str(toks_for_sent[wid])\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    if (~filled).any():\n",
    "        missing = int((~filled).sum())\n",
    "        print(f\"⚠ Missing vectors for {missing} tokens (skipped in PCA).\")\n",
    "        reps   = reps[:, filled]\n",
    "        words  = [w for w, f in zip(words, filled) if f]\n",
    "    del model; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    return reps, filled, words\n",
    "\n",
    "def _make_class_palette(classes: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"Return a stable mapping class -> color (hex).\"\"\"\n",
    "    base = pc.qualitative.Plotly + pc.qualitative.Set2 + pc.qualitative.Set3 + pc.qualitative.Alphabet\n",
    "    colors = (base * ((len(classes) // len(base)) + 1))[:len(classes)]\n",
    "    def _to_hex(c):\n",
    "        if isinstance(c, str): return c\n",
    "        r, g, b = c\n",
    "        return f\"rgb({int(r*255)},{int(g*255)},{int(b*255)})\"\n",
    "    return {cls: _to_hex(colors[i]) for i, cls in enumerate(classes)}\n",
    "\n",
    "# =============================== PCA + PLOTLY (by class, per layer) ===============================\n",
    "def pca3d_per_layer_by_class(reps: np.ndarray,\n",
    "                             words: List[str],\n",
    "                             class_arr: np.ndarray,\n",
    "                             classes: List[str],\n",
    "                             model_tag: str):\n",
    "    \"\"\"\n",
    "    reps:      (L, N, D)\n",
    "    words:     list[str] length N (hover)\n",
    "    class_arr: shape (N,) with class labels as strings (e.g., \"1\",\"2\",...,\"10\")\n",
    "    classes:   sorted list of unique class labels (strings)\n",
    "    \"\"\"\n",
    "    L, N, D = reps.shape\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "\n",
    " \n",
    "\n",
    "    palette = _make_class_palette(classes)\n",
    "\n",
    "    # Pre-split indices by class for quick masking\n",
    "    cls_to_idx = {c: np.where(class_arr == c)[0] for c in classes}\n",
    "\n",
    "    Y_layers: List[np.ndarray] = []\n",
    "    for l in range(L):\n",
    "        X = reps[l].astype(np.float32, copy=False)\n",
    "        X = X - X.mean(0, keepdims=True)\n",
    "        pca = PCA(n_components=3, random_state=RAND_SEED)\n",
    "        Y = pca.fit_transform(X)  # (N, 3)\n",
    "        Y_layers.append(Y)\n",
    "\n",
    "    traces = []\n",
    "    for l in range(L):\n",
    "        Y = Y_layers[l]\n",
    "        for ci, c in enumerate(classes):\n",
    "            idx = cls_to_idx[c]\n",
    "            if idx.size == 0:\n",
    "                traces.append(go.Scatter3d(\n",
    "                    x=[], y=[], z=[], mode=\"markers\",\n",
    "                    marker=dict(size=2, opacity=0.75, color=palette[c]),\n",
    "                    text=[], name=f\"{c}\",\n",
    "                    legendgroup=c, showlegend=(l == 0), visible=(l == 0)\n",
    "                ))\n",
    "                continue\n",
    "\n",
    "            traces.append(\n",
    "                go.Scatter3d(\n",
    "                    x=Y[idx, 0], y=Y[idx, 1], z=Y[idx, 2],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=2, opacity=0.75, color=palette[c]),\n",
    "                    text=[f\"{words[i]} | len={c}\" for i in idx],\n",
    "                    hovertemplate=(\n",
    "                        \"<b>%{text}</b><br>\"\n",
    "                        \"PC1=%{x:.3f}<br>PC2=%{y:.3f}<br>PC3=%{z:.3f}\"\n",
    "                        f\"<extra>Layer {l} • class {c}</extra>\"\n",
    "                    ),\n",
    "                    name=f\"{c}\",\n",
    "                    legendgroup=c,         \n",
    "                    showlegend=(l == 0),    \n",
    "\n",
    "                )\n",
    "            )\n",
    "\n",
    "    steps = []\n",
    "    traces_per_layer = len(classes)\n",
    "    total_traces = L * traces_per_layer\n",
    "    for l in range(L):\n",
    "        vis = [False] * total_traces\n",
    "        start = l * traces_per_layer\n",
    "        for k in range(traces_per_layer):\n",
    "            vis[start + k] = True\n",
    "        steps.append(dict(\n",
    "            method=\"update\",\n",
    "            args=[{\"visible\": vis},\n",
    "                  {\"title\": f\"{model_tag} • PCA 3D by length class • layer {l} (drag to rotate)\"}],\n",
    "            label=str(l),\n",
    "        ))\n",
    "\n",
    "    sliders = [dict(\n",
    "        active=0,\n",
    "        steps=steps,\n",
    "        currentvalue={\"prefix\": \"Layer: \"},\n",
    "        pad={\"t\": 10}\n",
    "    )]\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title=f\"{model_tag} • PCA 3D by length class • layer 0 (drag to rotate)\",\n",
    "        scene=dict(\n",
    "            xaxis_title=\"PC1\", yaxis_title=\"PC2\", zaxis_title=\"PC3\",\n",
    "            aspectmode=\"data\",\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=40),\n",
    "        sliders=sliders,\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "    fig.show()\n",
    "    fig.write_html(str(HTML_OUT), include_plotlyjs=\"cdn\")\n",
    "    print(\"✓ Saved interactive HTML to:\", HTML_OUT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_pca3d_by_length_classes():\n",
    "    # 1) Load tokens + length classes\n",
    "    df_sent, df_tok = load_length_from_column(CSV_PATH, length_max=LENGTH_MAX_CLASS, exclude_zero=EXCLUDE_ZERO_LENGTH)\n",
    "    classes = sorted(df_tok.length_class.unique(), key=lambda s: int(s))\n",
    "    print(f\"✓ corpus ready — {len(df_tok):,} tokens across length classes {classes}\")\n",
    "\n",
    "    plot_df = sample_for_plot(df_tok, PLOT_MAX_PER_CLASS)\n",
    "    print(\"Plot sample sizes per length (cap):\")\n",
    "    print(plot_df.length_class.value_counts().sort_index().to_dict())\n",
    "\n",
    "    reps, filled, words = embed_subset(df_sent, plot_df, BASELINE, WORD_REP_MODE, batch_size=4)\n",
    "    plot_df = plot_df.reset_index(drop=True).loc[filled].reset_index(drop=True)\n",
    "    class_arr = plot_df.length_class.values\n",
    "    L = reps.shape[0]\n",
    "    print(f\"✓ embedded {len(plot_df):,} tokens  • layers={L}\")\n",
    "\n",
    "    # 4) PCA 3D + Plotly\n",
    "    pca3d_per_layer_by_class(reps, words, class_arr, classes, model_tag=BASELINE)\n",
    "\n",
    "    # Cleanup\n",
    "    del reps; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pca3d_by_length_classes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e07c99b-bc07-4180-ae4f-2ade41391220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
