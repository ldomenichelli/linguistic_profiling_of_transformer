{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "881f50be-7db4-4e6c-b1c5-9c4a1d127fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Total tokens: 253,771\n",
      "\n",
      "=== Embedding all tokens • bert-base-uncased (auto first/last subtoken) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_177456/3235306246.py:202: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Subtoken policy for bert-base-uncased: first\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-base-uncased:first (embed→memmap): 100%|█| 12112/12112 [01:52<00:00, 107.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Missing vectors for 27 tokens (skipped).\n",
      "Shapes: layers=13, tokens=253771, dim=768\n",
      "bert-base-uncased: 3σ thresholds = [-1.187926, 1.139241]  • last-layer #OD=2\n",
      "\n",
      "=== Embedding all tokens • gpt2 (auto first/last subtoken) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_177456/3235306246.py:202: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Subtoken policy for openai-community/gpt2: last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gpt2:last (embed→memmap): 100%|███████████| 12112/12112 [02:08<00:00, 94.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: layers=13, tokens=253771, dim=768\n",
      "gpt2: 3σ thresholds = [-13.937875, 14.172828]  • last-layer #OD=4\n",
      "✓ wrote outlier_dims_all_tokens_3sigma/last_layer_means_both_3sigma.pdf\n",
      "✓ wrote outlier_dims_all_tokens_3sigma/od_counts_per_layer_both_3sigma_prev_bubbles.pdf\n",
      "✓ done. See: /home/ldomenichelli/geometric_profiling_of_a_neural_language_model/outlier_dims_all_tokens_3sigma\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ============================ CONFIG ============================\n",
    "CSV_PATH   = \"it_isdt-ud-train_sentences.csv\"      # must have: sentence_id (str), tokens (list[str])\n",
    "MODELS     = [\"bert-base-uncased\", \"gpt2\"]         # both models\n",
    "BATCH_SIZE = 1\n",
    "RAND_SEED  = 42\n",
    "\n",
    "# Outlier detection: using 3-sigma criterion on per-dimension MEANS\n",
    "USE_ABS_MEAN     = False      # if True, use mean(|x|); else mean(x)\n",
    "OUTLIER_SIGMAS   = 3          # number of standard deviations from mean to define outlier\n",
    "\n",
    "# Optional: cap points to accelerate embedding (None = all)\n",
    "TOKEN_CAP: int | None = None\n",
    "\n",
    "# Robust loading: work offline if models are cached\n",
    "LOCAL_ONLY = (\n",
    "    os.environ.get(\"TRANSFORMERS_OFFLINE\") == \"1\"\n",
    "    or os.environ.get(\"HF_HUB_OFFLINE\") == \"1\"\n",
    ")\n",
    "\n",
    "OUT_DIR = Path(\"outlier_dims_all_tokens_3sigma\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": torch.backends.cudnn.benchmark = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# ============================ LEGEND STYLE (SAME AS BEFORE) ============================\n",
    "# Legend-only font + compact spacing (does NOT change axis/title fonts from seaborn context)\n",
    "LEGEND_FP = FontProperties(\n",
    "    family=\"DejaVu Sans Mono\",  # cambia a \"DejaVu Sans\" / \"serif\" si quieres\n",
    "    size=10                     # tamaño de la leyenda (independiente de sns font_scale)\n",
    ")\n",
    "\n",
    "LEGEND_KW = dict(\n",
    "    prop=LEGEND_FP,\n",
    "    frameon=False,\n",
    "    borderpad=0.20,\n",
    "    labelspacing=0.25,\n",
    "    handlelength=1.15,\n",
    "    handletextpad=0.40,\n",
    "    borderaxespad=0.25,\n",
    "    markerscale=0.85,\n",
    ")\n",
    "\n",
    "def small_legend(ax, **kwargs):\n",
    "    \"\"\"Apply compact legend styling on a given axis.\"\"\"\n",
    "    kw = dict(LEGEND_KW)\n",
    "    kw.update(kwargs)\n",
    "    return ax.legend(**kw)\n",
    "\n",
    "# First figure y-axis clip: keep the axis small but still show the thresholds\n",
    "FIRST_FIG_Y_CLIP_QUANTILE = 0.995  # use 99.5th percentile to clip y-axis\n",
    "FIRST_FIG_TOP_PAD = 1.05           # padding factor above chosen limits\n",
    "\n",
    "# Bubble sizes (second figure)\n",
    "DOT_SIZE_MIN = 30.0\n",
    "DOT_SIZE_MAX = 900.0\n",
    "\n",
    "# ============================ HELPERS ============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _num_hidden_layers(model) -> int:\n",
    "    n = getattr(model.config, \"num_hidden_layers\", None)\n",
    "    if n is None:\n",
    "        n = getattr(model.config, \"n_layer\", None)\n",
    "    if n is None:\n",
    "        raise ValueError(\"Cannot determine number of hidden layers\")\n",
    "    return int(n)\n",
    "\n",
    "def _hidden_size(model) -> int:\n",
    "    d = getattr(model.config, \"hidden_size\", None)\n",
    "    if d is None:\n",
    "        d = getattr(model.config, \"n_embd\", None)\n",
    "    if d is None:\n",
    "        raise ValueError(\"Cannot determine hidden size\")\n",
    "    return int(d)\n",
    "\n",
    "def _subtoken_policy(model, resolved_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Decide whether to take the FIRST or LAST subtoken per word.\n",
    "    - encoder/MLM families -> 'first'\n",
    "    - decoder/causal families -> 'last'\n",
    "    \"\"\"\n",
    "    t = (getattr(model.config, \"model_type\", \"\") or \"\").lower()\n",
    "    enc_first = {\"bert\", \"roberta\", \"albert\", \"electra\", \"distilbert\", \"deberta\", \"deberta-v2\", \"camembert\"}\n",
    "    dec_last  = {\"gpt2\", \"gpt_neo\", \"gpt_neox\", \"gptj\", \"bloom\", \"opt\", \"llama\", \"falcon\", \"mistral\", \"xglm\", \"replit\"}\n",
    "    if t in dec_last or \"gpt\" in resolved_id.lower():\n",
    "        return \"last\"\n",
    "    if t in enc_first:\n",
    "        return \"first\"\n",
    "    return \"first\"\n",
    "\n",
    "def _load_tok_and_model(model_id: str):\n",
    "    \"\"\"Fast tokenizer (word_ids) + model; robust for GPT‑2; offline‑friendly.\"\"\"\n",
    "    tried: List[Tuple[str, str]] = []\n",
    "\n",
    "    def _try(mid: str):\n",
    "        tok = AutoTokenizer.from_pretrained(mid, use_fast=True, add_prefix_space=True, local_files_only=LOCAL_ONLY)\n",
    "        if getattr(tok, \"padding_side\", None) != \"right\":\n",
    "            tok.padding_side = \"right\"\n",
    "        if tok.pad_token is None and getattr(tok, \"eos_token\", None) is not None:\n",
    "            tok.pad_token = tok.eos_token  # GPT‑2 has no pad_token by default\n",
    "        mdl = AutoModel.from_pretrained(mid, output_hidden_states=True, local_files_only=LOCAL_ONLY)\n",
    "        if getattr(mdl.config, \"pad_token_id\", None) is None and tok.pad_token_id is not None:\n",
    "            mdl.config.pad_token_id = tok.pad_token_id\n",
    "        return tok, mdl\n",
    "\n",
    "    order = [model_id]\n",
    "    if model_id.lower() in {\"gpt2\", \"gpt-2\"}:\n",
    "        order += [\"openai-community/gpt2\", \"distilgpt2\"]\n",
    "\n",
    "    last_err = None\n",
    "    for mid in order:\n",
    "        try:\n",
    "            tok, mdl = _try(mid)\n",
    "            mdl = mdl.eval().to(device)\n",
    "            if device == \"cuda\": mdl.half()\n",
    "            return tok, mdl, mid\n",
    "        except Exception as e:\n",
    "            tried.append((mid, repr(e))); last_err = e\n",
    "\n",
    "    raise RuntimeError(\n",
    "        \"Could not load tokenizer/model. Attempts:\\n\" +\n",
    "        \"\\n\".join(f\" - {m}: {err}\" for m, err in tried)\n",
    "    ) from last_err\n",
    "\n",
    "def load_token_index(csv_path: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Return (df_all_sentences, token_index_df) for ALL tokens (no POS filtering).\"\"\"\n",
    "    df = pd.read_csv(csv_path, usecols=[\"sentence_id\",\"tokens\"])\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df.tokens = df.tokens.apply(_to_list)\n",
    "\n",
    "    rows = []\n",
    "    for sid, toks in df[[\"sentence_id\",\"tokens\"]].itertuples(index=False):\n",
    "        for wid in range(len(toks)):\n",
    "            rows.append((sid, wid))\n",
    "    word_df = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\"])\n",
    "\n",
    "    if TOKEN_CAP is not None and len(word_df) > TOKEN_CAP:\n",
    "        word_df = word_df.sample(TOKEN_CAP, random_state=RAND_SEED).reset_index(drop=True)\n",
    "    return df, word_df\n",
    "\n",
    "def _create_layer_memmaps(L: int, N: int, D: int, base_dir: Path, tag: str) -> tuple[list[Path], list[np.memmap]]:\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "    files, mms = [], []\n",
    "    for l in range(L):\n",
    "        fn = base_dir / f\"{tag}_layer{l:02d}.mmap\"\n",
    "        mm = np.memmap(fn, dtype=\"float32\", mode=\"w+\", shape=(N, D))\n",
    "        files.append(fn); mms.append(mm)\n",
    "    return files, mms\n",
    "\n",
    "def embed_to_memmaps(df_all: pd.DataFrame, token_df: pd.DataFrame, model_id: str,\n",
    "                     batch_size: int = 1, out_dir: Path | None = None) -> tuple[list[Path], int, int, int, str]:\n",
    "    \"\"\"\n",
    "    Embed ALL tokens and spill per-layer (N,D) memmaps.\n",
    "    BERT-like -> use FIRST subtoken; GPT-like -> use LAST subtoken.\n",
    "    \"\"\"\n",
    "    tokzr, model, resolved_id = _load_tok_and_model(model_id)\n",
    "    policy = _subtoken_policy(model, resolved_id)  # 'first' or 'last'\n",
    "    print(f\"• Subtoken policy for {resolved_id}: {policy}\")\n",
    "\n",
    "    token_df = token_df.copy()\n",
    "    token_df[\"sentence_id\"] = token_df[\"sentence_id\"].astype(str)\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(token_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(sid, []).append((gidx, int(wid)))\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_all[df_all.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    L = _num_hidden_layers(model) + 1\n",
    "    D = _hidden_size(model)\n",
    "    N = len(token_df)\n",
    "\n",
    "    tag = resolved_id.split(\"/\")[-1]\n",
    "    store_dir = (OUT_DIR / f\"{tag}_memmaps\") if out_dir is None else out_dir\n",
    "    fns, mms = _create_layer_memmaps(L, N, D, store_dir, f\"{tag}_{policy}\")\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "        enc_kwargs[\"add_prefix_space\"] = True\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{tag}:{policy} (embed→memmap)\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L, B, T, D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                mp: Dict[int, List[int]] = {}\n",
    "                wids = enc_be.word_ids(b)\n",
    "                if wids is None:\n",
    "                    raise RuntimeError(\"Fast tokenizer required (word_ids() unavailable).\")\n",
    "                for tidx, wid in enumerate(wids):\n",
    "                    if wid is not None:\n",
    "                        mp.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = mp.get(wid)\n",
    "                    if not toks:\n",
    "                        continue\n",
    "                    if policy == \"first\":\n",
    "                        vec = h[:, b, toks[0], :]      # (L, D)\n",
    "                    else:  # \"last\"\n",
    "                        vec = h[:, b, toks[-1], :]     # (L, D)\n",
    "                    for l in range(L):\n",
    "                        mms[l][gidx, :] = vec[l]\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    if (~filled).any():\n",
    "        print(f\"⚠ Missing vectors for {int((~filled).sum())} tokens (skipped).\")\n",
    "\n",
    "    for mm in mms:\n",
    "        mm.flush()\n",
    "        del mm\n",
    "    gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    return fns, L, N, D, tag\n",
    "\n",
    "# ---------- MEAN statistics ----------\n",
    "def per_layer_dim_means(memmap_paths: List[Path], L: int, N: int, D: int, use_abs: bool = True) -> np.ndarray:\n",
    "    \"\"\"Return per-layer, per-dim MEANS: shape (L, D).\"\"\"\n",
    "    means = np.zeros((L, D), dtype=np.float32)\n",
    "    for l in range(L):\n",
    "        mm = np.memmap(memmap_paths[l], dtype=\"float32\", mode=\"r\", shape=(N, D))\n",
    "        X = np.array(mm, copy=False)\n",
    "        if use_abs:\n",
    "            means[l] = np.mean(np.abs(X), axis=0)\n",
    "        else:\n",
    "            means[l] = np.mean(X, axis=0)\n",
    "        del mm; gc.collect()\n",
    "    return means\n",
    "\n",
    "def counts_prev_overlap_and_magnitude_3sigma(vals: np.ndarray, n_sigmas: float = 3.0\n",
    "                                             ) -> tuple[np.ndarray, np.ndarray, float, float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    vals: (L,D) array of per‑dimension statistics (means here).\n",
    "    n_sigmas: number of standard deviations for threshold.\n",
    "    Returns:\n",
    "      counts        (L,)   : # of outlier dims in each layer (≥ μ+σ*n or ≤ μ−σ*n)\n",
    "      overlaps_prev (L,)   : # outlier dims overlapping with previous layer (0 at layer 0)\n",
    "      thr_plus      (float): + threshold (global mean + nσ)\n",
    "      thr_minus     (float): − threshold (global mean − nσ)\n",
    "      magnitude     (L,)   : average exceedance magnitude for outlier dims in each layer\n",
    "    \"\"\"\n",
    "    L, D = vals.shape\n",
    "    flat = vals.reshape(-1)\n",
    "    mu   = float(flat.mean())\n",
    "    sigma= float(flat.std(ddof=0))\n",
    "    thr_plus  = mu + n_sigmas * sigma\n",
    "    thr_minus = mu - n_sigmas * sigma\n",
    "\n",
    "    od_sets = []\n",
    "    counts   = np.zeros(L, dtype=np.int32)\n",
    "    overlaps_prev = np.zeros(L, dtype=np.int32)\n",
    "    magnitude = np.zeros(L, dtype=np.float32)\n",
    "\n",
    "    for l in range(L):\n",
    "        mask = (vals[l] >= thr_plus) | (vals[l] <= thr_minus)\n",
    "        idx = np.where(mask)[0]\n",
    "        idx_set = set(idx)\n",
    "        od_sets.append(idx_set)\n",
    "        counts[l] = len(idx)\n",
    "\n",
    "        if len(idx) > 0:\n",
    "            above = vals[l, idx] - thr_plus\n",
    "            below = thr_minus - vals[l, idx]\n",
    "            exc = np.where(vals[l, idx] >= thr_plus, above, below)\n",
    "            magnitude[l] = float(np.mean(exc))\n",
    "        else:\n",
    "            magnitude[l] = 0.0\n",
    "\n",
    "        if l > 0:\n",
    "            overlaps_prev[l] = len(idx_set.intersection(od_sets[l-1]))\n",
    "\n",
    "    return counts, overlaps_prev, thr_plus, thr_minus, magnitude\n",
    "\n",
    "# ============================ PLOTTING ============================\n",
    "def make_plots(model_to_stats: Dict[str, dict], out_dir: Path = OUT_DIR):\n",
    "    \"\"\"\n",
    "    model_to_stats[model_tag] = {\n",
    "        'means': (L,D),\n",
    "        'thr_plus': float,\n",
    "        'thr_minus': float,\n",
    "        'counts': (L,),\n",
    "        'overlaps_prev': (L,),\n",
    "        'mag': (L,)  # average exceedance per layer\n",
    "    }\n",
    "    \"\"\"\n",
    "    # ---------- Left: last-layer means for each model ----------\n",
    "    fig, axes = plt.subplots(1, len(model_to_stats), figsize=(6.6 * len(model_to_stats), 4.6), sharey=True)\n",
    "    if len(model_to_stats) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (tag, st) in zip(axes, model_to_stats.items()):\n",
    "        means = st[\"means\"]; thr_plus = st[\"thr_plus\"]; thr_minus = st[\"thr_minus\"]\n",
    "        last_vals = means[-1]\n",
    "        ax.scatter(np.arange(last_vals.size), last_vals, s=8, alpha=0.6, edgecolor=\"none\")\n",
    "        ax.axhline(thr_plus,  color=\"orange\", linestyle=\"--\", linewidth=1.5, label=f\"μ+{OUTLIER_SIGMAS}σ\")\n",
    "        ax.axhline(thr_minus, color=\"orange\", linestyle=\"--\", linewidth=1.5, label=f\"μ−{OUTLIER_SIGMAS}σ\")\n",
    "\n",
    "        # (Tu código original mantiene ylim fijo)\n",
    "        ax.set_ylim(-80, 80)\n",
    "\n",
    "        ax.set_title(f\"{tag} • last layer\")\n",
    "        ax.set_xlabel(\"dimension\")\n",
    "        ax.set_ylabel(\"mean(|activation|)\" if USE_ABS_MEAN else \"mean(activation)\")\n",
    "\n",
    "        # ✅ SAME legend styling (small + different font)\n",
    "        small_legend(ax, loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_png1 = out_dir / \"last_layer_means_both_3sigma.pdf\"\n",
    "    plt.savefig(out_png1, dpi=220)\n",
    "    plt.close(fig)\n",
    "    print(\"✓ wrote\", out_png1)\n",
    "\n",
    "    # ---------- Right: counts & overlaps-with-previous + bubble magnitude ----------\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    sns.set_context(\"paper\", font_scale=2.5)\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    ax2 = plt.gca()\n",
    "\n",
    "    # For bubble scaling, normalize across all models/layers\n",
    "    all_mag = np.concatenate([st[\"mag\"] for st in model_to_stats.values()]) if len(model_to_stats) > 0 else np.array([0.0])\n",
    "    mag_min, mag_max = float(all_mag.min()), float(all_mag.max())\n",
    "    mag_span = max(mag_max - mag_min, 1e-12)\n",
    "\n",
    "    for tag, st in model_to_stats.items():\n",
    "        L = st[\"means\"].shape[0]\n",
    "        layers = np.arange(L)\n",
    "\n",
    "        ax2.plot(layers, st[\"counts\"], marker=\"o\", linewidth=2.5, alpha=0.95, label=f\"{tag}: #ODs\")\n",
    "        ax2.plot(layers, st[\"overlaps_prev\"], marker=\"s\", linestyle=\"--\", linewidth=2.5, alpha=0.95,\n",
    "                 label=f\"{tag}: #OD ∩ prev\")\n",
    "\n",
    "        mags  = st[\"mag\"]\n",
    "        sizes = DOT_SIZE_MIN + (DOT_SIZE_MAX - DOT_SIZE_MIN) * ((mags - mag_min) / mag_span)\n",
    "        ax2.scatter(layers, st[\"counts\"], s=sizes, alpha=0.35, edgecolor=\"none\")\n",
    "\n",
    "    ax2.set_xlabel(\"layer\")\n",
    "    ax2.set_ylabel(\"# outlier dims\")\n",
    "\n",
    "    # ✅ SAME legend styling (small + different font)\n",
    "    small_legend(ax2, ncol=2, loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_png2 = out_dir / \"od_counts_per_layer_both_3sigma_prev_bubbles.pdf\"\n",
    "    plt.savefig(out_png2, dpi=220)\n",
    "    plt.close()\n",
    "    print(\"✓ wrote\", out_png2)\n",
    "\n",
    "# ============================ DRIVER ============================\n",
    "def run():\n",
    "    df_all, token_df = load_token_index(CSV_PATH)\n",
    "    print(f\"✓ Total tokens: {len(token_df):,}\")\n",
    "\n",
    "    model_to_stats: Dict[str, dict] = {}\n",
    "    for model_id in MODELS:\n",
    "        print(f\"\\n=== Embedding all tokens • {model_id} (auto first/last subtoken) ===\")\n",
    "        mmap_paths, L, N, D, tag = embed_to_memmaps(df_all, token_df, model_id, batch_size=BATCH_SIZE)\n",
    "        print(f\"Shapes: layers={L}, tokens={N}, dim={D}\")\n",
    "\n",
    "        means = per_layer_dim_means(mmap_paths, L, N, D, use_abs=USE_ABS_MEAN)\n",
    "        counts, overlaps_prev, thr_plus, thr_minus, magnitudes = counts_prev_overlap_and_magnitude_3sigma(means, OUTLIER_SIGMAS)\n",
    "        print(f\"{tag}: 3σ thresholds = [{thr_minus:.6f}, {thr_plus:.6f}]  • last-layer #OD={counts[-1]}\")\n",
    "\n",
    "        model_to_stats[tag] = {\n",
    "            \"means\": means,\n",
    "            \"counts\": counts,\n",
    "            \"overlaps_prev\": overlaps_prev,\n",
    "            \"thr_plus\": thr_plus,\n",
    "            \"thr_minus\": thr_minus,\n",
    "            \"mag\": magnitudes\n",
    "        }\n",
    "\n",
    "        # Clean up memmaps (optional)\n",
    "        for p in mmap_paths:\n",
    "            try: os.remove(p)\n",
    "            except Exception: pass\n",
    "\n",
    "    make_plots(model_to_stats, OUT_DIR)\n",
    "    print(\"✓ done. See:\", OUT_DIR.resolve())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03edfc37-6011-42cf-a3a1-16dd3cdd4d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ============================ CONFIG ============================\n",
    "CSV_PATH   = \"it_isdt-ud-train_sentences.csv\"      # must contain: sentence_id(str), tokens(list[str])\n",
    "MODELS     = [\"bert-base-uncased\", \"gpt2\"]         # compare both\n",
    "BATCH_SIZE = 1\n",
    "RAND_SEED  = 42\n",
    "\n",
    "# Outlier rule: global quantile over all (layer,dim) medians\n",
    "USE_ABS_MEDIAN      = True     # use median(|x|) if True, else median(x)\n",
    "OUTLIER_QUANTILE    = 0.99     # e.g., top 1% across all layers & dims\n",
    "\n",
    "# First plot y-axis compression (clip to this quantile of last-layer medians)\n",
    "FIRST_PLOT_Y_QUANT  = 0.995\n",
    "\n",
    "# Bubble sizes for second plot (scaled from mean exceedance)\n",
    "DOT_SIZE_MIN        = 30.0\n",
    "DOT_SIZE_MAX        = 600.0\n",
    "\n",
    "# Optional: cap tokens to speed up embedding (None = all)\n",
    "TOKEN_CAP: int | None = None\n",
    "\n",
    "# Offline-friendly load if models are cached\n",
    "LOCAL_ONLY = (\n",
    "    os.environ.get(\"TRANSFORMERS_OFFLINE\") == \"1\"\n",
    "    or os.environ.get(\"HF_HUB_OFFLINE\") == \"1\"\n",
    ")\n",
    "\n",
    "OUT_DIR = Path(\"outlier_dims_all_tokens_quantile\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": torch.backends.cudnn.benchmark = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# ============================ HELPERS ============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _num_hidden_layers(model) -> int:\n",
    "    n = getattr(model.config, \"num_hidden_layers\", None)\n",
    "    if n is None: n = getattr(model.config, \"n_layer\", None)\n",
    "    if n is None: raise ValueError(\"Cannot determine number of hidden layers\")\n",
    "    return int(n)\n",
    "\n",
    "def _hidden_size(model) -> int:\n",
    "    d = getattr(model.config, \"hidden_size\", None)\n",
    "    if d is None: d = getattr(model.config, \"n_embd\", None)\n",
    "    if d is None: raise ValueError(\"Cannot determine hidden size\")\n",
    "    return int(d)\n",
    "\n",
    "def _subtoken_policy(model, resolved_id: str) -> str:\n",
    "    \"\"\"\n",
    "    FIRST subtoken for encoder/MLM families; LAST for decoder/causal families.\n",
    "    \"\"\"\n",
    "    t = (getattr(model.config, \"model_type\", \"\") or \"\").lower()\n",
    "    enc_first = {\"bert\", \"roberta\", \"albert\", \"electra\", \"distilbert\", \"deberta\", \"deberta-v2\", \"camembert\"}\n",
    "    dec_last  = {\"gpt2\", \"gpt_neo\", \"gpt_neox\", \"gptj\", \"bloom\", \"opt\", \"llama\", \"falcon\", \"mistral\", \"xglm\", \"replit\"}\n",
    "    if t in dec_last or \"gpt\" in resolved_id.lower():  # GPT-like → last piece\n",
    "        return \"last\"\n",
    "    if t in enc_first:                                  # BERT-like → first piece\n",
    "        return \"first\"\n",
    "    return \"first\"\n",
    "\n",
    "def _load_tok_and_model(model_id: str):\n",
    "    \"\"\"Fast tokenizer (word_ids) + model; robust for GPT‑2; offline‑friendly.\"\"\"\n",
    "    tried: List[Tuple[str, str]] = []\n",
    "\n",
    "    def _try(mid: str):\n",
    "        tok = AutoTokenizer.from_pretrained(mid, use_fast=True, add_prefix_space=True, local_files_only=LOCAL_ONLY)\n",
    "        if getattr(tok, \"padding_side\", None) != \"right\":\n",
    "            tok.padding_side = \"right\"\n",
    "        if tok.pad_token is None and getattr(tok, \"eos_token\", None) is not None:\n",
    "            tok.pad_token = tok.eos_token  # GPT‑2 has no pad_token by default\n",
    "        mdl = AutoModel.from_pretrained(mid, output_hidden_states=True, local_files_only=LOCAL_ONLY)\n",
    "        if getattr(mdl.config, \"pad_token_id\", None) is None and tok.pad_token_id is not None:\n",
    "            mdl.config.pad_token_id = tok.pad_token_id\n",
    "        return tok, mdl\n",
    "\n",
    "    order = [model_id]\n",
    "    if model_id.lower() in {\"gpt2\", \"gpt-2\"}:\n",
    "        order += [\"openai-community/gpt2\", \"distilgpt2\"]\n",
    "\n",
    "    last_err = None\n",
    "    for mid in order:\n",
    "        try:\n",
    "            tok, mdl = _try(mid)\n",
    "            mdl = mdl.eval().to(device)\n",
    "            if device == \"cuda\": mdl.half()\n",
    "            return tok, mdl, mid\n",
    "        except Exception as e:\n",
    "            tried.append((mid, repr(e))); last_err = e\n",
    "\n",
    "    raise RuntimeError(\"Could not load tokenizer/model. Attempts:\\n\" + \"\\n\".join(f\" - {m}: {err}\" for m, err in tried)) from last_err\n",
    "\n",
    "def load_token_index(csv_path: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Return (df_all_sentences, token_index_df) for ALL tokens.\"\"\"\n",
    "    df = pd.read_csv(csv_path, usecols=[\"sentence_id\",\"tokens\"])\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df.tokens = df.tokens.apply(_to_list)\n",
    "\n",
    "    rows = []\n",
    "    for sid, toks in df[[\"sentence_id\",\"tokens\"]].itertuples(index=False):\n",
    "        for wid in range(len(toks)):\n",
    "            rows.append((sid, wid))\n",
    "    word_df = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\"])\n",
    "\n",
    "    if TOKEN_CAP is not None and len(word_df) > TOKEN_CAP:\n",
    "        word_df = word_df.sample(TOKEN_CAP, random_state=RAND_SEED).reset_index(drop=True)\n",
    "    return df, word_df\n",
    "\n",
    "def _create_layer_memmaps(L: int, N: int, D: int, base_dir: Path, tag: str) -> tuple[list[Path], list[np.memmap]]:\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "    files, mms = [], []\n",
    "    for l in range(L):\n",
    "        fn = base_dir / f\"{tag}_layer{l:02d}.mmap\"\n",
    "        mm = np.memmap(fn, dtype=\"float32\", mode=\"w+\", shape=(N, D))\n",
    "        files.append(fn); mms.append(mm)\n",
    "    return files, mms\n",
    "\n",
    "def embed_to_memmaps(df_all: pd.DataFrame, token_df: pd.DataFrame, model_id: str,\n",
    "                     batch_size: int = 1, out_dir: Path | None = None) -> tuple[list[Path], int, int, int, str]:\n",
    "    \"\"\"\n",
    "    Embed ALL tokens and spill per-layer (N,D) memmaps.\n",
    "    BERT-like -> FIRST subtoken; GPT-like -> LAST subtoken.\n",
    "    \"\"\"\n",
    "    tokzr, model, resolved_id = _load_tok_and_model(model_id)\n",
    "    policy = _subtoken_policy(model, resolved_id)\n",
    "    print(f\"• Subtoken policy for {resolved_id}: {policy}\")\n",
    "\n",
    "    token_df = token_df.copy()\n",
    "    token_df[\"sentence_id\"] = token_df[\"sentence_id\"].astype(str)\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(token_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(sid, []).append((gidx, int(wid)))\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_all[df_all.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    L = _num_hidden_layers(model) + 1\n",
    "    D = _hidden_size(model)\n",
    "    N = len(token_df)\n",
    "\n",
    "    tag = resolved_id.split(\"/\")[-1]\n",
    "    store_dir = (OUT_DIR / f\"{tag}_memmaps\") if out_dir is None else out_dir\n",
    "    fns, mms = _create_layer_memmaps(L, N, D, store_dir, f\"{tag}_{policy}\")\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "        enc_kwargs[\"add_prefix_space\"] = True\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{tag}:{policy} (embed→memmap)\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L, B, T, D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                mp: Dict[int, List[int]] = {}\n",
    "                wids = enc_be.word_ids(b)\n",
    "                if wids is None:\n",
    "                    raise RuntimeError(\"Fast tokenizer required (word_ids() unavailable).\")\n",
    "                for tidx, wid in enumerate(wids):\n",
    "                    if wid is not None:\n",
    "                        mp.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = mp.get(wid)\n",
    "                    if not toks:\n",
    "                        continue\n",
    "                    if policy == \"first\":\n",
    "                        vec = h[:, b, toks[0], :]      # (L, D)\n",
    "                    else:  # \"last\"\n",
    "                        vec = h[:, b, toks[-1], :]     # (L, D)\n",
    "                    for l in range(L):\n",
    "                        mms[l][gidx, :] = vec[l]\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    if (~filled).any():\n",
    "        print(f\"⚠ Missing vectors for {int((~filled).sum())} tokens (skipped).\")\n",
    "\n",
    "    for mm in mms:\n",
    "        mm.flush()\n",
    "        del mm\n",
    "    gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    return fns, L, N, D, tag\n",
    "\n",
    "# ---------- per-layer, per-dim statistics (MEDIAN) ----------\n",
    "def per_layer_dim_medians(memmap_paths: List[Path], L: int, N: int, D: int, use_abs: bool = True) -> np.ndarray:\n",
    "    \"\"\"Return medians per layer & dim: shape (L, D).\"\"\"\n",
    "    meds = np.zeros((L, D), dtype=np.float32)\n",
    "    for l in range(L):\n",
    "        mm = np.memmap(memmap_paths[l], dtype=\"float32\", mode=\"r\", shape=(N, D))\n",
    "        X = np.array(mm, copy=False)\n",
    "        meds[l] = np.median(np.abs(X), axis=0) if use_abs else np.median(X, axis=0)\n",
    "        del mm; gc.collect()\n",
    "    return meds\n",
    "\n",
    "# ---------- outlier counting + prev-layer overlap + magnitude (exceedance) ----------\n",
    "def counts_prev_overlap_and_magnitude_quantile(meds: np.ndarray, q: float) -> tuple[np.ndarray, np.ndarray, float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    meds: (L, D) per-dim medians.\n",
    "    q: global quantile in [0,1].\n",
    "    Returns:\n",
    "        counts        (L,)  : # of outlier dimensions at each layer\n",
    "        overlaps_prev (L,)  : |OD_l ∩ OD_{l-1}| (with 0 for l=0)\n",
    "        thr           float : global quantile threshold\n",
    "        magnitude     (L,)  : mean exceedance of outliers at each layer, i.e. mean( med[l, idx] - thr )\n",
    "    \"\"\"\n",
    "    L, D = meds.shape\n",
    "    flat  = meds.reshape(-1)\n",
    "    thr   = float(np.quantile(flat, q))\n",
    "\n",
    "    od_sets = []\n",
    "    counts = np.zeros(L, dtype=np.int32)\n",
    "    overlaps_prev = np.zeros(L, dtype=np.int32)\n",
    "    magnitude = np.zeros(L, dtype=np.float32)\n",
    "\n",
    "    for l in range(L):\n",
    "        idx = np.where(meds[l] >= thr)[0]\n",
    "        od_sets.append(set(idx))\n",
    "        counts[l] = idx.size\n",
    "        magnitude[l] = float(np.mean(meds[l, idx] - thr)) if idx.size > 0 else 0.0\n",
    "        if l > 0:\n",
    "            overlaps_prev[l] = len(od_sets[l].intersection(od_sets[l-1]))\n",
    "    return counts, overlaps_prev, thr, magnitude\n",
    "\n",
    "# ============================ PLOTTING ============================\n",
    "def make_plots(model_to_stats: Dict[str, dict], out_dir: Path = OUT_DIR):\n",
    "    \"\"\"\n",
    "    model_to_stats[model_tag] = {\n",
    "        'meds': (L,D),\n",
    "        'thr': float,\n",
    "        'counts': (L,),\n",
    "        'overlaps_prev': (L,),\n",
    "        'mag': (L,)\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- Figure 1: last-layer medians per model (with global quantile line) ----------\n",
    "    fig, axes = plt.subplots(1, len(model_to_stats), figsize=(6.6 * len(model_to_stats), 4.8), sharey=True)\n",
    "    if len(model_to_stats) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (tag, st) in zip(axes, model_to_stats.items()):\n",
    "        meds = st[\"meds\"]; thr = st[\"thr\"]; L, D = meds.shape\n",
    "        last = meds[-1]\n",
    "        ax.scatter(np.arange(D), last, s=10, alpha=0.65, edgecolor=\"none\")\n",
    "        ax.axhline(thr, color=\"orange\", linestyle=\"--\", linewidth=1.5, label=f\"global q={OUTLIER_QUANTILE:.2f}\")\n",
    "        # compress y-axis using a high quantile (but not below the threshold)\n",
    "        y_max = float(max(np.quantile(last, FIRST_PLOT_Y_QUANT), thr*1.05))\n",
    "        ax.set_ylim(0.0, y_max)\n",
    "        ax.set_title(f\"{tag} • last layer\")\n",
    "        ax.set_xlabel(\"dimension\")\n",
    "        ax.set_ylabel(\"median(|activation|)\" if USE_ABS_MEDIAN else \"median(activation)\")\n",
    "        ax.legend(frameon=False, fontsize=\"small\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_png1 = out_dir / \"last_layer_medians_both_quantile.pdf\"\n",
    "    plt.savefig(out_png1, dpi=220); plt.close(fig)\n",
    "    print(\"✓ wrote\", out_png1)\n",
    "\n",
    "    # ---------- Figure 2: counts & overlaps w/previous + bubble size ∝ magnitude ----------\n",
    "    plt.figure(figsize=(10.5, 5.8))\n",
    "\n",
    "    # global scaling for bubble sizes\n",
    "    all_mag = np.concatenate([st[\"mag\"] for st in model_to_stats.values()])\n",
    "    mag_min, mag_max = float(all_mag.min()), float(all_mag.max())\n",
    "    mag_span = max(mag_max - mag_min, 1e-12)\n",
    "\n",
    "    for tag, st in model_to_stats.items():\n",
    "        L = st[\"meds\"].shape[0]\n",
    "        layers = np.arange(L)\n",
    "\n",
    "        # lines\n",
    "        plt.plot(layers, st[\"counts\"], marker=\"o\", linewidth=2.5, alpha=0.95, label=f\"{tag}: #ODs\")\n",
    "        plt.plot(layers, st[\"overlaps_prev\"], marker=\"s\", linestyle=\"--\", linewidth=2.5, alpha=0.95, label=f\"{tag}: #OD ∩ prev\")\n",
    "\n",
    "        # bubbles sized by mean exceedance\n",
    "        mags  = st[\"mag\"]\n",
    "        sizes = DOT_SIZE_MIN + (DOT_SIZE_MAX - DOT_SIZE_MIN) * ((mags - mag_min) / mag_span)\n",
    "        plt.scatter(layers, st[\"counts\"], s=sizes, alpha=0.55, edgecolor=\"none\")\n",
    "\n",
    "    plt.xlabel(\"layer\")\n",
    "    plt.ylabel(f\"# outlier dims (≥ global q={OUTLIER_QUANTILE:.2f})\")\n",
    "    plt.title(\"Outlier dimensions per layer (global-quantile rule)\\nSolid: count • Dashed: overlap with previous • Dot size ∝ exceedance\")\n",
    "    plt.legend(ncol=2, fontsize=\"small\", frameon=False)\n",
    "    plt.tight_layout()\n",
    "    out_png2 = out_dir / \"od_counts_per_layer_both_quantile_bubbles.pdf\"\n",
    "    plt.savefig(out_png2, dpi=220); plt.close()\n",
    "    print(\"✓ wrote\", out_png2)\n",
    "\n",
    "# ============================ DRIVER ============================\n",
    "def run():\n",
    "    df_all, token_df = load_token_index(CSV_PATH)\n",
    "    print(f\"✓ Total tokens: {len(token_df):,}\")\n",
    "\n",
    "    model_to_stats: Dict[str, dict] = {}\n",
    "    for model_id in MODELS:\n",
    "        print(f\"\\n=== Embedding all tokens • {model_id} (auto first/last subtoken) ===\")\n",
    "        mmap_paths, L, N, D, tag = embed_to_memmaps(df_all, token_df, model_id, batch_size=BATCH_SIZE)\n",
    "        print(f\"Shapes: layers={L}, tokens={N}, dim={D}\")\n",
    "\n",
    "        meds = per_layer_dim_medians(mmap_paths, L, N, D, use_abs=USE_ABS_MEDIAN)\n",
    "        counts, overlaps_prev, thr, mag = counts_prev_overlap_and_magnitude_quantile(meds, OUTLIER_QUANTILE)\n",
    "        print(f\"{tag}: global q={OUTLIER_QUANTILE:.2f}  → threshold={thr:.6f}  • last-layer #OD={counts[-1]}\")\n",
    "\n",
    "        model_to_stats[tag] = {\"meds\": meds, \"counts\": counts, \"overlaps_prev\": overlaps_prev, \"thr\": thr, \"mag\": mag}\n",
    "\n",
    "        # tidy up memmaps\n",
    "        for p in mmap_paths:\n",
    "            try: os.remove(p)\n",
    "            except Exception: pass\n",
    "\n",
    "    make_plots(model_to_stats, OUT_DIR)\n",
    "    print(\"✓ done. See:\", OUT_DIR.resolve())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916e0ee-551c-419f-adae-91bc82c4c9df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
