{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fccdd2f-009b-403c-a55e-4cd573675633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-14 09:00:00.822766: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np, pandas as pd, torch\n",
    "import torch.utils.data as torchdata\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel, BertModel, AutoConfig\n",
    "\n",
    "from IsoScore import IsoScore\n",
    "from dadapy import Data\n",
    "from skdim.id import MLE, MOM, TLE, CorrInt, FisherS, lPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa81b43f-c822-4458-808f-b0d01ae2cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ===== Optional deps (gracefully skipped if not installed) =====\n",
    "HAS_DADAPY = False\n",
    "try:\n",
    "    from dadapy import Data  # DADApy ID estimators (TwoNN, GRIDE)\n",
    "    HAS_DADAPY = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "HAS_SKDIM = False\n",
    "try:\n",
    "    from skdim.id import MOM, TLE, CorrInt, FisherS, lPCA, MLE, DANCo, ESS, MiND_ML, MADA, KNN\n",
    "    HAS_SKDIM = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# IsoScore: use library if available, else a simple monotone fallback\n",
    "try:\n",
    "    from isoscore import IsoScore\n",
    "    _HAS_ISOSCORE = True\n",
    "except Exception:\n",
    "    _HAS_ISOSCORE = False\n",
    "    class _IsoScoreFallback:\n",
    "        @staticmethod\n",
    "        def IsoScore(X: np.ndarray) -> float:\n",
    "            C = np.cov(X.T, ddof=0)\n",
    "            ev = np.linalg.eigvalsh(C)\n",
    "            if ev.mean() <= 0 or ev[-1] <= 0:\n",
    "                return 0.0\n",
    "            return float(np.clip(ev.mean() / (ev[-1] + 1e-9), 0.0, 1.0))\n",
    "    IsoScore = _IsoScoreFallback()\n",
    "\n",
    "# =============================== CONFIG ===============================\n",
    "CSV_PATH       = \"en_ewt-ud-train_sentences.csv\"\n",
    "REL_COL_HINT   = \"relation_type\"        # <-- your existing list[str] column with UD relations\n",
    "TOP_K_REL      = 10                      # keep only the 10 most frequent relations\n",
    "\n",
    "BASELINE       = \"bert-base-uncased\"     # set to \"gpt2\" for GPT-2\n",
    "WORD_REP_MODE  = \"first\"                 # BERT: {\"first\",\"last\",\"mean\"}; GPT-2: {\"last\",\"mean\"}\n",
    "\n",
    "RAW_MAX_PER_CLASS             = int(1e12)  # no cap per relation for fast metrics\n",
    "N_BOOTSTRAP_FAST              = 50\n",
    "N_BOOTSTRAP_HEAVY             = 20\n",
    "FAST_BS_MAX_SAMP_PER_CLASS    = int(1e12)\n",
    "HEAVY_BS_MAX_SAMP_PER_CLASS   = 5000\n",
    "\n",
    "RAND_SEED = 42\n",
    "PLOT_DIR  = Path(\"results_REL\"); PLOT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CSV_DIR   = Path(\"tables_REL\") / \"relation_bootstrap\"; CSV_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": torch.backends.cudnn.benchmark = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "EPS = 1e-12\n",
    "\n",
    "# =============================== HELPERS ===============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _center(X: np.ndarray) -> np.ndarray:\n",
    "    return X - X.mean(0, keepdims=True)\n",
    "\n",
    "def _eigvals_from_X(X: np.ndarray) -> np.ndarray:\n",
    "    Xc = _center(X.astype(np.float32, copy=False))\n",
    "    try:\n",
    "        _, S, _ = np.linalg.svd(Xc, full_matrices=False)\n",
    "        lam = (S**2).astype(np.float64)\n",
    "        lam.sort()\n",
    "        return lam[::-1]\n",
    "    except Exception:\n",
    "        return np.array([], dtype=np.float64)\n",
    "\n",
    "def _jitter_unique(X: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    try:\n",
    "        if np.unique(X, axis=0).shape[0] < X.shape[0]:\n",
    "            X = X + np.random.normal(scale=eps, size=X.shape).astype(X.dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return X\n",
    "\n",
    "def _num_hidden_layers(model) -> int:\n",
    "    n = getattr(model.config, \"num_hidden_layers\", None)\n",
    "    if n is None: n = getattr(model.config, \"n_layer\", None)\n",
    "    if n is None: raise ValueError(\"Cannot determine number of hidden layers\")\n",
    "    return int(n)\n",
    "\n",
    "def _hidden_size(model) -> int:\n",
    "    d = getattr(model.config, \"hidden_size\", None)\n",
    "    if d is None: d = getattr(model.config, \"n_embd\", None)\n",
    "    if d is None: raise ValueError(\"Cannot determine hidden size\")\n",
    "    return int(d)\n",
    "\n",
    "# ========= Metric single-call functions =========\n",
    "# --- Isotropy ---\n",
    "def _iso_once(X: np.ndarray) -> float:\n",
    "    return float(IsoScore.IsoScore(X))\n",
    "\n",
    "def _sf_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    gm = np.exp(np.mean(np.log(lam + EPS))); am = float(lam.mean() + EPS)\n",
    "    return float(gm / am)  # higher = flatter = more isotropic\n",
    "\n",
    "def _rand_once(X: np.ndarray, K: int = 2000) -> float:\n",
    "    n = X.shape[0]\n",
    "    if n < 2: return np.nan\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    K_eff = min(K, (n*(n-1))//2)\n",
    "    i = rng.integers(0, n, size=K_eff)\n",
    "    j = rng.integers(0, n, size=K_eff)\n",
    "    same = i == j\n",
    "    if same.any():\n",
    "        j[same] = rng.integers(0, n, size=same.sum())\n",
    "    A, B = X[i], X[j]\n",
    "    num = np.sum(A*B, axis=1)\n",
    "    den = (np.linalg.norm(A, axis=1)*np.linalg.norm(B, axis=1) + 1e-9)\n",
    "    return float(np.mean(np.abs(num/den)))  # higher ≈ more anisotropic\n",
    "\n",
    "def _vmf_kappa_once(X: np.ndarray) -> float:\n",
    "    if X.shape[0] < 2: return np.nan\n",
    "    Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
    "    R = np.linalg.norm(Xn.mean(axis=0))\n",
    "    d = Xn.shape[1]\n",
    "    if R < 1e-9: return 0.0\n",
    "    return float(max(R * (d - R**2) / (1.0 - R**2 + 1e-9), 0.0))  # higher = more anisotropic\n",
    "\n",
    "# --- Linear ID ---\n",
    "def _pcaXX_once(X: np.ndarray, var_ratio: float) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    c = np.cumsum(lam); thr = c[-1] * var_ratio\n",
    "    return float(np.searchsorted(c, thr) + 1)\n",
    "\n",
    "def _pca95_once(X: np.ndarray) -> float: return _pcaXX_once(X, 0.95)\n",
    "def _pca99_once(X: np.ndarray) -> float: return _pcaXX_once(X, 0.99)\n",
    "\n",
    "def _erank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    p = lam / (lam.sum() + EPS)\n",
    "    H = -(p * np.log(p + EPS)).sum()\n",
    "    return float(np.exp(H))\n",
    "\n",
    "def _pr_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    s1 = lam.sum(); s2 = (lam**2).sum()\n",
    "    return float((s1**2) / (s2 + EPS))\n",
    "\n",
    "def _stable_rank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    return float(lam.sum() / (lam.max() + EPS))\n",
    "\n",
    "# --- Non-linear (DADApy) ---\n",
    "def _dadapy_twonn_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    id_est, _, _ = d.compute_id_2NN()\n",
    "    return float(id_est)\n",
    "\n",
    "def _dadapy_gride_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    d.compute_distances(maxk=64)\n",
    "    ids, _, _ = d.return_id_scaling_gride(range_max=64)\n",
    "    return float(ids[-1])\n",
    "\n",
    "# --- Non-linear (scikit-dimension) ---\n",
    "def _skdim_factory(name: str):\n",
    "    if not HAS_SKDIM: return None\n",
    "    mapping = {\n",
    "        \"mom\": MOM, \"tle\": TLE, \"corrint\": CorrInt, \"fishers\": FisherS,\n",
    "        \"lpca\": lPCA, \"lpca95\": lPCA, \"lpca99\": lPCA,\n",
    "        \"mle\": MLE, \"danco\": DANCo, \"mind_ml\": MiND_ML, \"ess\": ESS,\n",
    "        \"mada\": MADA, \"knn\": KNN,\n",
    "    }\n",
    "    cls = mapping.get(name)\n",
    "    if cls is None: return None\n",
    "    def _builder():\n",
    "        if name == \"lpca\":   return cls(ver=\"FO\")\n",
    "        if name == \"lpca95\": return cls(ver=\"ratio\", alphaRatio=0.95)\n",
    "        if name == \"lpca99\": return cls(ver=\"ratio\", alphaRatio=0.99)\n",
    "        return cls()\n",
    "    return _builder\n",
    "\n",
    "def _skdim_once_builder(name: str) -> Callable[[np.ndarray], float] | None:\n",
    "    build = _skdim_factory(name)\n",
    "    if build is None: return None\n",
    "    def _once(X: np.ndarray) -> float:\n",
    "        est = build(); est.fit(_jitter_unique(X)); return float(getattr(est, \"dimension_\", np.nan))\n",
    "    return _once\n",
    "\n",
    "# ---- Metric registries (ALL METRICS) ----\n",
    "FAST_ONCE: Dict[str, Callable[[np.ndarray], float]] = {\n",
    "    # Isotropy\n",
    "    \"iso\": _iso_once, \"sf\": _sf_once, \"rand\": _rand_once, \"vmf_kappa\": _vmf_kappa_once,\n",
    "    # Linear ID\n",
    "    \"erank\": _erank_once, \"pr\": _pr_once, \"stable_rank\": _stable_rank_once,\n",
    "    \"pca95\": _pca95_once, \"pca99\": _pca99_once,\n",
    "}\n",
    "HEAVY_ONCE: Dict[str, Callable[[np.ndarray], float] | None] = {\n",
    "    # DADApy\n",
    "    \"twonn\": _dadapy_twonn_once, \"gride\": _dadapy_gride_once,\n",
    "    # scikit-dimension\n",
    "    \"mom\": _skdim_once_builder(\"mom\"), \"tle\": _skdim_once_builder(\"tle\"),\n",
    "    \"corrint\": _skdim_once_builder(\"corrint\"), \"fishers\": _skdim_once_builder(\"fishers\"),\n",
    "    \"lpca\": _skdim_once_builder(\"lpca\"), \"lpca95_skdim\": _skdim_once_builder(\"lpca95\"),\n",
    "    \"lpca99_skdim\": _skdim_once_builder(\"lpca99\"),\n",
    "    \"mle\": _skdim_once_builder(\"mle\"), \"danco\": _skdim_once_builder(\"danco\"),\n",
    "    \"mind_ml\": _skdim_once_builder(\"mind_ml\"), \"ess\": _skdim_once_builder(\"ess\"),\n",
    "    \"mada\": _skdim_once_builder(\"mada\"), \"knn\": _skdim_once_builder(\"knn\"),\n",
    "}\n",
    "LABELS = {\n",
    "    # Isotropy\n",
    "    \"iso\":\"IsoScore\", \"sf\":\"Spectral Flatness\", \"rand\":\"RandCos |μ| (anisotropy↑)\", \"vmf_kappa\":\"vMF κ (anisotropy↑)\",\n",
    "    # Linear ID\n",
    "    \"erank\":\"Effective Rank\", \"pr\":\"Participation Ratio\", \"stable_rank\":\"Stable Rank\",\n",
    "    \"pca95\":\"lPCA 0.95\", \"pca99\":\"lPCA 0.99\",\n",
    "    # Non-linear (DADApy)\n",
    "    \"twonn\":\"TwoNN ID\", \"gride\":\"GRIDE ID\",\n",
    "    # Non-linear (skdim)\n",
    "    \"mom\":\"MOM\", \"tle\":\"TLE\", \"corrint\":\"CorrInt\", \"fishers\":\"FisherS\",\n",
    "    \"lpca\":\"lPCA FO\", \"lpca95_skdim\":\"lPCA 0.95 (skdim)\", \"lpca99_skdim\":\"lPCA 0.99 (skdim)\",\n",
    "    \"mle\":\"MLE\", \"danco\":\"DANCo\", \"mind_ml\":\"MiND-ML\", \"ess\":\"ESS\", \"mada\":\"MADA\", \"knn\":\"KNN\",\n",
    "}\n",
    "#ALL_METRICS = list(FAST_ONCE.keys()) + [k for k, v in HEAVY_ONCE.items() if v is not None]\n",
    "ALL_METRICS =[\"gride\", \"lpca99_skdim\"]\n",
    "\n",
    "# =============================== DATA: use existing relation_type column ===============================\n",
    "def _pick_relation_col(df: pd.DataFrame) -> str:\n",
    "    cands = [REL_COL_HINT, \"typed_dependency\", \"relation\", \"deprel\", \"ud_rel\", \"rel\", \"REL\", \"Rel\"]\n",
    "    for c in cands:\n",
    "        if c in df.columns: return c\n",
    "    raise ValueError(f\"No relation column found. Tried: {', '.join(cands)}\")\n",
    "\n",
    "def load_relations_topk_from_column(csv_path: str, top_k: int = TOP_K_REL):\n",
    "    \"\"\"\n",
    "    Expects CSV with:\n",
    "      - sentence_id (str)\n",
    "      - tokens        (list[str]) — one row per sentence\n",
    "      - relation_type (list[str]) — UD relation labels per token (or similarly named)\n",
    "    Expands to token-level rows with 'relation_class' and keeps only top_k most frequent labels.\n",
    "    \"\"\"\n",
    "    df_all = pd.read_csv(csv_path)\n",
    "    rel_col = _pick_relation_col(df_all)\n",
    "    df = df_all[[\"sentence_id\",\"tokens\", rel_col]].copy()\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df.tokens  = df.tokens.apply(_to_list)\n",
    "    df[rel_col] = df[rel_col].apply(_to_list)\n",
    "\n",
    "    rows = []\n",
    "    for sid, toks, rels in df[[\"sentence_id\",\"tokens\", rel_col]].itertuples(index=False):\n",
    "        L = min(len(toks), len(rels))\n",
    "        for wid in range(L):\n",
    "            r = str(rels[wid])\n",
    "            rows.append((sid, wid, r, toks[wid]))\n",
    "    df_tok = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\",\"relation_class\",\"word\"])\n",
    "    if df_tok.empty:\n",
    "        raise ValueError(\"No token rows constructed—check that your relation column contains lists of strings.\")\n",
    "\n",
    "    # keep only top-k most frequent\n",
    "    top = df_tok.relation_class.value_counts().nlargest(top_k).index.tolist()\n",
    "    df_tok = df_tok[df_tok.relation_class.isin(top)].reset_index(drop=True)\n",
    "\n",
    "    df_sent = df[[\"sentence_id\",\"tokens\"]].drop_duplicates(\"sentence_id\")\n",
    "    return df_sent, df_tok, top\n",
    "\n",
    "def sample_raw(df_tok: pd.DataFrame, per_class_cap: int = RAW_MAX_PER_CLASS) -> pd.DataFrame:\n",
    "    picks = []\n",
    "    for c, sub in df_tok.groupby(\"relation_class\", sort=False):\n",
    "        n = min(len(sub), per_class_cap)\n",
    "        picks.append(sub.sample(n, random_state=RAND_SEED, replace=False))\n",
    "    return pd.concat(picks, ignore_index=True)\n",
    "\n",
    "def make_class_palette(classes: List[str]) -> Dict[str, Tuple[float, float, float]]:\n",
    "    # Use distinct qualitative colors; stable order by frequency list order\n",
    "    base = list(sns.color_palette(\"tab20\", 20)) + list(sns.color_palette(\"tab20b\", 20)) + list(sns.color_palette(\"tab20c\", 20))\n",
    "    if len(base) < len(classes):\n",
    "        base = list(sns.color_palette(\"husl\", len(classes)))\n",
    "    return {cls: base[i % len(base)] for i, cls in enumerate(classes)}\n",
    "\n",
    "# =============================== EMBEDDING (BERT & GPT‑2) ===============================\n",
    "def embed_subset(df_sent: pd.DataFrame,\n",
    "                 subset_df: pd.DataFrame,\n",
    "                 baseline: str = BASELINE,\n",
    "                 word_rep_mode: str = WORD_REP_MODE,\n",
    "                 batch_size: int = BATCH_SIZE) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df_sent[\"sentence_id\"]   = df_sent[\"sentence_id\"].astype(str)\n",
    "    subset_df[\"sentence_id\"] = subset_df[\"sentence_id\"].astype(str)\n",
    "\n",
    "    # sid -> list[(global_idx, word_id)]\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(subset_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(str(sid), []).append((gidx, int(wid)))\n",
    "\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_sent[df_sent.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    tokzr = AutoTokenizer.from_pretrained(baseline, use_fast=True)\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
    "    if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "        enc_kwargs[\"add_prefix_space\"] = True\n",
    "    if tokzr.pad_token is None and getattr(tokzr, \"eos_token\", None) is not None:\n",
    "        tokzr.pad_token = tokzr.eos_token\n",
    "\n",
    "    model = AutoModel.from_pretrained(baseline, output_hidden_states=True).eval().to(device)\n",
    "    if getattr(model.config, \"pad_token_id\", None) is None and tokzr.pad_token_id is not None:\n",
    "        model.config.pad_token_id = tokzr.pad_token_id\n",
    "    if device == \"cuda\": model.half()\n",
    "\n",
    "    L = _num_hidden_layers(model) + 1   # include embedding layer\n",
    "    D = _hidden_size(model)\n",
    "    N = len(subset_df)\n",
    "\n",
    "    reps   = np.zeros((L, N, D), np.float16)\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{baseline} (embed subset)\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L,B,T,D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                mp = {}\n",
    "                for tidx, wid in enumerate(enc_be.word_ids(b)):\n",
    "                    if wid is not None:\n",
    "                        mp.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = mp.get(wid)\n",
    "                    if not toks: continue\n",
    "                    if word_rep_mode == \"first\":\n",
    "                        vec = h[:, b, toks[0], :]\n",
    "                    elif word_rep_mode == \"last\":\n",
    "                        vec = h[:, b, toks[-1], :]\n",
    "                    elif word_rep_mode == \"mean\":\n",
    "                        vec = h[:, b, toks, :].mean(axis=1)\n",
    "                    else:\n",
    "                        raise ValueError(\"WORD_REP_MODE must be one of {'first','last','mean'} (for GPT-2 use 'last' or 'mean').\")\n",
    "                    reps[:, gidx, :] = vec.astype(np.float16, copy=False)\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    missing = int((~filled).sum())\n",
    "    if missing: print(f\"⚠ Missing vectors for {missing} of {N} tokens\")\n",
    "    del model; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    return reps, filled\n",
    "\n",
    "# =============================== BOOTSTRAP CORE ===============================\n",
    "def _bs_layer_loop(rep_sub: np.ndarray, M: int, n_reps: int, compute_once: Callable[[np.ndarray], float]):\n",
    "    L, N, D = rep_sub.shape\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    A = np.full((n_reps, L), np.nan, np.float32)\n",
    "    for r in range(n_reps):\n",
    "        idx = rng.integers(0, N, size=M)\n",
    "        for l in range(L):\n",
    "            X = rep_sub[l, idx].astype(np.float32, copy=False)\n",
    "            try:\n",
    "                A[r, l] = float(compute_once(X))\n",
    "            except Exception:\n",
    "                A[r, l] = np.nan\n",
    "    mu = np.nanmean(A, axis=0).astype(np.float32)\n",
    "    lo = np.nanpercentile(A, 2.5, axis=0).astype(np.float32)\n",
    "    hi = np.nanpercentile(A, 97.5, axis=0).astype(np.float32)\n",
    "    return mu, lo, hi\n",
    "\n",
    "# =============================== SAVE / PLOT ===============================\n",
    "def save_metric_csv_all_classes(metric: str,\n",
    "                                class_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                                layers: np.ndarray,\n",
    "                                baseline: str,\n",
    "                                subset_name: str = \"raw\"):\n",
    "    rows = []\n",
    "    for c, stats in class_to_stats.items():\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        for l, val in enumerate(mu):\n",
    "            rows.append({\n",
    "                \"subset\": subset_name, \"model\": baseline, \"feature\": \"relation_type\",\n",
    "                \"class\": c, \"metric\": metric, \"layer\": int(layers[l]),\n",
    "                \"mean\": float(val) if np.isfinite(val) else np.nan,\n",
    "                \"ci_low\": float(lo[l]) if isinstance(lo, np.ndarray) and np.isfinite(lo[l]) else np.nan,\n",
    "                \"ci_high\": float(hi[l]) if isinstance(hi, np.ndarray) and np.isfinite(hi[l]) else np.nan,\n",
    "                \"n_tokens\": int(stats.get(\"n\", 0)), \"word_rep_mode\": WORD_REP_MODE,\n",
    "                \"source_csv\": Path(CSV_PATH).name,\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    out = CSV_DIR / f\"relation_{subset_name}_{metric}_{baseline}.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "\n",
    "def plot_metric_with_ci(class_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                        layers: np.ndarray, metric: str, title: str, out_path: Path,\n",
    "                        palette: Dict[str, Tuple[float, float, float]] | None = None,\n",
    "                        classes_order: List[str] | None = None):\n",
    "    plt.figure(figsize=(10.5, 5.5))\n",
    "    order = classes_order if classes_order is not None else sorted(class_to_stats.keys())\n",
    "    for c in order:\n",
    "        stats = class_to_stats.get(c)\n",
    "        if not stats: continue\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        if mu is None or np.all(np.isnan(mu)): continue\n",
    "        color = (palette.get(c) if isinstance(palette, dict) else None) if palette else None\n",
    "        plt.plot(layers, mu, label=c, lw=1.8, color=color)\n",
    "        if isinstance(lo, np.ndarray) and isinstance(hi, np.ndarray) and not np.all(np.isnan(lo)):\n",
    "            plt.fill_between(layers, lo, hi, alpha=0.15, color=color)\n",
    "    plt.xlabel(\"Layer\"); plt.ylabel(LABELS.get(metric, metric.upper())); plt.title(title)\n",
    "    ncol = 5 if len(order) >= 10 else 3\n",
    "    plt.legend(ncol=ncol, fontsize=\"small\", title=\"UD relation (top‑10)\", frameon=False)\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=220); plt.close()\n",
    "\n",
    "# =============================== DRIVER ===============================\n",
    "def run_relation_topk_pipeline():\n",
    "    # 1) Load token lists + top‑K relations from existing column\n",
    "    df_sent, rel_df, top_rel = load_relations_topk_from_column(CSV_PATH, top_k=TOP_K_REL)\n",
    "    classes = list(top_rel)  # keep in frequency order\n",
    "    palette = make_class_palette(classes)\n",
    "    print(f\"✓ corpus ready — {len(rel_df):,} tokens across relations {classes}\")\n",
    "\n",
    "    # 2) Optional per-class cap (currently unlimited for fast metrics)\n",
    "    raw_df = sample_raw(rel_df, RAW_MAX_PER_CLASS)\n",
    "    print(\"Sample sizes per relation (raw cap):\")\n",
    "    counts = raw_df.relation_class.value_counts()\n",
    "    print({k: int(counts[k]) for k in classes})\n",
    "\n",
    "    # 3) Embed once (BERT: 'first/last/mean'; GPT‑2: set WORD_REP_MODE='last' or 'mean')\n",
    "    reps, filled = embed_subset(df_sent, raw_df, BASELINE, WORD_REP_MODE, BATCH_SIZE)\n",
    "    raw_df = raw_df.reset_index(drop=True).loc[filled].reset_index(drop=True)\n",
    "    cls_arr = raw_df.relation_class.values\n",
    "    L = reps.shape[0]; layers = np.arange(L)\n",
    "    print(f\"✓ embedded {len(raw_df):,} tokens  • layers={L}\")\n",
    "\n",
    "    # 4) Metric loop (ALL METRICS)\n",
    "    for metric in ALL_METRICS:\n",
    "        print(f\"\\n→ Computing metric: {metric} …\")\n",
    "        compute_once = FAST_ONCE.get(metric) or HEAVY_ONCE.get(metric)\n",
    "        if compute_once is None:\n",
    "            print(f\"  (skipping {metric}: estimator unavailable)\")\n",
    "            continue\n",
    "\n",
    "        n_bs = N_BOOTSTRAP_FAST if metric in FAST_ONCE else N_BOOTSTRAP_HEAVY\n",
    "        Mcap = FAST_BS_MAX_SAMP_PER_CLASS if metric in FAST_ONCE else HEAVY_BS_MAX_SAMP_PER_CLASS\n",
    "\n",
    "        class_results: Dict[str, Dict[str, np.ndarray]] = {}\n",
    "        for c in classes:\n",
    "            idx = np.where(cls_arr == c)[0]\n",
    "            if idx.size < 3:\n",
    "                continue\n",
    "            sub = reps[:, idx]  # (L, n_c, D)\n",
    "            Nc = sub.shape[1]\n",
    "            M = min(Mcap, Nc)\n",
    "            mu, lo, hi = _bs_layer_loop(sub, M, n_bs, compute_once)\n",
    "            class_results[c] = {\"mean\": mu, \"lo\": lo, \"hi\": hi, \"n\": int(Nc)}\n",
    "\n",
    "        save_metric_csv_all_classes(metric, class_results, layers, BASELINE, subset_name=\"raw\")\n",
    "        plot_metric_with_ci(class_results, layers, metric,\n",
    "                            title=f\"{LABELS.get(metric, metric.upper())} • {BASELINE}\",\n",
    "                            out_path=PLOT_DIR / f\"relation_raw_{metric}_{BASELINE}.png\",\n",
    "                            palette=palette, classes_order=classes)\n",
    "        print(f\"  ✓ saved: CSV= {CSV_DIR}/relation_raw_{metric}_{BASELINE}.csv  \"\n",
    "              f\"plot= {PLOT_DIR}/relation_raw_{metric}_{BASELINE}.png\")\n",
    "\n",
    "        del class_results; gc.collect()\n",
    "        if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    del reps; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    print(\"\\n✓ done (incremental outputs produced per metric).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_relation_topk_pipeline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a3ea47f-b2cb-401e-bc61-8cfbfada80af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ corpus ready — 118,011 tokens across relations ['punct', 'case', 'det', 'nsubj', 'obj', 'advmod', 'amod', 'root', 'obl', 'conj']\n",
      "✓ first word of each sentence removed (EXCLUDE_INDEX_0=True)\n",
      "Sample sizes per relation (raw cap):\n",
      "{'punct': 21787, 'case': 16041, 'det': 14417, 'nsubj': 12331, 'obj': 9661, 'advmod': 9485, 'amod': 9076, 'root': 8915, 'obl': 8741, 'conj': 7557}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openai-community/gpt2 (embed subset): 100%|█| 10062/10062 [01:31<00:00, 110.53it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ embedded 118,011 tokens  • layers=13\n",
      "\n",
      "→ Computing metric: gride …\n",
      "  ✓ saved: CSV= tables_REL_GPT2_no_idx0/relation_bootstrap/relation_raw_gride_gpt2.csv  plot= results_REL_GPT2_no_idx0/relation_raw_gride_gpt2.png\n",
      "\n",
      "→ Computing metric: iso …\n",
      "  ✓ saved: CSV= tables_REL_GPT2_no_idx0/relation_bootstrap/relation_raw_iso_gpt2.csv  plot= results_REL_GPT2_no_idx0/relation_raw_iso_gpt2.png\n",
      "\n",
      "→ Computing metric: lpca99_skdim …\n",
      "  ✓ saved: CSV= tables_REL_GPT2_no_idx0/relation_bootstrap/relation_raw_lpca99_skdim_gpt2.csv  plot= results_REL_GPT2_no_idx0/relation_raw_lpca99_skdim_gpt2.png\n",
      "\n",
      "✓ done (incremental outputs produced per metric).\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ===== Optional deps (gracefully skipped if not installed) =====\n",
    "HAS_DADAPY = False\n",
    "try:\n",
    "    from dadapy import Data  # DADApy ID estimators (TwoNN, GRIDE)\n",
    "    HAS_DADAPY = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "HAS_SKDIM = False\n",
    "try:\n",
    "    from skdim.id import MOM, TLE, CorrInt, FisherS, lPCA, MLE, DANCo, ESS, MiND_ML, MADA, KNN\n",
    "    HAS_SKDIM = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# IsoScore: use library if available, else a simple monotone fallback\n",
    "try:\n",
    "    from isoscore import IsoScore\n",
    "    _HAS_ISOSCORE = True\n",
    "except Exception:\n",
    "    _HAS_ISOSCORE = False\n",
    "    class _IsoScoreFallback:\n",
    "        @staticmethod\n",
    "        def IsoScore(X: np.ndarray) -> float:\n",
    "            C = np.cov(X.T, ddof=0)\n",
    "            ev = np.linalg.eigvalsh(C)\n",
    "            if ev.mean() <= 0 or ev[-1] <= 0:\n",
    "                return 0.0\n",
    "            return float(np.clip(ev.mean() / (ev[-1] + 1e-9), 0.0, 1.0))\n",
    "    IsoScore = _IsoScoreFallback()\n",
    "\n",
    "# =============================== CONFIG ===============================\n",
    "CSV_PATH       = \"en_ewt-ud-train_sentences.csv\"\n",
    "REL_COL_HINT   = \"relation_type\"        # column with UD relations (list[str])\n",
    "TOP_K_REL      = 10                     # keep only the 10 most frequent relations\n",
    "\n",
    "# ---- model config: GPT-2 + last token representation ----\n",
    "BASELINE       = \"gpt2\"                 # you can also try \"openai-community/gpt2\"\n",
    "WORD_REP_MODE  = \"last\"                 # for GPT-2 use {\"last\",\"mean\"}\n",
    "\n",
    "# ---- NEW: drop first word of each sentence (index 0) ----\n",
    "EXCLUDE_INDEX_0 = True\n",
    "\n",
    "RAW_MAX_PER_CLASS             = int(1e12)  # no cap per relation for fast metrics\n",
    "N_BOOTSTRAP_FAST              = 50\n",
    "N_BOOTSTRAP_HEAVY             = 20\n",
    "FAST_BS_MAX_SAMP_PER_CLASS    = int(1e12)\n",
    "HEAVY_BS_MAX_SAMP_PER_CLASS   = 5000\n",
    "\n",
    "RAND_SEED = 42\n",
    "PLOT_DIR  = Path(\"results_REL_GPT2_no_idx0\"); PLOT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CSV_DIR   = Path(\"tables_REL_GPT2_no_idx0\") / \"relation_bootstrap\"; CSV_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": torch.backends.cudnn.benchmark = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "EPS = 1e-12\n",
    "\n",
    "# =============================== HELPERS ===============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _center(X: np.ndarray) -> np.ndarray:\n",
    "    return X - X.mean(0, keepdims=True)\n",
    "\n",
    "def _eigvals_from_X(X: np.ndarray) -> np.ndarray:\n",
    "    Xc = _center(X.astype(np.float32, copy=False))\n",
    "    try:\n",
    "        _, S, _ = np.linalg.svd(Xc, full_matrices=False)\n",
    "        lam = (S**2).astype(np.float64)\n",
    "        lam.sort()\n",
    "        return lam[::-1]\n",
    "    except Exception:\n",
    "        return np.array([], dtype=np.float64)\n",
    "\n",
    "def _jitter_unique(X: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    try:\n",
    "        if np.unique(X, axis=0).shape[0] < X.shape[0]:\n",
    "            X = X + np.random.normal(scale=eps, size=X.shape).astype(X.dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return X\n",
    "\n",
    "def _num_hidden_layers(model) -> int:\n",
    "    n = getattr(model.config, \"num_hidden_layers\", None)\n",
    "    if n is None: n = getattr(model.config, \"n_layer\", None)\n",
    "    if n is None: raise ValueError(\"Cannot determine number of hidden layers\")\n",
    "    return int(n)\n",
    "\n",
    "def _hidden_size(model) -> int:\n",
    "    d = getattr(model.config, \"hidden_size\", None)\n",
    "    if d is None: d = getattr(model.config, \"n_embd\", None)\n",
    "    if d is None: raise ValueError(\"Cannot determine hidden size\")\n",
    "    return int(d)\n",
    "\n",
    "def _is_gpt_like(model) -> bool:\n",
    "    mt = str(getattr(model.config, \"model_type\", \"\")).lower()\n",
    "    name = str(getattr(getattr(model, \"name_or_path\", \"\"), \"lower\", lambda: \"\")())\n",
    "    return (\"gpt2\" in mt) or (\"gpt2\" in name)\n",
    "\n",
    "# ===== robust GPT‑2 loader to avoid NoneType vocab_file bug =====\n",
    "def _safe_load_tok_and_model(baseline: str):\n",
    "    \"\"\"\n",
    "    Robust tokenizer+model loader.\n",
    "    - For GPT-2, avoids 'vocab_file NoneType' problems in some transformers setups\n",
    "      by temporarily guarding os.path.isfile(None).\n",
    "    - Tries both 'gpt2' and 'openai-community/gpt2' where appropriate.\n",
    "    Returns: (tokzr, model, model_id_used)\n",
    "    \"\"\"\n",
    "    import os as _os\n",
    "\n",
    "    bl = baseline.lower()\n",
    "    if \"gpt2\" in bl:\n",
    "        candidates = []\n",
    "        for mid in (baseline, \"openai-community/gpt2\", \"gpt2\"):\n",
    "            if mid not in candidates:\n",
    "                candidates.append(mid)\n",
    "    else:\n",
    "        candidates = [baseline]\n",
    "\n",
    "    last_err = None\n",
    "    for mid in candidates:\n",
    "        try:\n",
    "            # patch os.path.isfile just during tokenizer init\n",
    "            _orig_isfile = _os.path.isfile\n",
    "            def _patched_isfile(p):\n",
    "                if p is None:  # avoid TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType\n",
    "                    return False\n",
    "                return _orig_isfile(p)\n",
    "            _os.path.isfile = _patched_isfile\n",
    "            try:\n",
    "                tokzr = AutoTokenizer.from_pretrained(mid, use_fast=True, add_prefix_space=True)\n",
    "            finally:\n",
    "                _os.path.isfile = _orig_isfile\n",
    "\n",
    "            # GPT‑ish niceties\n",
    "            if getattr(tokzr, \"padding_side\", None) != \"right\":\n",
    "                tokzr.padding_side = \"right\"\n",
    "            if tokzr.pad_token is None and getattr(tokzr, \"eos_token\", None) is not None:\n",
    "                tokzr.pad_token = tokzr.eos_token\n",
    "\n",
    "            # model\n",
    "            model = AutoModel.from_pretrained(mid, output_hidden_states=True)\n",
    "            if getattr(model.config, \"pad_token_id\", None) is None and tokzr.pad_token_id is not None:\n",
    "                model.config.pad_token_id = tokzr.pad_token_id\n",
    "\n",
    "            # attach call kwargs (add_prefix_space for GPT‑2 like models)\n",
    "            call_kwargs = {}\n",
    "            if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "                call_kwargs[\"add_prefix_space\"] = True\n",
    "            tokzr._safe_call_kwargs = call_kwargs\n",
    "\n",
    "            return tokzr, model, mid\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "\n",
    "    raise RuntimeError(f\"Failed to load tokenizer/model for {baseline}. Tried {candidates}. Last error: {last_err}\")\n",
    "\n",
    "# ========= Metric single-call functions =========\n",
    "# --- Isotropy ---\n",
    "def _iso_once(X: np.ndarray) -> float:\n",
    "    return float(IsoScore.IsoScore(X))\n",
    "\n",
    "def _sf_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    gm = np.exp(np.mean(np.log(lam + EPS))); am = float(lam.mean() + EPS)\n",
    "    return float(gm / am)  # higher = flatter = more isotropic\n",
    "\n",
    "def _rand_once(X: np.ndarray, K: int = 2000) -> float:\n",
    "    n = X.shape[0]\n",
    "    if n < 2: return np.nan\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    K_eff = min(K, (n*(n-1))//2)\n",
    "    i = rng.integers(0, n, size=K_eff)\n",
    "    j = rng.integers(0, n, size=K_eff)\n",
    "    same = i == j\n",
    "    if same.any():\n",
    "        j[same] = rng.integers(0, n, size=same.sum())\n",
    "    A, B = X[i], X[j]\n",
    "    num = np.sum(A*B, axis=1)\n",
    "    den = (np.linalg.norm(A, axis=1)*np.linalg.norm(B, axis=1) + 1e-9)\n",
    "    return float(np.mean(np.abs(num/den)))  # higher ≈ more anisotropic\n",
    "\n",
    "def _vmf_kappa_once(X: np.ndarray) -> float:\n",
    "    if X.shape[0] < 2: return np.nan\n",
    "    Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
    "    R = np.linalg.norm(Xn.mean(axis=0))\n",
    "    d = Xn.shape[1]\n",
    "    if R < 1e-9: return 0.0\n",
    "    return float(max(R * (d - R**2) / (1.0 - R**2 + 1e-9), 0.0))  # higher = more anisotropic\n",
    "\n",
    "# --- Linear ID ---\n",
    "def _pcaXX_once(X: np.ndarray, var_ratio: float) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    c = np.cumsum(lam); thr = c[-1] * var_ratio\n",
    "    return float(np.searchsorted(c, thr) + 1)\n",
    "\n",
    "def _pca95_once(X: np.ndarray) -> float: return _pcaXX_once(X, 0.95)\n",
    "def _pca99_once(X: np.ndarray) -> float: return _pcaXX_once(X, 0.99)\n",
    "\n",
    "def _erank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    p = lam / (lam.sum() + EPS)\n",
    "    H = -(p * np.log(p + EPS)).sum()\n",
    "    return float(np.exp(H))\n",
    "\n",
    "def _pr_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    s1 = lam.sum(); s2 = (lam**2).sum()\n",
    "    return float((s1**2) / (s2 + EPS))\n",
    "\n",
    "def _stable_rank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    return float(lam.sum() / (lam.max() + EPS))\n",
    "\n",
    "# --- Non-linear (DADApy) ---\n",
    "def _dadapy_twonn_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    id_est, _, _ = d.compute_id_2NN()\n",
    "    return float(id_est)\n",
    "\n",
    "def _dadapy_gride_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    d.compute_distances(maxk=64)\n",
    "    ids, _, _ = d.return_id_scaling_gride(range_max=64)\n",
    "    return float(ids[-1])\n",
    "\n",
    "# --- Non-linear (scikit-dimension) ---\n",
    "def _skdim_factory(name: str):\n",
    "    if not HAS_SKDIM: return None\n",
    "    mapping = {\n",
    "        \"mom\": MOM, \"tle\": TLE, \"corrint\": CorrInt, \"fishers\": FisherS,\n",
    "        \"lpca\": lPCA, \"lpca95\": lPCA, \"lpca99\": lPCA,\n",
    "        \"mle\": MLE, \"danco\": DANCo, \"mind_ml\": MiND_ML, \"ess\": ESS,\n",
    "        \"mada\": MADA, \"knn\": KNN,\n",
    "    }\n",
    "    cls = mapping.get(name)\n",
    "    if cls is None: return None\n",
    "    def _builder():\n",
    "        if name == \"lpca\":   return cls(ver=\"FO\")\n",
    "        if name == \"lpca95\": return cls(ver=\"ratio\", alphaRatio=0.95)\n",
    "        if name == \"lpca99\": return cls(ver=\"ratio\", alphaRatio=0.99)\n",
    "        return cls()\n",
    "    return _builder\n",
    "\n",
    "def _skdim_once_builder(name: str) -> Callable[[np.ndarray], float] | None:\n",
    "    build = _skdim_factory(name)\n",
    "    if build is None: return None\n",
    "    def _once(X: np.ndarray) -> float:\n",
    "        est = build(); est.fit(_jitter_unique(X)); return float(getattr(est, \"dimension_\", np.nan))\n",
    "    return _once\n",
    "\n",
    "# ---- Metric registries (ALL METRICS) ----\n",
    "FAST_ONCE: Dict[str, Callable[[np.ndarray], float]] = {\n",
    "    # Isotropy\n",
    "    \"iso\": _iso_once, \"sf\": _sf_once, \"rand\": _rand_once, \"vmf_kappa\": _vmf_kappa_once,\n",
    "    # Linear ID\n",
    "    \"erank\": _erank_once, \"pr\": _pr_once, \"stable_rank\": _stable_rank_once,\n",
    "    \"pca95\": _pca95_once, \"pca99\": _pca99_once,\n",
    "}\n",
    "HEAVY_ONCE: Dict[str, Callable[[np.ndarray], float] | None] = {\n",
    "    # DADApy\n",
    "    \"twonn\": _dadapy_twonn_once, \"gride\": _dadapy_gride_once,\n",
    "    # scikit-dimension\n",
    "    \"mom\": _skdim_once_builder(\"mom\"), \"tle\": _skdim_once_builder(\"tle\"),\n",
    "    \"corrint\": _skdim_once_builder(\"corrint\"), \"fishers\": _skdim_once_builder(\"fishers\"),\n",
    "    \"lpca\": _skdim_once_builder(\"lpca\"), \"lpca95_skdim\": _skdim_once_builder(\"lpca95\"),\n",
    "    \"lpca99_skdim\": _skdim_once_builder(\"lpca99\"),\n",
    "    \"mle\": _skdim_once_builder(\"mle\"), \"danco\": _skdim_once_builder(\"danco\"),\n",
    "    \"mind_ml\": _skdim_once_builder(\"mind_ml\"), \"ess\": _skdim_once_builder(\"ess\"),\n",
    "    \"mada\": _skdim_once_builder(\"mada\"), \"knn\": _skdim_once_builder(\"knn\"),\n",
    "}\n",
    "LABELS = {\n",
    "    # Isotropy\n",
    "    \"iso\":\"IsoScore\", \"sf\":\"Spectral Flatness\", \"rand\":\"RandCos |μ| (anisotropy↑)\", \"vmf_kappa\":\"vMF κ (anisotropy↑)\",\n",
    "    # Linear ID\n",
    "    \"erank\":\"Effective Rank\", \"pr\":\"Participation Ratio\", \"stable_rank\":\"Stable Rank\",\n",
    "    \"pca95\":\"lPCA 0.95\", \"pca99\":\"lPCA 0.99\",\n",
    "    # Non-linear (DADApy)\n",
    "    \"twonn\":\"TwoNN ID\", \"gride\":\"GRIDE ID\",\n",
    "    # Non-linear (skdim)\n",
    "    \"mom\":\"MOM\", \"tle\":\"TLE\", \"corrint\":\"CorrInt\", \"fishers\":\"FisherS\",\n",
    "    \"lpca\":\"lPCA FO\", \"lpca95_skdim\":\"lPCA 0.95 (skdim)\", \"lpca99_skdim\":\"lPCA 0.99 (skdim)\",\n",
    "    \"mle\":\"MLE\", \"danco\":\"DANCo\", \"mind_ml\":\"MiND-ML\", \"ess\":\"ESS\", \"mada\":\"MADA\", \"knn\":\"KNN\",\n",
    "}\n",
    "# Keep it small for now\n",
    "ALL_METRICS = [\"gride\", \"iso\", \"lpca99_skdim\"]\n",
    "\n",
    "# =============================== DATA: relation_type column ===============================\n",
    "def _pick_relation_col(df: pd.DataFrame) -> str:\n",
    "    cands = [REL_COL_HINT, \"typed_dependency\", \"relation\", \"deprel\", \"ud_rel\", \"rel\", \"REL\", \"Rel\"]\n",
    "    for c in cands:\n",
    "        if c in df.columns: return c\n",
    "    raise ValueError(f\"No relation column found. Tried: {', '.join(cands)}\")\n",
    "\n",
    "def load_relations_topk_from_column(csv_path: str, top_k: int = TOP_K_REL):\n",
    "    \"\"\"\n",
    "    Expects CSV with:\n",
    "      - sentence_id (str)\n",
    "      - tokens        (list[str])\n",
    "      - relation_type (list[str]) or similar\n",
    "    Expands to token-level rows with 'relation_class' and keeps only top_k most frequent.\n",
    "    Drops tokens at word_id == 0 (first word) if EXCLUDE_INDEX_0=True.\n",
    "    \"\"\"\n",
    "    df_all = pd.read_csv(csv_path)\n",
    "    rel_col = _pick_relation_col(df_all)\n",
    "    df = df_all[[\"sentence_id\",\"tokens\", rel_col]].copy()\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df.tokens  = df.tokens.apply(_to_list)\n",
    "    df[rel_col] = df[rel_col].apply(_to_list)\n",
    "\n",
    "    rows = []\n",
    "    for sid, toks, rels in df[[\"sentence_id\",\"tokens\", rel_col]].itertuples(index=False):\n",
    "        L = min(len(toks), len(rels))\n",
    "        for wid in range(L):\n",
    "            # ---- drop first word (index 0) ----\n",
    "            if EXCLUDE_INDEX_0 and wid == 0:\n",
    "                continue\n",
    "            r = str(rels[wid])\n",
    "            rows.append((sid, wid, r, toks[wid]))\n",
    "\n",
    "    df_tok = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\",\"relation_class\",\"word\"])\n",
    "    # extra safety (if logic above ever changes)\n",
    "    if EXCLUDE_INDEX_0 and not df_tok.empty:\n",
    "        df_tok = df_tok[df_tok.word_id != 0].reset_index(drop=True)\n",
    "\n",
    "    if df_tok.empty:\n",
    "        raise ValueError(\"No token rows constructed—check that your relation column contains lists of strings.\")\n",
    "\n",
    "    # keep only top-k most frequent\n",
    "    top = df_tok.relation_class.value_counts().nlargest(top_k).index.tolist()\n",
    "    df_tok = df_tok[df_tok.relation_class.isin(top)].reset_index(drop=True)\n",
    "\n",
    "    df_sent = df[[\"sentence_id\",\"tokens\"]].drop_duplicates(\"sentence_id\")\n",
    "    return df_sent, df_tok, top\n",
    "\n",
    "def sample_raw(df_tok: pd.DataFrame, per_class_cap: int = RAW_MAX_PER_CLASS) -> pd.DataFrame:\n",
    "    picks = []\n",
    "    for c, sub in df_tok.groupby(\"relation_class\", sort=False):\n",
    "        n = min(len(sub), per_class_cap)\n",
    "        picks.append(sub.sample(n, random_state=RAND_SEED, replace=False))\n",
    "    return pd.concat(picks, ignore_index=True)\n",
    "\n",
    "def make_class_palette(classes: List[str]) -> Dict[str, Tuple[float, float, float]]:\n",
    "    base = list(sns.color_palette(\"tab20\", 20)) \\\n",
    "         + list(sns.color_palette(\"tab20b\", 20)) \\\n",
    "         + list(sns.color_palette(\"tab20c\", 20))\n",
    "    if len(base) < len(classes):\n",
    "        base = list(sns.color_palette(\"husl\", len(classes)))\n",
    "    return {cls: base[i % len(base)] for i, cls in enumerate(classes)}\n",
    "\n",
    "# =============================== EMBEDDING (GPT‑2/other) ===============================\n",
    "def embed_subset(df_sent: pd.DataFrame,\n",
    "                 subset_df: pd.DataFrame,\n",
    "                 baseline: str = BASELINE,\n",
    "                 word_rep_mode: str = WORD_REP_MODE,\n",
    "                 batch_size: int = BATCH_SIZE) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df_sent[\"sentence_id\"]   = df_sent[\"sentence_id\"].astype(str)\n",
    "    subset_df[\"sentence_id\"] = subset_df[\"sentence_id\"].astype(str)\n",
    "\n",
    "    # sid -> list[(global_idx, word_id)]\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(subset_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(str(sid), []).append((gidx, int(wid)))\n",
    "\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_sent[df_sent.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    tokzr, model, model_id_used = _safe_load_tok_and_model(baseline)\n",
    "    model = model.eval().to(device)\n",
    "    if device == \"cuda\":\n",
    "        model.half()\n",
    "\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
    "    enc_kwargs.update(getattr(tokzr, \"_safe_call_kwargs\", {}))\n",
    "\n",
    "    L = _num_hidden_layers(model) + 1   # include embedding layer\n",
    "    D = _hidden_size(model)\n",
    "    N = len(subset_df)\n",
    "\n",
    "    reps   = np.zeros((L, N, D), np.float16)\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    # choose rep mode per model family\n",
    "    gpt_like = _is_gpt_like(model)\n",
    "    if gpt_like:\n",
    "        rep_mode = word_rep_mode if word_rep_mode in {\"last\",\"mean\"} else \"last\"\n",
    "    else:\n",
    "        rep_mode = word_rep_mode if word_rep_mode in {\"first\",\"last\",\"mean\"} else \"first\"\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{model_id_used} (embed subset)\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L,B,T,D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                mp: Dict[int, List[int]] = {}\n",
    "                for tidx, wid in enumerate(enc_be.word_ids(b)):\n",
    "                    if wid is not None:\n",
    "                        mp.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = mp.get(wid)\n",
    "                    if not toks:\n",
    "                        continue\n",
    "                    if rep_mode == \"first\":\n",
    "                        vec = h[:, b, toks[0], :]\n",
    "                    elif rep_mode == \"last\":\n",
    "                        vec = h[:, b, toks[-1], :]\n",
    "                    else:  # \"mean\"\n",
    "                        vec = h[:, b, toks, :].mean(axis=1)\n",
    "                    reps[:, gidx, :] = vec.astype(np.float16, copy=False)\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    missing = int((~filled).sum())\n",
    "    if missing:\n",
    "        print(f\"⚠ Missing vectors for {missing} of {N} tokens\")\n",
    "    del model; gc.collect()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    return reps, filled\n",
    "\n",
    "# =============================== BOOTSTRAP CORE ===============================\n",
    "def _bs_layer_loop(rep_sub: np.ndarray, M: int, n_reps: int, compute_once: Callable[[np.ndarray], float]):\n",
    "    L, N, D = rep_sub.shape\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    A = np.full((n_reps, L), np.nan, np.float32)\n",
    "    for r in range(n_reps):\n",
    "        idx = rng.integers(0, N, size=M)\n",
    "        for l in range(L):\n",
    "            X = rep_sub[l, idx].astype(np.float32, copy=False)\n",
    "            try:\n",
    "                A[r, l] = float(compute_once(X))\n",
    "            except Exception:\n",
    "                A[r, l] = np.nan\n",
    "    mu = np.nanmean(A, axis=0).astype(np.float32)\n",
    "    lo = np.nanpercentile(A, 2.5, axis=0).astype(np.float32)\n",
    "    hi = np.nanpercentile(A, 97.5, axis=0).astype(np.float32)\n",
    "    return mu, lo, hi\n",
    "\n",
    "# =============================== SAVE / PLOT ===============================\n",
    "def save_metric_csv_all_classes(metric: str,\n",
    "                                class_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                                layers: np.ndarray,\n",
    "                                baseline: str,\n",
    "                                subset_name: str = \"raw\"):\n",
    "    rows = []\n",
    "    for c, stats in class_to_stats.items():\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        for l, val in enumerate(mu):\n",
    "            rows.append({\n",
    "                \"subset\": subset_name, \"model\": baseline, \"feature\": \"relation_type\",\n",
    "                \"class\": c, \"metric\": metric, \"layer\": int(layers[l]),\n",
    "                \"mean\": float(val) if np.isfinite(val) else np.nan,\n",
    "                \"ci_low\": float(lo[l]) if isinstance(lo, np.ndarray) and np.isfinite(lo[l]) else np.nan,\n",
    "                \"ci_high\": float(hi[l]) if isinstance(hi, np.ndarray) and np.isfinite(hi[l]) else np.nan,\n",
    "                \"n_tokens\": int(stats.get(\"n\", 0)), \"word_rep_mode\": WORD_REP_MODE,\n",
    "                \"source_csv\": Path(CSV_PATH).name,\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    out = CSV_DIR / f\"relation_{subset_name}_{metric}_{baseline}.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "\n",
    "def plot_metric_with_ci(class_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                        layers: np.ndarray, metric: str, title: str, out_path: Path,\n",
    "                        palette: Dict[str, Tuple[float, float, float]] | None = None,\n",
    "                        classes_order: List[str] | None = None):\n",
    "    plt.figure(figsize=(10.5, 5.5))\n",
    "    order = classes_order if classes_order is not None else sorted(class_to_stats.keys())\n",
    "    for c in order:\n",
    "        stats = class_to_stats.get(c)\n",
    "        if not stats: continue\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        if mu is None or np.all(np.isnan(mu)): continue\n",
    "        color = (palette.get(c) if isinstance(palette, dict) else None) if palette else None\n",
    "        plt.plot(layers, mu, label=c, lw=1.8, color=color)\n",
    "        if isinstance(lo, np.ndarray) and isinstance(hi, np.ndarray) and not np.all(np.isnan(lo)):\n",
    "            plt.fill_between(layers, lo, hi, alpha=0.15, color=color)\n",
    "    plt.xlabel(\"Layer\"); plt.ylabel(LABELS.get(metric, metric.upper())); plt.title(title)\n",
    "    ncol = 5 if len(order) >= 10 else 3\n",
    "    plt.legend(ncol=ncol, fontsize=\"small\", title=\"UD relation (top‑10)\", frameon=False)\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=220); plt.close()\n",
    "\n",
    "# =============================== DRIVER ===============================\n",
    "def run_relation_topk_pipeline():\n",
    "    # 1) Load token lists + top‑K relations from existing column\n",
    "    df_sent, rel_df, top_rel = load_relations_topk_from_column(CSV_PATH, top_k=TOP_K_REL)\n",
    "    classes = list(top_rel)  # keep in frequency order\n",
    "    palette = make_class_palette(classes)\n",
    "    print(f\"✓ corpus ready — {len(rel_df):,} tokens across relations {classes}\")\n",
    "    print(f\"✓ first word of each sentence removed (EXCLUDE_INDEX_0={EXCLUDE_INDEX_0})\")\n",
    "\n",
    "    # 2) Optional per-class cap (currently unlimited for fast metrics)\n",
    "    raw_df = sample_raw(rel_df, RAW_MAX_PER_CLASS)\n",
    "    print(\"Sample sizes per relation (raw cap):\")\n",
    "    counts = raw_df.relation_class.value_counts()\n",
    "    print({k: int(counts[k]) for k in classes})\n",
    "\n",
    "    # 3) Embed once\n",
    "    reps, filled = embed_subset(df_sent, raw_df, BASELINE, WORD_REP_MODE, BATCH_SIZE)\n",
    "    raw_df = raw_df.reset_index(drop=True).loc[filled].reset_index(drop=True)\n",
    "    cls_arr = raw_df.relation_class.values\n",
    "    L = reps.shape[0]; layers = np.arange(L)\n",
    "    print(f\"✓ embedded {len(raw_df):,} tokens  • layers={L}\")\n",
    "\n",
    "    # 4) Metric loop (ALL METRICS)\n",
    "    for metric in ALL_METRICS:\n",
    "        print(f\"\\n→ Computing metric: {metric} …\")\n",
    "        compute_once = FAST_ONCE.get(metric) or HEAVY_ONCE.get(metric)\n",
    "        if compute_once is None:\n",
    "            print(f\"  (skipping {metric}: estimator unavailable)\")\n",
    "            continue\n",
    "\n",
    "        n_bs = N_BOOTSTRAP_FAST if metric in FAST_ONCE else N_BOOTSTRAP_HEAVY\n",
    "        Mcap = FAST_BS_MAX_SAMP_PER_CLASS if metric in FAST_ONCE else HEAVY_BS_MAX_SAMP_PER_CLASS\n",
    "\n",
    "        class_results: Dict[str, Dict[str, np.ndarray]] = {}\n",
    "        for c in classes:\n",
    "            idx = np.where(cls_arr == c)[0]\n",
    "            if idx.size < 3:\n",
    "                continue\n",
    "            sub = reps[:, idx]  # (L, n_c, D)\n",
    "            Nc = sub.shape[1]\n",
    "            M = min(Mcap, Nc)\n",
    "            mu, lo, hi = _bs_layer_loop(sub, M, n_bs, compute_once)\n",
    "            class_results[c] = {\"mean\": mu, \"lo\": lo, \"hi\": hi, \"n\": int(Nc)}\n",
    "\n",
    "        save_metric_csv_all_classes(metric, class_results, layers, BASELINE, subset_name=\"raw\")\n",
    "        plot_metric_with_ci(class_results, layers, metric,\n",
    "                            title=f\"{LABELS.get(metric, metric.upper())} • {BASELINE}\",\n",
    "                            out_path=PLOT_DIR / f\"relation_raw_{metric}_{BASELINE}.png\",\n",
    "                            palette=palette, classes_order=classes)\n",
    "        print(f\"  ✓ saved: CSV= {CSV_DIR}/relation_raw_{metric}_{BASELINE}.csv  \"\n",
    "              f\"plot= {PLOT_DIR}/relation_raw_{metric}_{BASELINE}.png\")\n",
    "\n",
    "        del class_results; gc.collect()\n",
    "        if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    del reps; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    print(\"\\n✓ done (incremental outputs produced per metric).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_relation_topk_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fbbcbd-7afb-49d1-8a90-2a5503033d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, GPT2TokenizerFast\n",
    "\n",
    "# Plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.colors as pc\n",
    "\n",
    "# =============================== CONFIG ===============================\n",
    "CSV_PATH       = \"en_ewt-ud-train_sentences.csv\"   # needs: sentence_id, tokens (list[str]), relation_type (list[str]) or similar\n",
    "REL_COL_HINT   = \"relation_type\"                   # will try common alternatives if this is absent\n",
    "TOP_K_REL      = 10                                # keep top-K relations (legend size / clarity)\n",
    "\n",
    "BASELINE       = \"gpt2\"                            # or \"bert-base-uncased\"\n",
    "WORD_REP_MODE  = \"last\"                            # BERT: {\"first\",\"last\",\"mean\"}; GPT-2: {\"last\",\"mean\"}\n",
    "\n",
    "# Optional: subsample for plotting smoothness (per class)\n",
    "PCA_MAX_PER_CLASS = None                           # e.g., 2000; None = use all tokens\n",
    "\n",
    "# Output\n",
    "OUT_DIR  = Path(\"pca3d_relation_type\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Throughput / device\n",
    "BATCH_SIZE = 2\n",
    "RAND_SEED  = 42\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Work offline if requested (pre-cached models)\n",
    "LOCAL_ONLY = (\n",
    "    os.environ.get(\"TRANSFORMERS_OFFLINE\") == \"1\"\n",
    "    or os.environ.get(\"HF_HUB_OFFLINE\") == \"1\"\n",
    ")\n",
    "\n",
    "# =============================== HELPERS ===============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _num_hidden_layers(model) -> int:\n",
    "    n = getattr(model.config, \"num_hidden_layers\", None)\n",
    "    if n is None: n = getattr(model.config, \"n_layer\", None)   # GPT-2\n",
    "    if n is None: raise ValueError(\"Cannot determine num_hidden_layers\")\n",
    "    return int(n)\n",
    "\n",
    "def _hidden_size(model) -> int:\n",
    "    d = getattr(model.config, \"hidden_size\", None)\n",
    "    if d is None: d = getattr(model.config, \"n_embd\", None)    # GPT-2\n",
    "    if d is None: raise ValueError(\"Cannot determine hidden size\")\n",
    "    return int(d)\n",
    "\n",
    "def _pick_relation_col(df: pd.DataFrame) -> str:\n",
    "    cands = [REL_COL_HINT, \"typed_dependency\", \"relation\", \"deprel\", \"ud_rel\", \"rel\", \"REL\", \"Rel\"]\n",
    "    for c in cands:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise ValueError(f\"No relation column found. Tried: {', '.join(cands)}\")\n",
    "\n",
    "def load_relations_topk_from_column(csv_path: str, top_k: int = TOP_K_REL):\n",
    "    \"\"\"\n",
    "    Expand per-sentence lists -> token rows and keep only top_k frequent relations.\n",
    "    Returns:\n",
    "      df_sent: sentence_id + tokens\n",
    "      df_tok : sentence_id, word_id, relation_class, word\n",
    "      top    : list[str] relation labels kept (by frequency)\n",
    "    \"\"\"\n",
    "    df_all = pd.read_csv(csv_path)\n",
    "    rel_col = _pick_relation_col(df_all)\n",
    "    df = df_all[[\"sentence_id\", \"tokens\", rel_col]].copy()\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df.tokens = df.tokens.apply(_to_list)\n",
    "    df[rel_col] = df[rel_col].apply(_to_list)\n",
    "\n",
    "    rows = []\n",
    "    for sid, toks, rels in df[[\"sentence_id\",\"tokens\", rel_col]].itertuples(index=False):\n",
    "        L = min(len(toks), len(rels))\n",
    "        for wid in range(L):\n",
    "            r = str(rels[wid])\n",
    "            rows.append((sid, wid, r, toks[wid]))\n",
    "    df_tok = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\",\"relation_class\",\"word\"])\n",
    "    if df_tok.empty:\n",
    "        raise ValueError(\"No token rows constructed—check your relation column contains lists.\")\n",
    "    top = df_tok.relation_class.value_counts().nlargest(top_k).index.tolist()\n",
    "    df_tok = df_tok[df_tok.relation_class.isin(top)].reset_index(drop=True)\n",
    "    df_sent = df[[\"sentence_id\",\"tokens\"]].drop_duplicates(\"sentence_id\")\n",
    "    return df_sent, df_tok, top\n",
    "\n",
    "def sample_per_class(df_tok: pd.DataFrame, per_class_cap: int | None) -> pd.DataFrame:\n",
    "    \"\"\"Optional per-class subsample for plotting.\"\"\"\n",
    "    if per_class_cap is None:\n",
    "        return df_tok.reset_index(drop=True)\n",
    "    picks = []\n",
    "    for c, sub in df_tok.groupby(\"relation_class\", sort=False):\n",
    "        n = min(len(sub), per_class_cap)\n",
    "        picks.append(sub.sample(n, random_state=RAND_SEED, replace=False))\n",
    "    return pd.concat(picks, ignore_index=True)\n",
    "\n",
    "# ---------- Robust loader (fixes GPT-2 pitfalls) ----------\n",
    "def _load_tok_and_model(model_id: str):\n",
    "    \"\"\"\n",
    "    Tries a few candidates, forces a Fast tokenizer, sets PAD=EOS for GPT‑2,\n",
    "    and returns (tokenizer, model, resolved_id).\n",
    "    \"\"\"\n",
    "    candidates = [model_id]\n",
    "    if model_id.lower() in {\"gpt2\", \"gpt-2\"}:\n",
    "        # The canonical repo is \"openai-community/gpt2\" on HF Hub; also try distilgpt2\n",
    "        candidates += [\"openai-community/gpt2\", \"distilgpt2\"]\n",
    "\n",
    "    last_err = None\n",
    "    for mid in candidates:\n",
    "        try:\n",
    "            if \"gpt2\" in mid.lower():\n",
    "                tok = GPT2TokenizerFast.from_pretrained(\n",
    "                    mid, add_prefix_space=True, local_files_only=LOCAL_ONLY\n",
    "                )\n",
    "            else:\n",
    "                tok = AutoTokenizer.from_pretrained(\n",
    "                    mid, use_fast=True, add_prefix_space=True, local_files_only=LOCAL_ONLY\n",
    "                )\n",
    "            # Right-padding + pad token (GPT‑2 has no pad token by default)\n",
    "            if getattr(tok, \"padding_side\", None) != \"right\":\n",
    "                tok.padding_side = \"right\"\n",
    "            if tok.pad_token is None and getattr(tok, \"eos_token\", None) is not None:\n",
    "                tok.pad_token = tok.eos_token  # <- critical for GPT‑2 batching\n",
    "            mdl = AutoModel.from_pretrained(\n",
    "                mid, output_hidden_states=True, local_files_only=LOCAL_ONLY\n",
    "            )\n",
    "            if getattr(mdl.config, \"pad_token_id\", None) is None and tok.pad_token_id is not None:\n",
    "                mdl.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "            mdl = mdl.eval().to(device)\n",
    "            if device == \"cuda\":\n",
    "                mdl.half()\n",
    "            return tok, mdl, mid\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    raise RuntimeError(\n",
    "        \"Could not load tokenizer/model. Attempts:\\n  \" +\n",
    "        \"\\n  \".join(candidates) +\n",
    "        f\"\\nLast error: {repr(last_err)}\"\n",
    "    )\n",
    "\n",
    "def embed_subset(df_sent: pd.DataFrame,\n",
    "                 subset_df: pd.DataFrame,\n",
    "                 baseline: str = BASELINE,\n",
    "                 word_rep_mode: str = WORD_REP_MODE,\n",
    "                 batch_size: int = BATCH_SIZE) -> Tuple[np.ndarray, np.ndarray, str]:\n",
    "    \"\"\"\n",
    "    Return (reps (L,N,D), filled mask (N,), resolved_model_id).\n",
    "    \"\"\"\n",
    "    df_sent[\"sentence_id\"]   = df_sent[\"sentence_id\"].astype(str)\n",
    "    subset_df[\"sentence_id\"] = subset_df[\"sentence_id\"].astype(str)\n",
    "\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(subset_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(str(sid), []).append((gidx, int(wid)))\n",
    "\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_sent[df_sent.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    tokzr, model, resolved_id = _load_tok_and_model(baseline)\n",
    "\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "        enc_kwargs[\"add_prefix_space\"] = True\n",
    "\n",
    "    L = _num_hidden_layers(model) + 1   # include embeddings\n",
    "    D = _hidden_size(model)\n",
    "    N = len(subset_df)\n",
    "\n",
    "    reps   = np.zeros((L, N, D), np.float16)\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{resolved_id} (embed subset)\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L,B,T,D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                mp = {}\n",
    "                wids = enc_be.word_ids(b)  # needs Fast tokenizer\n",
    "                if wids is None:\n",
    "                    raise RuntimeError(\"Fast tokenizer required (word_ids unavailable).\")\n",
    "                for tidx, wid in enumerate(wids):\n",
    "                    if wid is not None:\n",
    "                        mp.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = mp.get(wid)\n",
    "                    if not toks: continue\n",
    "                    if word_rep_mode == \"first\":\n",
    "                        vec = h[:, b, toks[0], :]\n",
    "                    elif word_rep_mode == \"last\":\n",
    "                        vec = h[:, b, toks[-1], :]\n",
    "                    elif word_rep_mode == \"mean\":\n",
    "                        vec = h[:, b, toks, :].mean(axis=1)\n",
    "                    else:\n",
    "                        raise ValueError(\"WORD_REP_MODE must be in {'first','last','mean'}.\")\n",
    "                    reps[:, gidx, :] = vec.astype(np.float16, copy=False)\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    missing = int((~filled).sum())\n",
    "    if missing:\n",
    "        print(f\"⚠ Missing vectors for {missing} tokens (skipped in PCA).\")\n",
    "        reps = reps[:, filled]\n",
    "    del model; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    return reps, filled, resolved_id\n",
    "\n",
    "# =============================== PCA 3D PER LAYER ===============================\n",
    "def _pca3d_layer(X: np.ndarray, n_components: int = 3) -> np.ndarray:\n",
    "    \"\"\"Lightweight PCA to 3D via SVD (no sklearn dependency).\"\"\"\n",
    "    X = X.astype(np.float32, copy=False)\n",
    "    Xc = X - X.mean(0, keepdims=True)\n",
    "    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "    return (U[:, :n_components] * S[:n_components]).astype(np.float32, copy=False)  # (n,3)\n",
    "\n",
    "def _qual_palette_for_classes(classes: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Build a large discrete palette:\n",
    "    1) stack multiple qualitative sets,\n",
    "    2) if still not enough, sample a continuous scale.\n",
    "    \"\"\"\n",
    "    seqs = [\n",
    "        px.colors.qualitative.Alphabet,   # 26\n",
    "        px.colors.qualitative.Dark24,     # 24\n",
    "        px.colors.qualitative.Light24,    # 24\n",
    "        px.colors.qualitative.Set3,       # 12\n",
    "        px.colors.qualitative.Bold,       # 10\n",
    "        px.colors.qualitative.Set2,       # 8\n",
    "        px.colors.qualitative.Set1,       # 9\n",
    "        px.colors.qualitative.Pastel2,    # 8\n",
    "        px.colors.qualitative.Pastel1,    # 9\n",
    "        px.colors.qualitative.Safe,       # 11\n",
    "        px.colors.qualitative.Vivid,      # 11\n",
    "        px.colors.qualitative.D3,         # 10\n",
    "    ]\n",
    "    pool = []\n",
    "    for s in seqs:\n",
    "        pool.extend(s)\n",
    "    k = len(classes)\n",
    "    if len(pool) < k:\n",
    "        t = np.linspace(0.0, 1.0, k, endpoint=True)\n",
    "        cont = [pc.sample_colorscale(\"Turbo\", [ti])[0] for ti in t]\n",
    "        colors = cont\n",
    "    else:\n",
    "        colors = pool[:k]\n",
    "    return {cls: colors[i] for i, cls in enumerate(classes)}\n",
    "\n",
    "def pca3d_by_relation_and_plot(reps: np.ndarray,\n",
    "                               words: List[str],\n",
    "                               classes_arr: np.ndarray,\n",
    "                               all_classes: List[str],\n",
    "                               model_tag: str,\n",
    "                               html_out: Path):\n",
    "    \"\"\"\n",
    "    Build one 3D scatter trace per class per layer, with a layer slider.\n",
    "    \"\"\"\n",
    "    L, N, D = reps.shape\n",
    "    print(f\"PCA plotting on {N:,} tokens across {L} layers…\")\n",
    "    # PCA per layer\n",
    "    Y_layers: List[np.ndarray] = []\n",
    "    for l in range(L):\n",
    "        Y_layers.append(_pca3d_layer(reps[l]))  # (N,3)\n",
    "\n",
    "    cmap = _qual_palette_for_classes(all_classes)\n",
    "    traces = []\n",
    "    for l in range(L):\n",
    "        Y = Y_layers[l]\n",
    "        show_legend = (l == 0)\n",
    "        for j, c in enumerate(all_classes):\n",
    "            mask = (classes_arr == c)\n",
    "            if not np.any(mask):\n",
    "                x = y = z = []\n",
    "                hov = []\n",
    "            else:\n",
    "                x, y, z = Y[mask, 0], Y[mask, 1], Y[mask, 2]\n",
    "                hov = [f\"{w} | rel={c}\" for w in np.asarray(words)[mask]]\n",
    "            traces.append(\n",
    "                go.Scatter3d(\n",
    "                    x=x, y=y, z=z,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=2, opacity=0.75, color=cmap[c]),\n",
    "                    name=f\"{c} (Layer {l})\",\n",
    "                    hovertext=hov,\n",
    "                    # NOTE: not an f-string; contains %{...} tokens for Plotly\n",
    "                    hovertemplate=(\n",
    "                        \"<b>%{hovertext}</b><br>\"\n",
    "                        \"x=%{x:.3f}<br>y=%{y:.3f}<br>z=%{z:.3f}\"\n",
    "                        \"<extra></extra>\"\n",
    "                    ),\n",
    "                    visible=(l == 0),\n",
    "                    showlegend=show_legend,\n",
    "                    legendgroup=c\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Slider: toggle visibility for the traces of the selected layer\n",
    "    n_per_layer = len(all_classes)\n",
    "    n_total = n_per_layer * L\n",
    "    steps = []\n",
    "    for l in range(L):\n",
    "        vis = [False] * n_total\n",
    "        s = l * n_per_layer\n",
    "        vis[s : s + n_per_layer] = [True] * n_per_layer\n",
    "        steps.append(dict(\n",
    "            method=\"update\",\n",
    "            args=[{\"visible\": vis},\n",
    "                  {\"title\": f\"{model_tag} • PCA 3D by relation_type • Layer {l} (drag to rotate)\"}],\n",
    "            label=str(l),\n",
    "        ))\n",
    "\n",
    "    sliders = [dict(\n",
    "        active=0, steps=steps,\n",
    "        currentvalue={\"prefix\": \"Layer: \"},\n",
    "        pad={\"t\": 10}\n",
    "    )]\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title=f\"{model_tag} • PCA 3D by relation_type • Layer 0 (drag to rotate)\",\n",
    "        scene=dict(xaxis_title=\"PC1\", yaxis_title=\"PC2\", zaxis_title=\"PC3\", aspectmode=\"data\"),\n",
    "        margin=dict(l=0, r=0, b=0, t=40),\n",
    "        sliders=sliders,\n",
    "        showlegend=True,\n",
    "        legend=dict(title=\"relation_type (Top‑K)\", itemsizing=\"trace\")\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "    fig.show()\n",
    "    html_out = Path(html_out)  # ensure Path-like\n",
    "    fig.write_html(str(html_out), include_plotlyjs=\"cdn\")\n",
    "    print(\"✓ Saved interactive HTML to:\", html_out.resolve())\n",
    "\n",
    "# =============================== DRIVER ===============================\n",
    "def run_pca3d_relation_type():\n",
    "    # 1) Load top‑K relation classes\n",
    "    df_sent, rel_df, top_rel = load_relations_topk_from_column(CSV_PATH, top_k=TOP_K_REL)\n",
    "    classes = list(top_rel)  # keep frequency order\n",
    "    print(f\"✓ plotting subset — {len(rel_df):,} tokens across relation classes {classes}\")\n",
    "\n",
    "    # 2) Optional per-class cap\n",
    "    raw_df = sample_per_class(rel_df, PCA_MAX_PER_CLASS)\n",
    "\n",
    "    # 3) Embed once\n",
    "    reps, filled, resolved_id = embed_subset(df_sent, raw_df, BASELINE, WORD_REP_MODE, BATCH_SIZE)\n",
    "    raw_df = raw_df.reset_index(drop=True).loc[filled].reset_index(drop=True)\n",
    "\n",
    "    # Labels and words for hover\n",
    "    cls_arr = raw_df.relation_class.values.astype(str)\n",
    "    words   = raw_df.word.astype(str).tolist()\n",
    "\n",
    "    # 4) PCA→3D per layer + Plotly\n",
    "    html_out = OUT_DIR / f\"{resolved_id.replace('/','_')}_relation_type_pca3d_layers.html\"\n",
    "    pca3d_by_relation_and_plot(\n",
    "        reps.astype(np.float32, copy=False), words, cls_arr, classes,\n",
    "        model_tag=resolved_id, html_out=html_out\n",
    "    )\n",
    "\n",
    "    # Cleanup\n",
    "    del reps; gc.collect()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pca3d_relation_type()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a330db-d10f-459e-b107-65ffb286405a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
