{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b256994f",
   "metadata": {},
   "source": [
    "# Outlier dimensions (plot-only demo)\n",
    "\n",
    "This notebook is a **lightweight, plot-only** version of the outlier-dimension experiment.\n",
    "\n",
    "- It reproduces the *idea* of the analysis (word-level pooling â†’ layerwise mean activation per dimension â†’ outlier threshold `Î¼ + 3Ïƒ` â†’ plot).\n",
    "- It intentionally **does not** write intermediate embeddings to disk (no memmaps / caches).  \n",
    "  If you need the scalable version, keep it in a separate script/notebook.\n",
    "\n",
    "**Method (sketch)**  \n",
    "1. Tokenize each sentence as a *pre-tokenized list of words* and use `word_ids()` to align sub-tokens back to words.  \n",
    "2. Pool sub-tokens into a single word vector (FIRST for BERT-like encoders, LAST for GPT-like decoders).  \n",
    "3. For each layer `â„“` and dimension `i`, compute the corpus mean activation `v_i(â„“)` across words.  \n",
    "4. Flatten all `v_i(â„“)` across layers/dims, compute global `Î¼` and `Ïƒ`, and mark outliers where `v_i(â„“) â‰¥ Î¼ + 3Ïƒ`.  \n",
    "5. Plot number of outlier dimensions per layer, and overlap with the previous layer.\n",
    "\n",
    "References:\n",
    "- Outlier-dimension criterion `Î¼ + 3Ïƒ`: Kovaleva et al. (2021), *BERT Busters*.  \n",
    "- Word/sub-token alignment via `is_split_into_words=True` and `word_ids()`: ðŸ¤— Transformers docs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690956c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# --------------------\n",
    "# Config (edit me)\n",
    "# --------------------\n",
    "DATA_PATH = \"data/ewt_tokens.csv\"   # must contain columns: sentence_id, tokens (list-like)\n",
    "MAX_SENTENCES = 500                # keep small if you just want the plot quickly\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Models to compare: pooling is the word-level aggregation strategy\n",
    "MODELS = [\n",
    "    {\"name\": \"bert-base-uncased\", \"pooling\": \"first\"},\n",
    "    {\"name\": \"gpt2\", \"pooling\": \"last\"},\n",
    "]\n",
    "\n",
    "# Plot settings\n",
    "FIGSIZE = (8, 4)\n",
    "SAVE_FIG = False\n",
    "FIG_PATH = \"outlier_dims.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eac534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Load data\n",
    "# --------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# `tokens` can be a Python list, or a string that can be parsed with ast.literal_eval\n",
    "def _parse_tokens(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip()\n",
    "        if x.startswith(\"[\") and x.endswith(\"]\"):\n",
    "            return ast.literal_eval(x)\n",
    "        # fallback: whitespace split\n",
    "        return x.split()\n",
    "    raise TypeError(f\"Unsupported tokens type: {type(x)}\")\n",
    "\n",
    "sentences = [ _parse_tokens(x) for x in df[\"tokens\"].tolist() ]\n",
    "if MAX_SENTENCES is not None:\n",
    "    sentences = sentences[:MAX_SENTENCES]\n",
    "\n",
    "print(f\"Loaded {len(sentences)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7b614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layerwise_mean_activation(\n",
    "    sentences,\n",
    "    model_name: str,\n",
    "    pooling: str = \"first\",   # \"first\" or \"last\" (word-level pooling)\n",
    "    device: str = \"cpu\",\n",
    "    max_length: int = 512,\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    layer_sum = None   # (L, D) float64\n",
    "    n_words_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for words in sentences:\n",
    "            enc = tokenizer(\n",
    "                words,\n",
    "                is_split_into_words=True,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "            )\n",
    "            word_ids = enc.word_ids(batch_index=0)\n",
    "            enc = enc.to(device)\n",
    "\n",
    "            out = model(**enc, output_hidden_states=True)\n",
    "            hidden_states = out.hidden_states  # tuple of length L; each (1, T, D)\n",
    "\n",
    "            # lazily initialize accumulator\n",
    "            if layer_sum is None:\n",
    "                L = len(hidden_states)\n",
    "                D = hidden_states[0].shape[-1]\n",
    "                layer_sum = np.zeros((L, D), dtype=np.float64)\n",
    "\n",
    "            # pick one representative sub-token index per word (FIRST or LAST)\n",
    "            word_pos = {}\n",
    "            for tok_i, wid in enumerate(word_ids):\n",
    "                if wid is None:\n",
    "                    continue\n",
    "                if pooling == \"first\":\n",
    "                    word_pos.setdefault(wid, tok_i)\n",
    "                elif pooling == \"last\":\n",
    "                    word_pos[wid] = tok_i\n",
    "                else:\n",
    "                    raise ValueError(\"pooling must be 'first' or 'last' in this demo\")\n",
    "\n",
    "            # keep positions in word order\n",
    "            max_wid = max(word_pos.keys()) if word_pos else -1\n",
    "            positions = [word_pos[i] for i in range(max_wid + 1) if i in word_pos]\n",
    "            if not positions:\n",
    "                continue\n",
    "\n",
    "            # accumulate per-layer sums over the selected word vectors\n",
    "            for l, hs in enumerate(hidden_states):\n",
    "                # hs: (1, T, D) -> select (W, D)\n",
    "                sel = hs[0, positions, :]          # (W, D)\n",
    "                layer_sum[l] += sel.sum(dim=0).detach().cpu().numpy()\n",
    "\n",
    "            n_words_total += len(positions)\n",
    "\n",
    "    layer_mean = layer_sum / max(n_words_total, 1)\n",
    "    return layer_mean  # (L, D)\n",
    "\n",
    "\n",
    "def outlier_dims_from_layer_means(layer_means: np.ndarray, z: float = 3.0):\n",
    "    # layer_means: (L, D)\n",
    "    flat = layer_means.reshape(-1)\n",
    "    mu = float(flat.mean())\n",
    "    sigma = float(flat.std(ddof=0))\n",
    "    thr = mu + z * sigma\n",
    "\n",
    "    outlier_mask = layer_means >= thr  # (L, D) boolean\n",
    "    outlier_counts = outlier_mask.sum(axis=1)  # (L,)\n",
    "\n",
    "    # overlap with previous layer's outliers\n",
    "    overlap_prev = np.zeros_like(outlier_counts)\n",
    "    overlap_prev[1:] = (outlier_mask[1:] & outlier_mask[:-1]).sum(axis=1)\n",
    "\n",
    "    # mean exceedance among outlier dims in each layer (for optional bubble sizing)\n",
    "    exceed = np.maximum(0.0, layer_means - thr)\n",
    "    mean_exceed = np.zeros(layer_means.shape[0], dtype=np.float64)\n",
    "    for l in range(layer_means.shape[0]):\n",
    "        if outlier_counts[l] > 0:\n",
    "            mean_exceed[l] = exceed[l][outlier_mask[l]].mean()\n",
    "\n",
    "    return {\"mu\": mu, \"sigma\": sigma, \"thr\": thr,\n",
    "            \"mask\": outlier_mask, \"counts\": outlier_counts,\n",
    "            \"overlap_prev\": overlap_prev, \"mean_exceed\": mean_exceed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d56bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for cfg in MODELS:\n",
    "    name = cfg[\"name\"]\n",
    "    pooling = cfg[\"pooling\"]\n",
    "    print(f\"Running: {name} ({pooling})\")\n",
    "\n",
    "    layer_means = layerwise_mean_activation(\n",
    "        sentences,\n",
    "        model_name=name,\n",
    "        pooling=pooling,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    stats = outlier_dims_from_layer_means(layer_means, z=3.0)\n",
    "    stats[\"model\"] = name\n",
    "    stats[\"pooling\"] = pooling\n",
    "    stats[\"n_layers\"] = layer_means.shape[0]\n",
    "    stats[\"hidden_size\"] = layer_means.shape[1]\n",
    "    results.append(stats)\n",
    "\n",
    "    print(f\"  layers={stats['n_layers']}  hidden={stats['hidden_size']}  thr={stats['thr']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853c0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Plot: # outlier dims per layer (+ overlap with previous)\n",
    "# --------------------\n",
    "plt.figure(figsize=FIGSIZE)\n",
    "\n",
    "for r in results:\n",
    "    L = r[\"n_layers\"]\n",
    "    x = np.arange(L)\n",
    "    plt.plot(x, r[\"counts\"], marker=\"o\", linewidth=1.5, label=f\"{r['model']} ({r['pooling']})\")\n",
    "    plt.plot(x, r[\"overlap_prev\"], linestyle=\"--\", linewidth=1.0, alpha=0.8,\n",
    "             label=f\"{r['model']} overlap(prev)\")\n",
    "\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"# outlier dimensions\")\n",
    "plt.title(\"Outlier dimensions across layers (threshold Î¼ + 3Ïƒ)\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(FIG_PATH, dpi=200)\n",
    "    print(f\"Saved: {FIG_PATH}\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
