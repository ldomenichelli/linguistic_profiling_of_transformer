{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fccdd2f-009b-403c-a55e-4cd573675633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 09:55:19.165274: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np, pandas as pd, torch\n",
    "import torch.utils.data as torchdata\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel, BertModel, AutoConfig\n",
    "\n",
    "from IsoScore import IsoScore\n",
    "from dadapy import Data\n",
    "from skdim.id import MLE, MOM, TLE, CorrInt, FisherS, lPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a417da-9d25-405b-8ed3-f6a427b7cacd",
   "metadata": {},
   "source": [
    "## Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e7d0111-36ea-4865-bb46-74925fdc7006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Loading corpus …\n",
      "  tokens available after POS filter: 189,167\n",
      "• Using 4 shards × 5000 words = 20,000 tokens\n",
      "\n",
      "=== Shard 1/4 (N=5,000) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embed (bert-base-uncased) shard: 100%|████████| 231/231 [00:05<00:00, 41.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: your data might contain duplicate rows, which can affect results\n",
      "Warning: your data might contain duplicate rows, which can affect results\n",
      "Warning: your data might contain duplicate rows, which can affect results\n",
      "Warning: your data might contain duplicate rows, which can affect results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import annotations\n",
    "\n",
    "# ------------------------- CONFIG YOU LIKELY WANT TO TWEAK -------------------------\n",
    "CSV_PATH         = \"en_ewt-ud-train_sentences.csv\"\n",
    "BASELINE         = \"bert-base-uncased\"\n",
    "WORD_REP_MODE    = \"first\"               # {\"first\",\"last\",\"mean\"}\n",
    "EXCLUDE_POS      = {\"X\",\"SYM\",\"INTJ\",\"PART\"}\n",
    "\n",
    "MIN_PER_SHARD    = 5000                  # <-- your “metrics need ≥5k samples” requirement\n",
    "# --- correlation points: want ≥ 47 ---\n",
    "# For BERT-base, L = 13 layers  => set shards to 4 → 13 * 4 = 52 points\n",
    "SHARDS_K = 4\n",
    "\n",
    "# --- heavy metrics need ≥ 5000 samples per layer ---\n",
    "HEAVY_MAX_PER_LAYER = 5000\n",
    "\n",
    "# Ensure each shard can supply ~5000 tokens per layer:\n",
    "# N_TOTAL_CAP should be at least 5000 * SHARDS_K\n",
    "N_TOTAL_CAP = max(N_TOTAL_CAP, 5000 * SHARDS_K)  # e.g., 20_000 for 4 shards\n",
    "\n",
    "BATCH_SIZE       = 16                    # embedding batch size (tune for your GPU/CPU)\n",
    "RAND_SEED        = 42\n",
    "DADAPY_GRID_RANGE= 32                    # GRIDE neighborhood range\n",
    "HEAVY_MIN_PER_L  = 5000                  # per-layer sample floor for heavy metrics\n",
    "HEAVY_MAX_PER_L  = 5000                  # also cap at 5k to bound RAM/time\n",
    "DUP_JITTER_EPS   = 1e-6                  # tiny noise to avoid duplicate-row warnings\n",
    "\n",
    "# Plot / output\n",
    "NORMALIZE_HEATMAP_01 = False             # set True to map [-1,1] -> [0,1] in heatmaps\n",
    "OUT_DIR  = Path(\"metric_corr_pos\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOT_DIR = OUT_DIR / \"plots\";  PLOT_DIR.mkdir(exist_ok=True)\n",
    "CSV_DIR  = OUT_DIR / \"tables\"; CSV_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "# Stats + multiple testing\n",
    "from scipy.stats import spearmanr\n",
    "HAS_STATSMODELS = False\n",
    "try:\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    HAS_STATSMODELS = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# System / libs\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Optional deps\n",
    "HAS_ISOSCORE = False\n",
    "try:\n",
    "    from isoscore import IsoScore\n",
    "    HAS_ISOSCORE = True\n",
    "except Exception:\n",
    "    class _IsoScoreFallback:\n",
    "        @staticmethod\n",
    "        def IsoScore(X: np.ndarray) -> float:\n",
    "            C = np.cov(X.T, ddof=0)\n",
    "            ev = np.linalg.eigvalsh(C)\n",
    "            if ev.mean() <= 0 or ev[-1] <= 0: return 0.0\n",
    "            return float(np.clip(ev.mean() / ev[-1], 0.0, 1.0))\n",
    "    IsoScore = _IsoScoreFallback()\n",
    "\n",
    "HAS_DADAPY = False\n",
    "try:\n",
    "    from dadapy import Data\n",
    "    HAS_DADAPY = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "HAS_SKDIM = False\n",
    "try:\n",
    "    from skdim.id import (\n",
    "        MOM, TLE, CorrInt, FisherS, lPCA, MLE, ESS, MADA, KNN\n",
    "    )\n",
    "    HAS_SKDIM = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Repro & device\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": torch.backends.cudnn.benchmark = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "\n",
    "# ------------------------------ utility helpers ------------------------------\n",
    "def _model_tag(name: str) -> str:\n",
    "    return name.split(\"/\")[-1].replace(\":\", \"_\")\n",
    "\n",
    "def _to_list(x):\n",
    "    if isinstance(x, str) and x.startswith(\"[\"):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except Exception:\n",
    "            return []\n",
    "    return x\n",
    "\n",
    "def _center(X): return X - X.mean(0, keepdims=True)\n",
    "\n",
    "\n",
    "def _has_dupes(X: np.ndarray) -> bool:\n",
    "    try:\n",
    "        return np.unique(X, axis=0).shape[0] < X.shape[0]\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _jitter_unique(X: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return X with exact duplicates removed and (if needed) tiny noise added\n",
    "    to break remaining ties. All operations in float32 to avoid float16 ties.\n",
    "    \"\"\"\n",
    "    X = X.astype(np.float32, copy=False)\n",
    "\n",
    "    # Remove exact duplicates first (cheap and stable)\n",
    "    try:\n",
    "        Xu = np.unique(X, axis=0)\n",
    "    except Exception:\n",
    "        Xu = X  # fallback if axis=0 unique unsupported (older numpy)\n",
    "\n",
    "    # If we dropped many rows or still suspect ties, add tiny jitter\n",
    "    if Xu.shape[0] < X.shape[0] or _has_dupes(Xu):\n",
    "        noise = np.random.normal(scale=eps, size=Xu.shape).astype(np.float32)\n",
    "        Xu = Xu + noise\n",
    "        # Optionally enforce uniqueness again (mostly for peace of mind)\n",
    "        try:\n",
    "            Xu = np.unique(Xu, axis=0)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return Xu\n",
    "def _prep_for_knn(\n",
    "    X: np.ndarray,\n",
    "    cap: int | None,\n",
    "    rng: np.random.Generator = np.random.default_rng(42),\n",
    "    jitter_scale: float = 1e-6,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    1) Make contiguous float32\n",
    "    2) Drop exact duplicate rows\n",
    "    3) Optionally subsample to 'cap'\n",
    "    4) Add tiny jitter big enough for float32 to break remaining ties\n",
    "    \"\"\"\n",
    "    X = np.ascontiguousarray(X, dtype=np.float32)\n",
    "\n",
    "    # Drop exact duplicates first (fast, low‑mem)\n",
    "    X = np.unique(X, axis=0)\n",
    "\n",
    "    # Subsample AFTER dedup (so cap doesn't reintroduce dup indices)\n",
    "    if (cap is not None) and (X.shape[0] > cap):\n",
    "        idx = rng.choice(X.shape[0], cap, replace=False)\n",
    "        X = X[idx]\n",
    "\n",
    "    # Add small jitter (scaled to global std; ensures it survives float32)\n",
    "    std = float(X.std()) or 1.0\n",
    "    eps = jitter_scale * std\n",
    "    X = X + rng.normal(0.0, eps, size=X.shape).astype(np.float32)\n",
    "    return X\n",
    "\n",
    "\n",
    "def load_all_words(csv_path: str, exclude_pos: set[str] = EXCLUDE_POS) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df = pd.read_csv(csv_path, usecols=[\"sentence_id\",\"tokens\",\"pos\"])\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df.tokens = df.tokens.apply(_to_list); df.pos = df.pos.apply(_to_list)\n",
    "    rows = []\n",
    "    for sid, toks, poss in df[[\"sentence_id\",\"tokens\",\"pos\"]].itertuples(index=False):\n",
    "        for wid, p in enumerate(poss):\n",
    "            if (exclude_pos is None) or (p not in exclude_pos):\n",
    "                rows.append((sid, wid))\n",
    "    all_words = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\"])\n",
    "    return df, all_words\n",
    "\n",
    "def pick_words_for_shards(all_words: pd.DataFrame,\n",
    "                          shards_k: int,\n",
    "                          min_per_shard: int,\n",
    "                          cap: int | None = N_TOTAL_CAP) -> list[pd.DataFrame]:\n",
    "    \"\"\"Return a list of shard DataFrames, each with exactly min_per_shard rows.\"\"\"\n",
    "    total_needed = shards_k * min_per_shard\n",
    "    if cap is None or cap < total_needed:\n",
    "        cap = total_needed\n",
    "    if len(all_words) < total_needed:\n",
    "        raise ValueError(f\"Not enough tokens: need {total_needed}, have {len(all_words)}.\")\n",
    "\n",
    "    pool = all_words.sample(cap, random_state=RAND_SEED).reset_index(drop=True)\n",
    "    idx = np.arange(len(pool))\n",
    "    np.random.default_rng(RAND_SEED).shuffle(idx)\n",
    "    shards = []\n",
    "    for s in range(shards_k):\n",
    "        start = s * min_per_shard\n",
    "        end   = start + min_per_shard\n",
    "        sel   = pool.iloc[idx[start:end]].reset_index(drop=True)\n",
    "        shards.append(sel)\n",
    "    return shards\n",
    "\n",
    "# ------------------------------ embedding (per shard) ------------------------------\n",
    "def embed_words(df_sent: pd.DataFrame, words_df: pd.DataFrame,\n",
    "                baseline: str = BASELINE, word_rep_mode: str = WORD_REP_MODE,\n",
    "                batch_size: int = BATCH_SIZE) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Embed ONLY the words in words_df and return reps of shape (L, N, D) for this shard.\n",
    "    \"\"\"\n",
    "    # Build sid -> list[(gidx, wid)]\n",
    "    words_df[\"sentence_id\"] = words_df[\"sentence_id\"].astype(str)\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(words_df.itertuples(index=False)):\n",
    "        by_sid.setdefault(sid, []).append((gidx, int(wid)))\n",
    "\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_sent[df_sent.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    tokzr = AutoTokenizer.from_pretrained(baseline, use_fast=True)\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
    "    if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "        enc_kwargs[\"add_prefix_space\"] = True\n",
    "\n",
    "    model = AutoModel.from_pretrained(baseline, output_hidden_states=True).eval().to(device)\n",
    "    if device == \"cuda\": model.half()\n",
    "\n",
    "    # model dims\n",
    "    L = (getattr(model.config, \"num_hidden_layers\", None) or getattr(model.config, \"n_layer\", 0)) + 1\n",
    "    D = (getattr(model.config, \"hidden_size\", None)     or getattr(model.config, \"n_embd\", 0))\n",
    "    N = len(words_df)\n",
    "\n",
    "    reps   = np.zeros((L, N, D), np.float16)   #reps   = np.zeros((L, N, D), np.float32)\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    tag = _model_tag(baseline)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"Embed ({tag}) shard\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L,B,T,D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                mp = {}\n",
    "                for tidx, wid in enumerate(enc_be.word_ids(b)):\n",
    "                    if wid is not None:\n",
    "                        mp.setdefault(int(wid), []).append(int(tidx))\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = mp.get(wid)\n",
    "                    if not toks: continue\n",
    "                    if word_rep_mode == \"first\":\n",
    "                        vec = h[:, b, toks[0], :]\n",
    "                    elif word_rep_mode == \"last\":\n",
    "                        vec = h[:, b, toks[-1], :]\n",
    "                    else:\n",
    "                        vec = h[:, b, toks, :].mean(axis=1)\n",
    "                    reps[:, gidx, :] = vec.astype(np.float16, copy=False)\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    miss = int((~filled).sum())\n",
    "    if miss:\n",
    "        print(f\"⚠ Missing vectors for {miss} of {N} sampled words in this shard (dropping them).\")\n",
    "        reps = reps[:, filled]\n",
    "    del model; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    return reps  # (L, N_filled, D)\n",
    "\n",
    "# ------------------------------ spectral helpers & metrics ------------------------------\n",
    "EPS = 1e-12\n",
    "\n",
    "def _eigvals_from_X(X: np.ndarray) -> np.ndarray:\n",
    "    Xc = _center(X.astype(np.float32, copy=False))\n",
    "    try:\n",
    "        _, S, _ = np.linalg.svd(Xc, full_matrices=False)\n",
    "        lam = (S**2).astype(np.float64)\n",
    "        lam.sort(); return lam[::-1]\n",
    "    except Exception:\n",
    "        return np.array([], dtype=np.float64)\n",
    "\n",
    "def iso_per_layer(rep: np.ndarray) -> np.ndarray:\n",
    "    L = rep.shape[0]; out = np.full(L, np.nan, np.float32)\n",
    "    for l in range(L):\n",
    "        try: out[l] = float(IsoScore.IsoScore(rep[l].astype(np.float32)))\n",
    "        except: pass\n",
    "    return out\n",
    "\n",
    "def spectral_flatness_per_layer(rep: np.ndarray) -> np.ndarray:\n",
    "    L = rep.shape[0]; out = np.full(L, np.nan, np.float32)\n",
    "    for l in range(L):\n",
    "        lam = _eigvals_from_X(rep[l]); \n",
    "        if lam.size: \n",
    "            gm = np.exp(np.mean(np.log(lam + EPS))); am = float(lam.mean() + EPS)\n",
    "            out[l] = float(gm / am)\n",
    "    return out\n",
    "\n",
    "def vmf_kappa_per_layer(rep: np.ndarray) -> np.ndarray:\n",
    "    L = rep.shape[0]; out = np.full(L, np.nan, np.float32)\n",
    "    for l in range(L):\n",
    "        X = rep[l].astype(np.float32, copy=False)\n",
    "        if X.shape[0] < 2: continue\n",
    "        Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
    "        R = np.linalg.norm(Xn.mean(axis=0)); d = Xn.shape[1]\n",
    "        kappa = 0.0 if R < 1e-9 else R * (d - R**2) / (1.0 - R**2 + 1e-9)\n",
    "        out[l] = float(max(kappa, 0.0))\n",
    "    return out\n",
    "\n",
    "def erank_per_layer(rep: np.ndarray) -> np.ndarray:\n",
    "    L = rep.shape[0]; out = np.full(L, np.nan, np.float32)\n",
    "    for l in range(L):\n",
    "        lam = _eigvals_from_X(rep[l])\n",
    "        if lam.size:\n",
    "            p = lam / (lam.sum() + EPS); H = -(p * np.log(p + EPS)).sum()\n",
    "            out[l] = float(np.exp(H))\n",
    "    return out\n",
    "\n",
    "def pr_per_layer(rep: np.ndarray) -> np.ndarray:\n",
    "    L = rep.shape[0]; out = np.full(L, np.nan, np.float32)\n",
    "    for l in range(L):\n",
    "        lam = _eigvals_from_X(rep[l])\n",
    "        if lam.size:\n",
    "            s1 = lam.sum(); s2 = (lam**2).sum()\n",
    "            out[l] = float((s1**2) / (s2 + EPS))\n",
    "    return out\n",
    "\n",
    "def stable_rank_per_layer(rep: np.ndarray) -> np.ndarray:\n",
    "    L = rep.shape[0]; out = np.full(L, np.nan, np.float32)\n",
    "    for l in range(L):\n",
    "        lam = _eigvals_from_X(rep[l])\n",
    "        if lam.size:\n",
    "            out[l] = float(lam.sum() / (lam.max() + EPS))\n",
    "    return out\n",
    "\n",
    "def _cap_layer(X: np.ndarray, cap: int | None = HEAVY_MAX_PER_LAYER) -> np.ndarray:\n",
    "    n = X.shape[0]\n",
    "    if cap is None or n <= cap:\n",
    "        return X.astype(np.float32, copy=False)\n",
    "    idx = np.random.default_rng(RAND_SEED).choice(n, int(cap), replace=False)\n",
    "    return X[idx].astype(np.float32, copy=False)\n",
    "\n",
    "def twonn_per_layer(rep: np.ndarray) -> np.ndarray:\n",
    "    if not HAS_DADAPY: return np.full(rep.shape[0], np.nan, np.float32)\n",
    "    L = rep.shape[0]; out = np.full(L, np.nan, np.float32)\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    for l in range(L):\n",
    "        X = _prep_for_knn(rep[l], cap=HEAVY_MAX_PER_LAYER, rng=rng)\n",
    "        if X.shape[0] < 3: continue\n",
    "        try:\n",
    "            d = Data(coordinates=X)\n",
    "            id_est, _, _ = d.compute_id_2NN()\n",
    "            out[l] = float(id_est)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out\n",
    "\n",
    "def gride_per_layer(rep: np.ndarray, range_max: int = DADAPY_GRID_RANGE) -> np.ndarray:\n",
    "    if not HAS_DADAPY: return np.full(rep.shape[0], np.nan, np.float32)\n",
    "    L = rep.shape[0]; out = np.full(L, np.nan, np.float32)\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    for l in range(L):\n",
    "        X = _prep_for_knn(rep[l], cap=HEAVY_MAX_PER_LAYER, rng=rng)\n",
    "        if X.shape[0] < 20: continue\n",
    "        try:\n",
    "            d = Data(coordinates=X); d.compute_distances(maxk=range_max)\n",
    "            ids, _, _ = d.return_id_scaling_gride(range_max=range_max)\n",
    "            out[l] = float(ids[-1])\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out\n",
    "\n",
    "def skdim_layer(rep: np.ndarray, est_name: str) -> np.ndarray:\n",
    "    if not HAS_SKDIM: return np.full(rep.shape[0], np.nan, np.float32)\n",
    "    makers = {\n",
    "        \"mom\": lambda: MOM(), \"tle\": lambda: TLE(), \"corrint\": lambda: CorrInt(),\n",
    "        \"fishers\": lambda: FisherS(), \"lpca\": lambda: lPCA(ver=\"FO\"),\n",
    "        \"lpca99\": lambda: lPCA(ver=\"ratio\", alphaRatio=0.99),\n",
    "        \"lpca95\": lambda: lPCA(ver=\"ratio\", alphaRatio=0.95),\n",
    "        \"mle\": lambda: MLE(), \"ess\": lambda: ESS(), \"mada\": lambda: MADA(),\n",
    "    }\n",
    "    make = makers.get(est_name)\n",
    "    if make is None: return np.full(rep.shape[0], np.nan, np.float32)\n",
    "\n",
    "    L = rep.shape[0]; out = np.full(L, np.nan, np.float32)\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    for l in range(L):\n",
    "        X = _prep_for_knn(rep[l], cap=HEAVY_MAX_PER_LAYER, rng=rng)\n",
    "        if X.shape[0] < 3: continue\n",
    "        try:\n",
    "            est = make(); est.fit(X)\n",
    "            out[l] = float(getattr(est, \"dimension_\", np.nan))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out\n",
    "\n",
    "\n",
    "# registry\n",
    "METRIC_FUNS: Dict[str, Callable[[np.ndarray], np.ndarray]] = {\n",
    "    \"iso\": iso_per_layer,\n",
    "    \"sf\": spectral_flatness_per_layer,\n",
    "    \"vmf_kappa\": vmf_kappa_per_layer,   # (anisotropy↑)\n",
    "    \"erank\": erank_per_layer, \"pr\": pr_per_layer, \"stable_rank\": stable_rank_per_layer,\n",
    "    \"twonn\": twonn_per_layer, \"gride\": gride_per_layer,\n",
    "    \"mom\": lambda R: skdim_layer(R,\"mom\"),\n",
    "    \"tle\": lambda R: skdim_layer(R,\"tle\"),\n",
    "    \"corrint\": lambda R: skdim_layer(R,\"corrint\"),\n",
    "    \"fishers\": lambda R: skdim_layer(R,\"fishers\"),\n",
    "    \"mle\": lambda R: skdim_layer(R,\"mle\"),\n",
    "    \"ess\": lambda R: skdim_layer(R,\"ess\"),\n",
    "    \"mada\": lambda R: skdim_layer(R,\"mada\"),\n",
    "    \"lpca\": lambda R: skdim_layer(R,\"lpca\"),\n",
    "    \"lpca99\": lambda R: skdim_layer(R,\"lpca99\"),\n",
    "    \"lpca95\": lambda R: skdim_layer(R,\"lpca95\"),\n",
    "}\n",
    "\n",
    "LABELS = {\n",
    "    \"iso\":\"IsoScore\",\"sf\":\"Spectral Flatness\",\"vmf_kappa\":\"vMF κ\",\n",
    "    \"erank\":\"Effective Rank\",\"pr\":\"Participation Ratio\",\"stable_rank\":\"Stable Rank\",\n",
    "    \"twonn\":\"TwoNN ID\",\"gride\":\"GRIDE ID\",\"mom\":\"MOM\",\"tle\":\"TLE\",\n",
    "    \"corrint\":\"CorrInt\",\"fishers\":\"FisherS\",\"mle\":\"MLE\",\"ess\":\"ESS\",\"mada\":\"MADA\",\n",
    "    \"lpca\":\"lPCA (FO)\",\"lpca99\":\"lPCA 0.99\",\"lpca95\":\"lPCA 0.95\",\n",
    "}\n",
    "\n",
    "FAMILIES = {\n",
    "    \"isotropy\":   [\"iso\",\"sf\",\"vmf_kappa\"],\n",
    "    \"linear_id\":  [\"erank\",\"pr\",\"stable_rank\",\"lpca\",\"lpca95\",\"lpca99\"],\n",
    "    \"nonlinear\":  [\"twonn\",\"gride\",\"mom\",\"tle\",\"corrint\",\"fishers\",\"mle\",\"ess\",\"mada\"],\n",
    "}\n",
    "\n",
    "# ------------------------------ correlation & plotting ------------------------------\n",
    "def compute_metrics_for_shard(rep: np.ndarray, shard_idx: int) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for m, fn in METRIC_FUNS.items():\n",
    "        vals = fn(rep)   # (L,)\n",
    "        for l, v in enumerate(vals.tolist()):\n",
    "            rows.append({\"layer\": l, \"shard\": shard_idx, \"metric\": m,\n",
    "                         \"value\": (float(v) if (v is not None and np.isfinite(v)) else np.nan)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def build_spearman_corr_and_pvals(df_long: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, int]:\n",
    "    dfp = df_long.pivot_table(index=[\"layer\",\"shard\"], columns=\"metric\",\n",
    "                              values=\"value\", aggfunc=\"mean\")\n",
    "    dfp = dfp.dropna(axis=1, how=\"all\").dropna(axis=0, how=\"any\")\n",
    "    n_points = dfp.shape[0]\n",
    "    if dfp.shape[1] < 2 or n_points < 2:\n",
    "        cols = dfp.columns\n",
    "        return (pd.DataFrame(np.nan, index=cols, columns=cols),\n",
    "                pd.DataFrame(np.nan, index=cols, columns=cols), n_points)\n",
    "    res = spearmanr(dfp.values, axis=0, nan_policy='omit')\n",
    "    rho = pd.DataFrame(res.statistic, index=dfp.columns, columns=dfp.columns)\n",
    "    p   = pd.DataFrame(res.pvalue,   index=dfp.columns, columns=dfp.columns)\n",
    "    return rho, p, n_points\n",
    "\n",
    "def fdr_bh_matrix(pval_df: pd.DataFrame, alpha: float = 0.05) -> tuple[pd.DataFrame, pd.DataFrame | None]:\n",
    "    if not HAS_STATSMODELS:\n",
    "        return pval_df.copy(), None\n",
    "    p = pval_df.to_numpy(copy=True); k = p.shape[0]\n",
    "    mask = np.ones_like(p, dtype=bool); np.fill_diagonal(mask, False)\n",
    "    r, c = np.where(mask); p_vec = p[r, c]\n",
    "    reject, q_vec, *_ = multipletests(p_vec, alpha=alpha, method=\"fdr_bh\")\n",
    "    q = np.full_like(p, np.nan, dtype=float); q[r, c] = q_vec\n",
    "    rej = np.zeros_like(p, dtype=bool); rej[r, c] = reject\n",
    "    return (pd.DataFrame(q, index=pval_df.index, columns=pval_df.columns),\n",
    "            pd.DataFrame(rej, index=pval_df.index, columns=pval_df.columns))\n",
    "\n",
    "def _grouped_order(cols: list[str]) -> tuple[list[str], list[int]]:\n",
    "    present = [c for c in cols if isinstance(c, str)]\n",
    "    order, bounds = [], []\n",
    "    for fam in (\"isotropy\",\"linear_id\",\"nonlinear\"):\n",
    "        wanted = FAMILIES[fam]\n",
    "        have   = [m for m in wanted if m in present]\n",
    "        if have:\n",
    "            order.extend(have); bounds.append(len(order))\n",
    "    # leftovers\n",
    "    rest = [c for c in present if c not in order]\n",
    "    if rest: order.extend(rest); bounds.append(len(order))\n",
    "    return order, bounds\n",
    "\n",
    "def _reorder_grouped(rho_df: pd.DataFrame, pval_df: pd.DataFrame | None):\n",
    "    order, bounds = _grouped_order(list(rho_df.columns))\n",
    "    rho_o = rho_df.loc[order, order]\n",
    "    p_o   = (pval_df.loc[order, order] if pval_df is not None else None)\n",
    "    pretty = {m: LABELS.get(m, m) for m in order}\n",
    "    return rho_o, p_o, pretty, bounds\n",
    "\n",
    "def _draw_family_boxes(ax, bounds: list[int], lw: float=2.0):\n",
    "    # bounds are cumulative lengths; draw rectangles around each block\n",
    "    start = 0\n",
    "    for b in bounds:\n",
    "        size = b - start\n",
    "        if size <= 0: \n",
    "            start = b; continue\n",
    "        rect = Rectangle((start, start), width=size, height=size,\n",
    "                         fill=False, lw=lw)\n",
    "        ax.add_patch(rect)\n",
    "        start = b\n",
    "\n",
    "def plot_heatmap_grouped(corr: pd.DataFrame, title: str, out: Path,\n",
    "                         pvals: pd.DataFrame | None = None, alpha: float = 0.05,\n",
    "                         boxes: list[int] | None = None, normalize01: bool = NORMALIZE_HEATMAP_01):\n",
    "    data = corr.copy()\n",
    "    if normalize01:\n",
    "        data = (data + 1.0) / 2.0  # map [-1,1] -> [0,1]\n",
    "        vmin, vmax = 0.0, 1.0\n",
    "    else:\n",
    "        vmin, vmax = -1.0, 1.0\n",
    "    plt.figure(figsize=(1.0+0.55*len(data.columns), 1.0+0.55*len(data.columns)))\n",
    "    ax = sns.heatmap(data, vmin=vmin, vmax=vmax, annot=True, fmt=\".2f\",\n",
    "                     square=True, cbar=True, annot_kws={\"size\":8})\n",
    "    ax.set_title(title)\n",
    "    # significance stars\n",
    "    if pvals is not None and not pvals.empty:\n",
    "        k = len(data)\n",
    "        for i in range(k):\n",
    "            for j in range(k):\n",
    "                if i == j: continue\n",
    "                p = pvals.iat[i, j]\n",
    "                if np.isfinite(p) and p < alpha:\n",
    "                    stars = \"***\" if p < 1e-3 else \"**\" if p < 1e-2 else \"*\"\n",
    "                    ax.text(j + 0.5, i + 0.5, stars, ha=\"center\", va=\"bottom\",\n",
    "                            fontsize=9, fontweight=\"bold\")\n",
    "    # family boxes\n",
    "    if boxes:\n",
    "        _draw_family_boxes(ax, boxes, lw=2.0)\n",
    "    plt.tight_layout(); plt.savefig(out, dpi=220); plt.close()\n",
    "\n",
    "# ------------------------------ main driver (streaming by shards) ------------------------------\n",
    "def run():\n",
    "    tag = _model_tag(BASELINE)\n",
    "    print(\"• Loading corpus …\")\n",
    "    df_sent, all_words = load_all_words(CSV_PATH, EXCLUDE_POS)\n",
    "    print(f\"  tokens available after POS filter: {len(all_words):,}\")\n",
    "\n",
    "    # Build shards with exactly MIN_PER_SHARD words each\n",
    "    shards = pick_words_for_shards(all_words, SHARDS_K, MIN_PER_SHARD, cap=N_TOTAL_CAP)\n",
    "    print(f\"• Using {SHARDS_K} shards × {MIN_PER_SHARD} words = {SHARDS_K*MIN_PER_SHARD:,} tokens\")\n",
    "\n",
    "    df_long_parts = []\n",
    "    for s, words_s in enumerate(shards):\n",
    "        print(f\"\\n=== Shard {s+1}/{SHARDS_K} (N={len(words_s):,}) ===\")\n",
    "        reps = embed_words(df_sent, words_s, BASELINE, WORD_REP_MODE, BATCH_SIZE)  # (L, N, D) for this shard\n",
    "        L, N, D = reps.shape\n",
    "        if N < HEAVY_MIN_PER_L:\n",
    "            raise RuntimeError(f\"Shard {s}: need ≥{HEAVY_MIN_PER_L} tokens for heavy metrics; got {N}.\")\n",
    "        df_s = compute_metrics_for_shard(reps, s)\n",
    "        df_long_parts.append(df_s)\n",
    "        # free shard memory\n",
    "        del reps, df_s; gc.collect()\n",
    "        if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    # concat all shards\n",
    "    df_long = pd.concat(df_long_parts, ignore_index=True)\n",
    "    df_long[\"model\"] = BASELINE\n",
    "    df_long[\"word_rep_mode\"] = WORD_REP_MODE\n",
    "    df_long.to_csv(CSV_DIR / f\"metrics_all_long_{tag}.csv\", index=False)\n",
    "    print(f\"\\n✓ metrics saved → {CSV_DIR / f'metrics_all_long_{tag}.csv'}\")\n",
    "\n",
    "    # Correlation (global, across all metrics)\n",
    "    rho_df, pval_df, n_pts = build_spearman_corr_and_pvals(df_long)\n",
    "    rho_df.to_csv(CSV_DIR / f\"corr_spearman_{tag}.csv\")\n",
    "    pval_df.to_csv(CSV_DIR / f\"corr_spearman_pvals_{tag}.csv\")\n",
    "    print(f\"Correlation points (layers × shards with complete rows): n = {n_pts}\")\n",
    "\n",
    "    # Optional FDR\n",
    "    q_df, rej_mat = fdr_bh_matrix(pval_df, alpha=0.05)\n",
    "    if HAS_STATSMODELS:\n",
    "        q_df.to_csv(CSV_DIR / f\"corr_spearman_qvals_fdrbh_{tag}.csv\")\n",
    "\n",
    "    # Grouped + pretty labels + boxes around families\n",
    "    rho_g, p_for_plot, pretty, bounds = _reorder_grouped(rho_df, (q_df if HAS_STATSMODELS else pval_df))\n",
    "    rho_g_pretty = rho_g.rename(index=pretty, columns=pretty)\n",
    "    p_plot_pretty = (p_for_plot.rename(index=pretty, columns=pretty) if p_for_plot is not None else None)\n",
    "\n",
    "    # Plain heatmap\n",
    "    plot_heatmap_grouped(\n",
    "        rho_g_pretty,\n",
    "        f\"Spearman correlation • {tag}\",\n",
    "        PLOT_DIR / f\"corr_spearman_{tag}.png\",\n",
    "        pvals=None, boxes=bounds, normalize01=NORMALIZE_HEATMAP_01\n",
    "    )\n",
    "    # With significance\n",
    "    plot_heatmap_grouped(\n",
    "        rho_g_pretty,\n",
    "        f\"Spearman correlation • {tag} (significance)\",\n",
    "        PLOT_DIR / f\"corr_spearman_with_sig_{tag}.png\",\n",
    "        pvals=p_plot_pretty, boxes=bounds, normalize01=NORMALIZE_HEATMAP_01\n",
    "    )\n",
    "    print(\"✓ plots stored in\", PLOT_DIR.resolve())\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n",
    "MIN_POINTS = 47\n",
    "# ...\n",
    "L, N, D = reps.shape\n",
    "shards_needed = max(1, math.ceil(MIN_POINTS / L))\n",
    "df_long = compute_metrics_over_shards(reps, shards_needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dbd76e-89a8-4594-b6bc-a6a7d895c122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
