{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8581345c",
   "metadata": {},
   "source": [
    "# Metric correlation (Spearman) across subsamples\n",
    "\n",
    "This notebook computes a Spearman correlation matrix between *geometry metrics* computed on many random subsamples of the **same embedding pool** (single layer, fixed model).\n",
    "\n",
    "The goal is to visualize how strongly different metrics co-vary when you perturb the sample of points but keep the underlying representation space fixed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5707e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import random\n",
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ---------------------- deps (real metrics) ----------------------\n",
    "# IsoScore\n",
    "try:\n",
    "    from IsoScore.IsoScore import IsoScore  # function: IsoScore(points)\n",
    "except Exception as e:\n",
    "    raise ImportError(\n",
    "        \"Missing dependency: IsoScore. Install with: pip install IsoScore\\n\"\n",
    "        f\"Original error: {e}\"\n",
    "    )\n",
    "\n",
    "# DADApy\n",
    "try:\n",
    "    from dadapy.data import Data\n",
    "except Exception as e:\n",
    "    raise ImportError(\n",
    "        \"Missing dependency: dadapy. Install with: pip install dadapy\\n\"\n",
    "        f\"Original error: {e}\"\n",
    "    )\n",
    "\n",
    "# scikit-dimension\n",
    "try:\n",
    "    from skdim.id import MOM, TLE, CorrInt, FisherS, lPCA, MLE, MADA, ESS\n",
    "except Exception as e:\n",
    "    raise ImportError(\n",
    "        \"Missing dependency: scikit-dimension (skdim). Install with: pip install scikit-dimension\\n\"\n",
    "        f\"Original error: {e}\"\n",
    "    )\n",
    "\n",
    "# Repro\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "RAND_SEED = 42\n",
    "random.seed(RAND_SEED)\n",
    "np.random.seed(RAND_SEED)\n",
    "torch.manual_seed(RAND_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde120a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- config ----------------------\n",
    "CSV_PATH = \"en_ewt-ud-train_sentences.csv\"   # expects columns: sentence_id, tokens, pos\n",
    "MODEL_NAME = \"openai-community/gpt2\"\n",
    "LAYER_IDX = 12                 # 0 = embeddings, last layer depends on model depth\n",
    "WORD_REP_MODE = \"last\"         # {\"first\",\"last\",\"mean\"}\n",
    "EXCLUDE_POS = {\"X\", \"SYM\", \"INTJ\", \"PART\"}  # optional\n",
    "\n",
    "# pool and subsampling\n",
    "POOL_SIZE = 20_000             # embed once\n",
    "SUBSAMPLE_M = 2_000            # each \"point\" is a subsample of X_pool\n",
    "N_POINTS = 200                 # number of subsamples (correlation uses N_POINTS rows)\n",
    "\n",
    "# heavy metrics (kNN-based)\n",
    "HEAVY_MAX_N = 2_000            # cap for kNN estimators (keep = SUBSAMPLE_M for paper settings)\n",
    "DADAPY_GRID_RANGE_MAX = 64     # GRIDE range_max\n",
    "\n",
    "# embedding\n",
    "BATCH_SIZE_SENT = 16\n",
    "MAX_LENGTH = 256\n",
    "\n",
    "# plotting\n",
    "SAVE_FIG = False\n",
    "FIG_PATH = \"corr_heatmap.pdf\"\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac7d81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_list(x):\n",
    "    if isinstance(x, str) and x.startswith(\"[\"):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except Exception:\n",
    "            return []\n",
    "    return x\n",
    "\n",
    "def load_sentences_and_words(csv_path: str, exclude_pos: Optional[set[str]] = None):\n",
    "    df = pd.read_csv(csv_path, usecols=[\"sentence_id\", \"tokens\", \"pos\"])\n",
    "    df[\"tokens\"] = df[\"tokens\"].apply(_as_list)\n",
    "    df[\"pos\"] = df[\"pos\"].apply(_as_list)\n",
    "    df = df.set_index(\"sentence_id\", drop=False)\n",
    "\n",
    "    rows = []\n",
    "    for sid, row in df.iterrows():\n",
    "        toks = row[\"tokens\"]\n",
    "        poss = row[\"pos\"]\n",
    "        for wid, (_, pos) in enumerate(zip(toks, poss)):\n",
    "            if exclude_pos and pos in exclude_pos:\n",
    "                continue\n",
    "            rows.append((sid, wid))\n",
    "\n",
    "    words_df = pd.DataFrame(rows, columns=[\"sentence_id\", \"word_id\"])\n",
    "    return df, words_df\n",
    "\n",
    "def sample_pool_words(words_df: pd.DataFrame, n: int, seed: int = 0) -> pd.DataFrame:\n",
    "    if n >= len(words_df):\n",
    "        out = words_df.copy()\n",
    "    else:\n",
    "        out = words_df.sample(n=n, random_state=seed).copy()\n",
    "\n",
    "    out = out.reset_index(drop=True)\n",
    "    out[\"global_idx\"] = np.arange(len(out), dtype=np.int64)\n",
    "    return out\n",
    "\n",
    "df_sent, all_words = load_sentences_and_words(CSV_PATH, exclude_pos=EXCLUDE_POS)\n",
    "pool_words = sample_pool_words(all_words, POOL_SIZE, seed=RAND_SEED)\n",
    "\n",
    "print(\"sentences:\", len(df_sent))\n",
    "print(\"candidate words:\", len(all_words))\n",
    "print(\"pool words:\", len(pool_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e5219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenizer_and_model(model_name: str):\n",
    "    # GPT-2 often needs add_prefix_space=True for word alignment with is_split_into_words\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True, add_prefix_space=True)\n",
    "    except TypeError:\n",
    "        tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    model = AutoModel.from_pretrained(model_name, output_hidden_states=True).eval().to(device)\n",
    "\n",
    "    # optional speed on GPU\n",
    "    if device.type == \"cuda\":\n",
    "        model.half()\n",
    "\n",
    "    return tok, model\n",
    "\n",
    "@torch.inference_mode()\n",
    "def embed_words_one_layer(\n",
    "    df_sent: pd.DataFrame,\n",
    "    words_df: pd.DataFrame,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    layer_idx: int,\n",
    "    mode: str = \"last\",\n",
    "    batch_size: int = 16,\n",
    "    max_length: int = 256,\n",
    ") -> np.ndarray:\n",
    "    by_sent = words_df.groupby(\"sentence_id\")\n",
    "    sent_ids = list(by_sent.groups.keys())\n",
    "\n",
    "    hidden_size = int(model.config.hidden_size)\n",
    "    reps = np.zeros((len(words_df), hidden_size), dtype=np.float32)\n",
    "    filled = np.zeros((len(words_df),), dtype=bool)\n",
    "\n",
    "    for b0 in tqdm(range(0, len(sent_ids), batch_size), desc=\"embedding\"):\n",
    "        batch_sids = sent_ids[b0 : b0 + batch_size]\n",
    "        batch_tokens = df_sent.loc[batch_sids, \"tokens\"].tolist()\n",
    "\n",
    "        enc = tokenizer(\n",
    "            batch_tokens,\n",
    "            is_split_into_words=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "        out = model(**enc)\n",
    "        hs = out.hidden_states[layer_idx].float()  # (B,T,D)\n",
    "\n",
    "        for bi, sid in enumerate(batch_sids):\n",
    "            word_ids = enc.word_ids(batch_index=bi)\n",
    "\n",
    "            pos_map: Dict[int, List[int]] = {}\n",
    "            for ti, wid in enumerate(word_ids):\n",
    "                if wid is None:\n",
    "                    continue\n",
    "                pos_map.setdefault(int(wid), []).append(int(ti))\n",
    "\n",
    "            req = by_sent.get_group(sid)\n",
    "            for _, r in req.iterrows():\n",
    "                wid = int(r[\"word_id\"])\n",
    "                gidx = int(r[\"global_idx\"])\n",
    "                toks = pos_map.get(wid)\n",
    "                if not toks:\n",
    "                    continue\n",
    "\n",
    "                if mode == \"first\":\n",
    "                    vec = hs[bi, toks[0], :]\n",
    "                elif mode == \"mean\":\n",
    "                    vec = hs[bi, toks, :].mean(dim=0)\n",
    "                else:  # \"last\"\n",
    "                    vec = hs[bi, toks[-1], :]\n",
    "\n",
    "                reps[gidx] = vec.detach().cpu().numpy().astype(np.float32, copy=False)\n",
    "                filled[gidx] = True\n",
    "\n",
    "        del enc, out, hs\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    if not filled.all():\n",
    "        reps = reps[filled]\n",
    "        print(f\"⚠ Dropped {(~filled).sum()} words (alignment / truncation). pool now:\", reps.shape[0])\n",
    "\n",
    "    return reps\n",
    "\n",
    "tokenizer, model = build_tokenizer_and_model(MODEL_NAME)\n",
    "X_pool = embed_words_one_layer(\n",
    "    df_sent=df_sent,\n",
    "    words_df=pool_words,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    layer_idx=LAYER_IDX,\n",
    "    mode=WORD_REP_MODE,\n",
    "    batch_size=BATCH_SIZE_SENT,\n",
    "    max_length=MAX_LENGTH,\n",
    ")\n",
    "print(\"X_pool:\", X_pool.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e6101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-9\n",
    "\n",
    "def _center(X: np.ndarray) -> np.ndarray:\n",
    "    return X - X.mean(0, keepdims=True)\n",
    "\n",
    "def _eigvals_from_X(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Return eigenvalues of X^T X (descending), computed via SVD of centered X.\n",
    "    Uses GPU if available (torch), falls back to numpy.\n",
    "    \"\"\"\n",
    "    Xc = X.astype(np.float32, copy=False)\n",
    "    if device.type == \"cuda\":\n",
    "        Xt = torch.as_tensor(Xc, device=device)\n",
    "        Xt = Xt - Xt.mean(dim=0, keepdim=True)\n",
    "        S = torch.linalg.svdvals(Xt)            # singular values\n",
    "        lam = (S * S).detach().cpu().numpy()    # eigenvalues of Xt^T Xt (up to scalar)\n",
    "        lam = lam.astype(np.float64, copy=False)\n",
    "        lam.sort()\n",
    "        return lam[::-1]\n",
    "    else:\n",
    "        Xc = _center(Xc)\n",
    "        _, S, _ = np.linalg.svd(Xc, full_matrices=False)\n",
    "        lam = (S ** 2).astype(np.float64)\n",
    "        lam.sort()\n",
    "        return lam[::-1]\n",
    "\n",
    "def _jitter_unique(X: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"Add tiny noise if there are duplicate rows (helps NN-based estimators).\"\"\"\n",
    "    try:\n",
    "        if np.unique(X, axis=0).shape[0] < X.shape[0]:\n",
    "            X = X + np.random.normal(scale=eps, size=X.shape).astype(X.dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return X\n",
    "\n",
    "# ---------- spectral metrics from eigenvalues ----------\n",
    "def spectral_flatness_from_eigs(lam: np.ndarray) -> float:\n",
    "    gm = np.exp(np.mean(np.log(lam + EPS)))\n",
    "    am = float(lam.mean() + EPS)\n",
    "    return float(gm / am)\n",
    "\n",
    "def effective_rank_from_eigs(lam: np.ndarray) -> float:\n",
    "    p = lam / (lam.sum() + EPS)\n",
    "    H = -(p * np.log(p + EPS)).sum()\n",
    "    return float(np.exp(H))\n",
    "\n",
    "def participation_ratio_from_eigs(lam: np.ndarray) -> float:\n",
    "    s1 = lam.sum()\n",
    "    s2 = (lam ** 2).sum()\n",
    "    return float((s1 ** 2) / (s2 + EPS))\n",
    "\n",
    "def stable_rank_from_eigs(lam: np.ndarray) -> float:\n",
    "    return float(lam.sum() / (lam.max() + EPS))\n",
    "\n",
    "# ---------- other metrics ----------\n",
    "def iso_score(X: np.ndarray) -> float:\n",
    "    Xt = torch.from_numpy(X.astype(np.float32, copy=False))\n",
    "    return float(IsoScore(Xt))\n",
    "\n",
    "def vmf_kappa(X: np.ndarray) -> float:\n",
    "    if X.shape[0] < 2:\n",
    "        return float(\"nan\")\n",
    "    Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
    "    R = np.linalg.norm(Xn.mean(axis=0))\n",
    "    d = Xn.shape[1]\n",
    "    if R < 1e-9:\n",
    "        return 0.0\n",
    "    return float(max(R * (d - R**2) / (1.0 - R**2 + 1e-9), 0.0))\n",
    "\n",
    "def lpca_fo(X: np.ndarray) -> float:\n",
    "    est = lPCA(ver=\"FO\")\n",
    "    est.fit(X)\n",
    "    return float(est.dimension_)\n",
    "\n",
    "def lpca_099(X: np.ndarray) -> float:\n",
    "    est = lPCA(ver=\"ratio\", alphaRatio=0.99)\n",
    "    est.fit(X)\n",
    "    return float(est.dimension_)\n",
    "\n",
    "# ---------- nonlinear ID (real estimators) ----------\n",
    "def twonn_id(X: np.ndarray) -> float:\n",
    "    d = Data(_jitter_unique(X))\n",
    "    id_est, _, _ = d.compute_id_2NN()\n",
    "    return float(id_est)\n",
    "\n",
    "def gride_id(X: np.ndarray) -> float:\n",
    "    d = Data(_jitter_unique(X))\n",
    "    d.compute_distances(maxk=DADAPY_GRID_RANGE_MAX)\n",
    "    ids, _, _ = d.return_id_scaling_gride(range_max=DADAPY_GRID_RANGE_MAX)\n",
    "    return float(ids[-1])\n",
    "\n",
    "def _skdim_fit_dim(X: np.ndarray, est_ctor: Callable[[], object]) -> float:\n",
    "    est = est_ctor()\n",
    "    est.fit(_jitter_unique(X))\n",
    "    return float(getattr(est, \"dimension_\", np.nan))\n",
    "\n",
    "def mom_id(X: np.ndarray) -> float:      return _skdim_fit_dim(X, MOM)\n",
    "def tle_id(X: np.ndarray) -> float:      return _skdim_fit_dim(X, TLE)\n",
    "def corrint_id(X: np.ndarray) -> float:  return _skdim_fit_dim(X, CorrInt)\n",
    "def fishers_id(X: np.ndarray) -> float:  return _skdim_fit_dim(X, FisherS)\n",
    "def mle_id(X: np.ndarray) -> float:      return _skdim_fit_dim(X, MLE)\n",
    "def ess_id(X: np.ndarray) -> float:      return _skdim_fit_dim(X, ESS)\n",
    "def mada_id(X: np.ndarray) -> float:     return _skdim_fit_dim(X, MADA)\n",
    "\n",
    "HEAVY: Dict[str, Callable[[np.ndarray], float]] = {\n",
    "    \"twonn\": twonn_id,\n",
    "    \"gride\": gride_id,\n",
    "    \"mle\": mle_id,\n",
    "    \"mom\": mom_id,\n",
    "    \"tle\": tle_id,\n",
    "    \"corrint\": corrint_id,\n",
    "    \"fishers\": fishers_id,\n",
    "    \"ess\": ess_id,\n",
    "    \"mada\": mada_id,\n",
    "}\n",
    "\n",
    "DISPLAY = {\n",
    "    \"iso\": \"IsoScore\",\n",
    "    \"sf\": \"Spectral Flatness\",\n",
    "    \"vmf_kappa\": \"vMF κ\",\n",
    "\n",
    "    \"erank\": \"Effective Rank\",\n",
    "    \"pr\": \"Participation Ratio\",\n",
    "    \"stable_rank\": \"Stable Rank\",\n",
    "    \"lpca\": \"PCA–FO\",\n",
    "    \"lpca99\": \"PCA–α (0.99)\",\n",
    "\n",
    "    \"twonn\": \"TwoNN\",\n",
    "    \"gride\": \"GRIDE\",\n",
    "    \"mle\": \"MLE\",\n",
    "    \"mom\": \"MOM\",\n",
    "    \"tle\": \"TLE\",\n",
    "    \"corrint\": \"CorrInt\",\n",
    "    \"fishers\": \"FisherS\",\n",
    "    \"ess\": \"ESS\",\n",
    "    \"mada\": \"MADA\",\n",
    "}\n",
    "\n",
    "METRIC_ORDER = [\n",
    "    \"iso\", \"sf\", \"vmf_kappa\",\n",
    "    \"erank\", \"pr\", \"stable_rank\", \"lpca\", \"lpca99\",\n",
    "    \"twonn\", \"gride\", \"mle\", \"mom\", \"tle\", \"corrint\", \"fishers\", \"ess\", \"mada\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2a6c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_points(X_pool: np.ndarray, m: int, n_points: int, seed: int = 0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = X_pool.shape[0]\n",
    "    if m > n:\n",
    "        raise ValueError(f\"SUBSAMPLE_M={m} > pool size {n}\")\n",
    "    return [rng.choice(n, size=m, replace=False) for _ in range(n_points)]\n",
    "\n",
    "def compute_points_df(X_pool: np.ndarray, idxs: List[np.ndarray], heavy_cap: Optional[int] = None):\n",
    "    rows = []\n",
    "    for pid, idx in enumerate(tqdm(idxs, desc=\"metrics\")):\n",
    "        X = X_pool[idx].astype(np.float32, copy=False)\n",
    "        rec = {\"point_id\": pid, \"n\": int(X.shape[0])}\n",
    "\n",
    "        # eigvals once -> sf / erank / pr / stable_rank\n",
    "        lam = _eigvals_from_X(X)\n",
    "        rec[\"sf\"] = spectral_flatness_from_eigs(lam)\n",
    "        rec[\"erank\"] = effective_rank_from_eigs(lam)\n",
    "        rec[\"pr\"] = participation_ratio_from_eigs(lam)\n",
    "        rec[\"stable_rank\"] = stable_rank_from_eigs(lam)\n",
    "\n",
    "        # other fast metrics\n",
    "        rec[\"iso\"] = iso_score(X)\n",
    "        rec[\"vmf_kappa\"] = vmf_kappa(X)\n",
    "        rec[\"lpca\"] = lpca_fo(X)\n",
    "        rec[\"lpca99\"] = lpca_099(X)\n",
    "\n",
    "        # heavy metrics on an optional cap\n",
    "        Xh = X\n",
    "        if heavy_cap is not None and X.shape[0] > heavy_cap:\n",
    "            rng = np.random.default_rng(RAND_SEED + pid)\n",
    "            sub = rng.choice(X.shape[0], size=heavy_cap, replace=False)\n",
    "            Xh = X[sub].astype(np.float32, copy=False)\n",
    "\n",
    "        for name, fn in HEAVY.items():\n",
    "            try:\n",
    "                rec[name] = float(fn(Xh))\n",
    "            except Exception:\n",
    "                rec[name] = np.nan\n",
    "\n",
    "        rows.append(rec)\n",
    "\n",
    "    df = pd.DataFrame(rows).set_index(\"point_id\")\n",
    "    df = df[[m for m in ([\"n\"] + METRIC_ORDER) if m in df.columns]]\n",
    "    return df\n",
    "\n",
    "idxs = subsample_points(X_pool, m=SUBSAMPLE_M, n_points=N_POINTS, seed=RAND_SEED)\n",
    "points_df = compute_points_df(X_pool, idxs, heavy_cap=HEAVY_MAX_N)\n",
    "points_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5574fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _spearman_pair(x: pd.Series, y: pd.Series) -> Tuple[float, float]:\n",
    "    mask = x.notna() & y.notna()\n",
    "    if int(mask.sum()) < 3:\n",
    "        return np.nan, np.nan\n",
    "    res = spearmanr(x[mask].to_numpy(), y[mask].to_numpy())\n",
    "    rho = getattr(res, \"correlation\", res[0])\n",
    "    p = getattr(res, \"pvalue\", res[1])\n",
    "    return float(rho), float(p)\n",
    "\n",
    "def spearman_corr_and_pvals(df: pd.DataFrame, cols: List[str]):\n",
    "    corr = pd.DataFrame(np.eye(len(cols)), index=cols, columns=cols, dtype=float)\n",
    "    pval = pd.DataFrame(np.ones((len(cols), len(cols))), index=cols, columns=cols, dtype=float)\n",
    "\n",
    "    for i, a in enumerate(cols):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            b = cols[j]\n",
    "            r, p = _spearman_pair(df[a], df[b])\n",
    "            corr.loc[a, b] = corr.loc[b, a] = r\n",
    "            pval.loc[a, b] = pval.loc[b, a] = p\n",
    "    return corr, pval\n",
    "\n",
    "metric_cols = [m for m in METRIC_ORDER if m in points_df.columns and points_df[m].notna().any()]\n",
    "corr, pval = spearman_corr_and_pvals(points_df, metric_cols)\n",
    "\n",
    "def p_to_stars(p: float) -> str:\n",
    "    if not np.isfinite(p):\n",
    "        return \"\"\n",
    "    if p < 1e-3:\n",
    "        return \"***\"\n",
    "    if p < 1e-2:\n",
    "        return \"**\"\n",
    "    if p < 5e-2:\n",
    "        return \"*\"\n",
    "    return \"\"\n",
    "\n",
    "corr_star = pd.DataFrame(\"\", index=metric_cols, columns=metric_cols, dtype=object)\n",
    "for i in metric_cols:\n",
    "    for j in metric_cols:\n",
    "        if i == j:\n",
    "            corr_star.loc[i, j] = \"1.00\"\n",
    "        else:\n",
    "            r = corr.loc[i, j]\n",
    "            corr_star.loc[i, j] = \"\" if not np.isfinite(r) else f\"{r:.2f}{p_to_stars(pval.loc[i, j])}\"\n",
    "\n",
    "corr_plot = corr.rename(index=DISPLAY, columns=DISPLAY)\n",
    "corr_star_plot = corr_star.rename(index=DISPLAY, columns=DISPLAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d78fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "ax = sns.heatmap(\n",
    "    corr_plot,\n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=\"vlag\",\n",
    "    square=True,\n",
    "    annot=corr_star_plot,\n",
    "    fmt=\"\",\n",
    "    annot_kws={\"fontsize\": 8},\n",
    "    linewidths=0.5,\n",
    ")\n",
    "\n",
    "ax.tick_params(axis=\"x\", labelsize=11)\n",
    "ax.tick_params(axis=\"y\", labelsize=11)\n",
    "\n",
    "# block separators (iso | linear | nonlinear)\n",
    "cuts = [3, 3 + 5]  # 3 iso metrics, 5 linear metrics\n",
    "for cut in cuts:\n",
    "    ax.axhline(cut, color=\"black\", lw=2.0)\n",
    "    ax.axvline(cut, color=\"black\", lw=2.0)\n",
    "\n",
    "plt.title(\"Spearman correlation (ρ)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(FIG_PATH, dpi=220)\n",
    "    print(\"saved:\", FIG_PATH)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
