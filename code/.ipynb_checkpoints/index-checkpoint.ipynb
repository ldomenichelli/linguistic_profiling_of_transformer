{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fccdd2f-009b-403c-a55e-4cd573675633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 10:10:19.537434: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np, pandas as pd, torch\n",
    "import torch.utils.data as torchdata\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel, BertModel, AutoConfig\n",
    "\n",
    "from IsoScore import IsoScore\n",
    "from dadapy import Data\n",
    "from skdim.id import MLE, MOM, TLE, CorrInt, FisherS, lPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e2e118b-93f7-47a3-8789-50e9acd4423c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ corpus ready — 194,916 tokens across index classes ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
      "Sample sizes per index (raw cap):\n",
      "{'1': 10067, '2': 9647, '3': 9800, '4': 9932, '5': 9948, '6': 9972, '7': 9559, '8': 9142, '9': 8704, '10': 108145}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openai-community/gpt2 (embed subset): 100%|█| 10067/10067 [01:32<00:00, 108.76it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ embedded 194,916 tokens  • layers=13\n",
      "\n",
      "→ Computing metric: gride …\n",
      "  ✓ saved: CSV= tables_INDEX/index_bootstrap/index_raw_gride_gpt2.csv  plot= results_INDEX/index_raw_gride_gpt2.png\n",
      "\n",
      "✓ done (incremental outputs produced per metric).\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel, GPT2TokenizerFast\n",
    "\n",
    "# ===== Optional deps (gracefully skipped if not installed) =====\n",
    "HAS_DADAPY = False\n",
    "try:\n",
    "    from dadapy import Data  # DADApy ID estimators (TwoNN, GRIDE)\n",
    "    HAS_DADAPY = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "HAS_SKDIM = False\n",
    "try:\n",
    "    from skdim.id import MOM, TLE, CorrInt, FisherS, lPCA, MLE, DANCo, ESS, MiND_ML, MADA, KNN\n",
    "    HAS_SKDIM = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# IsoScore: use library if available, else a simple monotone fallback\n",
    "try:\n",
    "    from isoscore import IsoScore\n",
    "    _HAS_ISOSCORE = True\n",
    "except Exception:\n",
    "    _HAS_ISOSCORE = False\n",
    "    class _IsoScoreFallback:\n",
    "        @staticmethod\n",
    "        def IsoScore(X: np.ndarray) -> float:\n",
    "            C = np.cov(X.T, ddof=0)\n",
    "            ev = np.linalg.eigvalsh(C)\n",
    "            if ev.size == 0 or ev.mean() <= 0 or ev[-1] <= 0:\n",
    "                return 0.0\n",
    "            return float(np.clip(ev.mean() / (ev[-1] + 1e-9), 0.0, 1.0))\n",
    "    IsoScore = _IsoScoreFallback()\n",
    "\n",
    "# =============================== CONFIG ===============================\n",
    "CSV_PATH      = \"en_ewt-ud-train_sentences.csv\"\n",
    "INDEX_COL     = \"index\"                 # your existing column with token positions (list[int])\n",
    "\n",
    "BASELINE      = \"gpt2\"                  # set to \"gpt2\" for GPT‑2\n",
    "WORD_REP_MODE = \"last\"                  # GPT‑2: {\"last\",\"mean\"}\n",
    "\n",
    "# NEW: drop first word of each sentence (word_id == 0)\n",
    "EXCLUDE_FIRST_WORD = False\n",
    "\n",
    "RAW_MAX_PER_CLASS            = 184_870  # cap per class\n",
    "N_BOOTSTRAP_FAST             = 50\n",
    "N_BOOTSTRAP_HEAVY            = 200\n",
    "FAST_BS_MAX_SAMP_PER_CLASS   = 184_870\n",
    "HEAVY_BS_MAX_SAMP_PER_CLASS  = 5_000\n",
    "\n",
    "INDEX_MAX_CLASS      = 10                # keep classes within [1..10] (10 means 10+)\n",
    "INCLUDE_ZERO_CLASS   = True             # set True if your positions are 0-based and you want class \"0\"\n",
    "\n",
    "RAND_SEED=42\n",
    "PLOT_DIR = Path(\"results_INDEX\"); PLOT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CSV_DIR  = Path(\"tables_INDEX\") / \"index_bootstrap\"; CSV_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": torch.backends.cudnn.benchmark = True\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "EPS = 1e-12\n",
    "\n",
    "# =============================== HELPERS ===============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _center(X: np.ndarray) -> np.ndarray:\n",
    "    return X - X.mean(0, keepdims=True)\n",
    "\n",
    "def _eigvals_from_X(X: np.ndarray) -> np.ndarray:\n",
    "    Xc = _center(X.astype(np.float32, copy=False))\n",
    "    try:\n",
    "        _, S, _ = np.linalg.svd(Xc, full_matrices=False)\n",
    "        lam = (S**2).astype(np.float64)\n",
    "        lam.sort()\n",
    "        return lam[::-1]\n",
    "    except Exception:\n",
    "        return np.array([], dtype=np.float64)\n",
    "\n",
    "def _jitter_unique(X: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"Add tiny noise if there are duplicate rows (helps NN-based estimators).\"\"\"\n",
    "    try:\n",
    "        if np.unique(X, axis=0).shape[0] < X.shape[0]:\n",
    "            X = X + np.random.normal(scale=eps, size=X.shape).astype(X.dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return X\n",
    "\n",
    "def _num_hidden_layers(model) -> int:\n",
    "    n = getattr(model.config, \"num_hidden_layers\", None)\n",
    "    if n is None: n = getattr(model.config, \"n_layer\", None)\n",
    "    if n is None: raise ValueError(\"Cannot determine number of hidden layers\")\n",
    "    return int(n)\n",
    "\n",
    "def _hidden_size(model) -> int:\n",
    "    d = getattr(model.config, \"hidden_size\", None)\n",
    "    if d is None: d = getattr(model.config, \"n_embd\", None)\n",
    "    if d is None: raise ValueError(\"Cannot determine hidden size\")\n",
    "    return int(d)\n",
    "\n",
    "def _is_gpt_like(model) -> bool:\n",
    "    mt = str(getattr(model.config, \"model_type\", \"\")).lower()\n",
    "    name = str(getattr(getattr(model, \"name_or_path\", \"\"), \"lower\", lambda: \"\")())\n",
    "    return (\"gpt2\" in mt) or (\"gpt2\" in name)\n",
    "\n",
    "# ====================== Robust GPT‑2 / general loader ======================\n",
    "def _load_tokenizer_and_model(baseline: str):\n",
    "    \"\"\"\n",
    "    Robustly load tokenizer+model.\n",
    "    - For GPT‑2, use GPT2TokenizerFast and try both 'openai-community/gpt2' and 'gpt2'\n",
    "      to avoid the 'vocab_file NoneType' bug.\n",
    "    - For others, fall back to AutoTokenizer + AutoModel.\n",
    "    \"\"\"\n",
    "    b = baseline.lower()\n",
    "    if \"gpt2\" in b:\n",
    "        candidates = []\n",
    "        for mid in [baseline, \"openai-community/gpt2\", \"gpt2\"]:\n",
    "            if mid not in candidates:\n",
    "                candidates.append(mid)\n",
    "        last_err = None\n",
    "        for mid in candidates:\n",
    "            try:\n",
    "                tokzr = GPT2TokenizerFast.from_pretrained(mid, add_prefix_space=True)\n",
    "                model = AutoModel.from_pretrained(mid, output_hidden_states=True)\n",
    "                return tokzr, model, mid\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "        raise RuntimeError(f\"Failed to load GPT‑2 tokenizer/model. Tried {candidates}. Last error: {last_err}\")\n",
    "    else:\n",
    "        tokzr = AutoTokenizer.from_pretrained(baseline, use_fast=True, add_prefix_space=True)\n",
    "        model = AutoModel.from_pretrained(baseline, output_hidden_states=True)\n",
    "        return tokzr, model, baseline\n",
    "\n",
    "# ========= Metric single-call functions =========\n",
    "# --- Isotropy family ---\n",
    "def _iso_once(X: np.ndarray) -> float:\n",
    "    return float(IsoScore.IsoScore(X))\n",
    "\n",
    "def _sf_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    gm = np.exp(np.mean(np.log(lam + EPS)))\n",
    "    am = float(lam.mean() + EPS)\n",
    "    return float(gm / am)\n",
    "\n",
    "def _rand_once(X: np.ndarray, K: int = 2000) -> float:\n",
    "    n = X.shape[0]\n",
    "    if n < 2: return np.nan\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    K_eff = min(K, (n*(n-1))//2)\n",
    "    i = rng.integers(0, n, size=K_eff); j = rng.integers(0, n, size=K_eff)\n",
    "    same = (i == j)\n",
    "    if same.any(): j[same] = rng.integers(0, n, size=same.sum())\n",
    "    A, B = X[i], X[j]\n",
    "    num = np.sum(A*B, axis=1)\n",
    "    den = (np.linalg.norm(A, axis=1)*np.linalg.norm(B, axis=1) + 1e-9)\n",
    "    return float(np.mean(np.abs(num/den)))  # higher ⇒ more anisotropic\n",
    "\n",
    "def _vmf_kappa_once(X: np.ndarray) -> float:\n",
    "    if X.shape[0] < 2: return np.nan\n",
    "    Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
    "    R = np.linalg.norm(Xn.mean(axis=0))\n",
    "    d = Xn.shape[1]\n",
    "    if R < 1e-9: return 0.0\n",
    "    return float(max(R * (d - R**2) / (1.0 - R**2 + 1e-9), 0.0))  # higher ⇒ more anisotropic\n",
    "\n",
    "# --- Linear ID family ---\n",
    "def _pcaXX_once(X: np.ndarray, var_ratio: float) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    c = np.cumsum(lam); thr = c[-1] * var_ratio\n",
    "    return float(np.searchsorted(c, thr) + 1)\n",
    "\n",
    "def _pca95_once(X: np.ndarray) -> float: return _pcaXX_once(X, 0.95)\n",
    "def _pca99_once(X: np.ndarray) -> float: return _pcaXX_once(X, 0.99)\n",
    "\n",
    "def _erank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    p = lam / (lam.sum() + EPS)\n",
    "    H = -(p * np.log(p + EPS)).sum()\n",
    "    return float(np.exp(H))\n",
    "\n",
    "def _pr_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    s1 = lam.sum(); s2 = (lam**2).sum()\n",
    "    return float((s1**2) / (s2 + EPS))\n",
    "\n",
    "def _stable_rank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    return float(lam.sum() / (lam.max() + EPS))\n",
    "\n",
    "# --- Non-linear family ---\n",
    "def _dadapy_twonn_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    id_est, _, _ = d.compute_id_2NN()\n",
    "    return float(id_est)\n",
    "\n",
    "def _dadapy_gride_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    d.compute_distances(maxk=64)\n",
    "    ids, _, _ = d.return_id_scaling_gride(range_max=64)\n",
    "    return float(ids[-1])\n",
    "\n",
    "def _skdim_factory(name: str):\n",
    "    if not HAS_SKDIM: return None\n",
    "    mapping = {\n",
    "        \"mom\": MOM, \"tle\": TLE, \"corrint\": CorrInt, \"fishers\": FisherS,\n",
    "        \"lpca\": lPCA, \"lpca99\": lPCA, \"lpca95\": lPCA,\n",
    "        \"mle\": MLE, \"danco\": DANCo, \"mind_ml\": MiND_ML, \"ess\": ESS,\n",
    "        \"mada\": MADA, \"knn\": KNN,\n",
    "    }\n",
    "    cls = mapping.get(name)\n",
    "    if cls is None: return None\n",
    "    def _builder():\n",
    "        if name == \"lpca\":      return cls(ver=\"FO\")\n",
    "        if name == \"lpca99\":    return cls(ver=\"ratio\", alphaRatio=0.99)\n",
    "        if name == \"lpca95\":    return cls(ver=\"ratio\", alphaRatio=0.95)\n",
    "        return cls()\n",
    "    return _builder\n",
    "\n",
    "def _skdim_once_builder(name: str) -> Callable[[np.ndarray], float] | None:\n",
    "    build = _skdim_factory(name)\n",
    "    if build is None: return None\n",
    "    def _once(X: np.ndarray) -> float:\n",
    "        est = build(); est.fit(_jitter_unique(X)); return float(getattr(est, \"dimension_\", np.nan))\n",
    "    return _once\n",
    "\n",
    "# Registries\n",
    "FAST_ONCE: Dict[str, Callable[[np.ndarray], float]] = {\n",
    "    # Isotropy\n",
    "    \"iso\": _iso_once, \"sf\": _sf_once, \"rand\": _rand_once, \"vmf_kappa\": _vmf_kappa_once,\n",
    "    # Linear ID\n",
    "    \"erank\": _erank_once, \"pr\": _pr_once, \"stable_rank\": _stable_rank_once,\n",
    "    \"pca95\": _pca95_once, \"pca99\": _pca99_once,\n",
    "}\n",
    "HEAVY_ONCE: Dict[str, Callable[[np.ndarray], float] | None] = {\n",
    "    # Non-linear (plus skdim linear variants)\n",
    "    \"twonn\": _dadapy_twonn_once, \"gride\": _dadapy_gride_once,\n",
    "    \"mom\": _skdim_once_builder(\"mom\"), \"tle\": _skdim_once_builder(\"tle\"),\n",
    "    \"corrint\": _skdim_once_builder(\"corrint\"), \"fishers\": _skdim_once_builder(\"fishers\"),\n",
    "    \"lpca\": _skdim_once_builder(\"lpca\"), \"lpca95\": _skdim_once_builder(\"lpca95\"),\n",
    "    \"lpca99\": _skdim_once_builder(\"lpca99\"), \"mle\": _skdim_once_builder(\"mle\"),\n",
    "    \"danco\": _skdim_once_builder(\"danco\"), \"mind_ml\": _skdim_once_builder(\"mind_ml\"),\n",
    "    \"ess\": _skdim_once_builder(\"ess\"), \"mada\": _skdim_once_builder(\"mada\"),\n",
    "    \"knn\": _skdim_once_builder(\"knn\"),\n",
    "}\n",
    "LABELS = {\n",
    "    # Isotropy\n",
    "    \"iso\":\"IsoScore\",\"sf\":\"Spectral Flatness\",\"rand\":\"RandCos |μ|\",\"vmf_kappa\":\"vMF κ\",\n",
    "    # Linear ID\n",
    "    \"erank\":\"Effective Rank\",\"pr\":\"Participation Ratio\",\"stable_rank\":\"Stable Rank\",\n",
    "    \"pca95\":\"lPCA 0.95\",\"pca99\":\"lPCA 0.99\",\"lpca\":\"lPCA FO\",\"lpca95\":\"lPCA 0.95 (skdim)\",\"lpca99\":\"lPCA 0.99 (skdim)\",\n",
    "    # Non-linear\n",
    "    \"twonn\":\"TwoNN ID\",\"gride\":\"GRIDE\",\"mom\":\"MOM\",\"tle\":\"TLE\",\"corrint\":\"CorrInt\",\n",
    "    \"fishers\":\"FisherS\",\"mle\":\"MLE\",\"danco\":\"DANCo\",\"mind_ml\":\"MiND_ML\",\"ess\":\"ESS\",\"mada\":\"MADA\",\"knn\":\"KNN\",\n",
    "}\n",
    "#ALL_METRICS = list(FAST_ONCE.keys()) + [k for k,v in HEAVY_ONCE.items() if v is not None]\n",
    "ALL_METRICS = [\"gride\"]\n",
    "\n",
    "# =============================== DATA: use existing index column ===============================\n",
    "def _pick_index_col(df: pd.DataFrame) -> str:\n",
    "    for cand in [INDEX_COL, \"token_index\", \"positions\", \"position\", \"idx\", \"INDEX\", \"Index\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    raise ValueError(\n",
    "        f\"No index column found. Tried: {INDEX_COL}, token_index, positions, position, idx, INDEX.\"\n",
    "    )\n",
    "\n",
    "def load_index_from_column(csv_path: str,\n",
    "                           index_max: int = INDEX_MAX_CLASS,\n",
    "                           include_zero: bool = INCLUDE_ZERO_CLASS):\n",
    "    \"\"\"\n",
    "    Expects CSV with:\n",
    "      - sentence_id (str)\n",
    "      - tokens      (list[str]) — one row per sentence\n",
    "      - index       (list[int]) — 1-based or 0-based token positions per sentence\n",
    "    Produces token-level rows with 'index_class' in {1..index_max} (and optionally 0).\n",
    "      'index_max' class means 'index_max+'.\n",
    "\n",
    "    Also: if EXCLUDE_FIRST_WORD=True, drop tokens with word_id == 0 (first word).\n",
    "    \"\"\"\n",
    "    df_all = pd.read_csv(csv_path)\n",
    "    idx_col = _pick_index_col(df_all)\n",
    "    df = df_all[[\"sentence_id\",\"tokens\", idx_col]].copy()\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df.tokens  = df.tokens.apply(_to_list)\n",
    "    df[idx_col] = df[idx_col].apply(_to_list)\n",
    "\n",
    "    rows = []\n",
    "    for sid, toks, idxs in df[[\"sentence_id\",\"tokens\", idx_col]].itertuples(index=False):\n",
    "        L = min(len(toks), len(idxs))\n",
    "        for wid in range(L):\n",
    "            # --- NEW: optionally skip first word of each sentence (word_id == 0) ---\n",
    "            if EXCLUDE_FIRST_WORD and wid == 0:\n",
    "                continue\n",
    "            try:\n",
    "                k = int(idxs[wid])\n",
    "            except Exception:\n",
    "                continue\n",
    "            if k == 0 and not include_zero:\n",
    "                # skip zero if we only want 1..N\n",
    "                continue\n",
    "            if k < 0:\n",
    "                continue\n",
    "            if k > index_max:\n",
    "                k = index_max  # bucket as index_max+\n",
    "            rows.append((sid, wid, str(k), toks[wid]))\n",
    "    df_tok = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\",\"index_class\",\"word\"])\n",
    "\n",
    "    # Safety: enforce first-word exclusion even if logic changes later\n",
    "    if EXCLUDE_FIRST_WORD and not df_tok.empty:\n",
    "        df_tok = df_tok[df_tok.word_id != 0].reset_index(drop=True)\n",
    "\n",
    "    df_sent = df[[\"sentence_id\",\"tokens\"]].drop_duplicates(\"sentence_id\")\n",
    "    if df_tok.empty:\n",
    "        raise ValueError(\"No token rows constructed—check that your 'index' column contains integer lists.\")\n",
    "    return df_sent, df_tok\n",
    "\n",
    "def sample_raw(df_tok: pd.DataFrame, per_class_cap: int = RAW_MAX_PER_CLASS) -> pd.DataFrame:\n",
    "    picks = []\n",
    "    for c, sub in df_tok.groupby(\"index_class\", sort=False):\n",
    "        n = min(len(sub), per_class_cap)\n",
    "        picks.append(sub.sample(n, random_state=RAND_SEED, replace=False))\n",
    "    return pd.concat(picks, ignore_index=True)\n",
    "\n",
    "def make_index_palette(classes: List[str]) -> Dict[str, Tuple[float, float, float]]:\n",
    "    vals = sorted([int(c) for c in classes])\n",
    "    cmap = sns.color_palette(\"viridis\", len(vals))\n",
    "    return {str(v): cmap[i] for i, v in enumerate(vals)}\n",
    "\n",
    "# =============================== EMBEDDING (BERT & GPT‑2) ===============================\n",
    "def embed_subset(df_sent: pd.DataFrame,\n",
    "                 subset_df: pd.DataFrame,\n",
    "                 baseline: str = BASELINE,\n",
    "                 word_rep_mode: str = WORD_REP_MODE,\n",
    "                 batch_size: int = BATCH_SIZE) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df_sent[\"sentence_id\"]   = df_sent[\"sentence_id\"].astype(str)\n",
    "    subset_df[\"sentence_id\"] = subset_df[\"sentence_id\"].astype(str)\n",
    "\n",
    "    # sid -> list[(global_idx, word_id)]\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(subset_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(str(sid), []).append((gidx, int(wid)))\n",
    "\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_sent[df_sent.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    # Robust loader (GPT‑2 safe)\n",
    "    tokzr, model, model_id_used = _load_tokenizer_and_model(baseline)\n",
    "\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
    "    # add_prefix_space for GPT‑2-like models\n",
    "    if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "        enc_kwargs[\"add_prefix_space\"] = True\n",
    "    # ensure pad token\n",
    "    if tokzr.pad_token is None and getattr(tokzr, \"eos_token\", None) is not None:\n",
    "        tokzr.pad_token = tokzr.eos_token\n",
    "\n",
    "    if getattr(model.config, \"pad_token_id\", None) is None and tokzr.pad_token_id is not None:\n",
    "        model.config.pad_token_id = tokzr.pad_token_id\n",
    "    if device == \"cuda\":\n",
    "        model.half()\n",
    "    model = model.eval().to(device)\n",
    "\n",
    "    L = _num_hidden_layers(model) + 1   # include embedding layer\n",
    "    D = _hidden_size(model)\n",
    "    N = len(subset_df)\n",
    "\n",
    "    reps   = np.zeros((L, N, D), np.float16)\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    # Choose/validate rep mode depending on model family\n",
    "    gpt_like = _is_gpt_like(model)\n",
    "    if gpt_like:\n",
    "        rep_mode = word_rep_mode if word_rep_mode in {\"last\",\"mean\"} else \"last\"\n",
    "    else:\n",
    "        rep_mode = word_rep_mode if word_rep_mode in {\"first\",\"last\",\"mean\"} else \"first\"\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{model_id_used} (embed subset)\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L,B,T,D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                mp: Dict[int, List[int]] = {}\n",
    "                for tidx, wid in enumerate(enc_be.word_ids(b)):\n",
    "                    if wid is not None:\n",
    "                        mp.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = mp.get(wid)\n",
    "                    if not toks: \n",
    "                        continue\n",
    "                    if rep_mode == \"first\":\n",
    "                        vec = h[:, b, toks[0], :]\n",
    "                    elif rep_mode == \"last\":\n",
    "                        vec = h[:, b, toks[-1], :]\n",
    "                    else:  # \"mean\"\n",
    "                        vec = h[:, b, toks, :].mean(axis=1)\n",
    "                    reps[:, gidx, :] = vec.astype(np.float16, copy=False)\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    missing = int((~filled).sum())\n",
    "    if missing:\n",
    "        print(f\"⚠ Missing vectors for {missing} of {N} tokens\")\n",
    "    del model; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    return reps, filled\n",
    "\n",
    "# =============================== BOOTSTRAP CORE ===============================\n",
    "def _bs_layer_loop(rep_sub: np.ndarray, M: int, n_reps: int, compute_once: Callable[[np.ndarray], float]):\n",
    "    L, N, D = rep_sub.shape\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    A = np.full((n_reps, L), np.nan, np.float32)\n",
    "    for r in range(n_reps):\n",
    "        idx = rng.integers(0, N, size=M)\n",
    "        for l in range(L):\n",
    "            X = rep_sub[l, idx].astype(np.float32, copy=False)\n",
    "            try:\n",
    "                A[r, l] = float(compute_once(X))\n",
    "            except Exception:\n",
    "                A[r, l] = np.nan\n",
    "    mu = np.nanmean(A, axis=0).astype(np.float32)\n",
    "    lo = np.nanpercentile(A, 2.5, axis=0).astype(np.float32)\n",
    "    hi = np.nanpercentile(A, 97.5, axis=0).astype(np.float32)\n",
    "    return mu, lo, hi\n",
    "\n",
    "# =============================== SAVE / PLOT ===============================\n",
    "def save_metric_csv_all_classes(metric: str,\n",
    "                                class_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                                layers: np.ndarray,\n",
    "                                baseline: str,\n",
    "                                subset_name: str = \"raw\"):\n",
    "    rows = []\n",
    "    for c, stats in class_to_stats.items():\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        for l, val in enumerate(mu):\n",
    "            rows.append({\n",
    "                \"subset\": subset_name, \"model\": baseline, \"feature\": \"index\",\n",
    "                \"class\": c, \"metric\": metric, \"layer\": int(layers[l]),\n",
    "                \"mean\": float(val) if np.isfinite(val) else np.nan,\n",
    "                \"ci_low\": float(lo[l]) if isinstance(lo, np.ndarray) and np.isfinite(lo[l]) else np.nan,\n",
    "                \"ci_high\": float(hi[l]) if isinstance(hi, np.ndarray) and np.isfinite(hi[l]) else np.nan,\n",
    "                \"n_tokens\": int(stats.get(\"n\", 0)), \"word_rep_mode\": WORD_REP_MODE,\n",
    "                \"source_csv\": Path(CSV_PATH).name,\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    out = CSV_DIR / f\"index_{subset_name}_{metric}_{baseline}.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "\n",
    "def plot_metric_with_ci(class_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                        layers: np.ndarray, metric: str, title: str, out_path: Path,\n",
    "                        palette: Dict[str, Tuple[float, float, float]] | None = None):\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    for c in sorted(class_to_stats.keys(), key=lambda s: int(s)):\n",
    "        stats = class_to_stats[c]\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        if mu is None or np.all(np.isnan(mu)): continue\n",
    "        color = palette.get(c) if isinstance(palette, dict) else None\n",
    "        plt.plot(layers, mu, label=c, lw=1.8, color=color)\n",
    "        if isinstance(lo, np.ndarray) and isinstance(hi, np.ndarray) and not np.all(np.isnan(lo)):\n",
    "            plt.fill_between(layers, lo, hi, alpha=0.15, color=color)\n",
    "    plt.xlabel(\"Layer\"); plt.ylabel(LABELS.get(metric, metric.upper())); plt.title(title)\n",
    "    plt.legend(ncol=5, fontsize=\"small\", title=f\"Index ({INDEX_MAX_CLASS} = {INDEX_MAX_CLASS}+)\", frameon=False)\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=220); plt.close()\n",
    "\n",
    "# =============================== DRIVER ===============================\n",
    "def run_index_from_col_pipeline():\n",
    "    # 1) Load token lists + index classes from existing column\n",
    "    df_sent, idx_df = load_index_from_column(\n",
    "        CSV_PATH, index_max=INDEX_MAX_CLASS, include_zero=INCLUDE_ZERO_CLASS\n",
    "    )\n",
    "    classes = sorted(idx_df.index_class.unique(), key=lambda s: int(s))\n",
    "    palette = make_index_palette(classes)\n",
    "    print(f\"✓ corpus ready — {len(idx_df):,} tokens across index classes {classes}\")\n",
    "\n",
    "    # 2) Optional per-class cap\n",
    "    raw_df = sample_raw(idx_df, RAW_MAX_PER_CLASS)\n",
    "    print(\"Sample sizes per index (raw cap):\")\n",
    "    counts = raw_df.index_class.value_counts()\n",
    "    counts = counts.reindex(sorted(counts.index, key=lambda x: int(x)))\n",
    "    print(counts.to_dict())\n",
    "\n",
    "    # 3) Embed once\n",
    "    reps, filled = embed_subset(df_sent, raw_df, BASELINE, WORD_REP_MODE, BATCH_SIZE)\n",
    "    raw_df = raw_df.reset_index(drop=True).loc[filled].reset_index(drop=True)\n",
    "    cls_arr = raw_df.index_class.values\n",
    "    L = reps.shape[0]; layers = np.arange(L)\n",
    "    print(f\"✓ embedded {len(raw_df):,} tokens  • layers={L}\")\n",
    "\n",
    "    # 4) Metric loop\n",
    "    for metric in ALL_METRICS:\n",
    "        print(f\"\\n→ Computing metric: {metric} …\")\n",
    "        compute_once = FAST_ONCE.get(metric) or HEAVY_ONCE.get(metric)\n",
    "        if compute_once is None:\n",
    "            print(f\"  (skipping {metric}: estimator unavailable)\")\n",
    "            continue\n",
    "\n",
    "        n_bs = N_BOOTSTRAP_FAST if metric in FAST_ONCE else N_BOOTSTRAP_HEAVY\n",
    "        Mcap = FAST_BS_MAX_SAMP_PER_CLASS if metric in FAST_ONCE else HEAVY_BS_MAX_SAMP_PER_CLASS\n",
    "\n",
    "        class_results: Dict[str, Dict[str, np.ndarray]] = {}\n",
    "        for c in classes:\n",
    "            idx = np.where(cls_arr == c)[0]\n",
    "            if idx.size < 3:\n",
    "                continue\n",
    "            sub = reps[:, idx]  # (L, n_c, D)\n",
    "            Nc = sub.shape[1]\n",
    "            M = min(Mcap, Nc)\n",
    "            mu, lo, hi = _bs_layer_loop(sub, M, n_bs, compute_once)\n",
    "            class_results[c] = {\"mean\": mu, \"lo\": lo, \"hi\": hi, \"n\": int(Nc)}\n",
    "\n",
    "        save_metric_csv_all_classes(metric, class_results, layers, BASELINE, subset_name=\"raw\")\n",
    "        plot_metric_with_ci(class_results, layers, metric,\n",
    "                            title=f\"{LABELS.get(metric, metric.upper())} • {BASELINE}\",\n",
    "                            out_path=PLOT_DIR / f\"index_raw_{metric}_{BASELINE}.png\",\n",
    "                            palette=palette)\n",
    "        print(f\"  ✓ saved: CSV= {CSV_DIR}/index_raw_{metric}_{BASELINE}.csv  \"\n",
    "              f\"plot= {PLOT_DIR}/index_raw_{metric}_{BASELINE}.png\")\n",
    "\n",
    "        del class_results; gc.collect()\n",
    "        if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    del reps; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    print(\"\\n✓ done (incremental outputs produced per metric).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_index_from_col_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33d141ee-637e-4fde-9a78-0914f3555ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ plotting subset — 194,916 tokens across index classes ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openai-community/gpt2 (embed subset): 100%|█| 5034/5034 [01:06<00:00, 75.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA plotting on 194,916 tokens across 13 layers...\n",
      "\n",
      "Palette (class -> color):\n",
      "    1 -> rgb(48, 18, 59)\n",
      "    2 -> rgb(68, 96, 208)\n",
      "    3 -> rgb(54, 167, 248)\n",
      "    4 -> rgb(33, 226, 181)\n",
      "    5 -> rgb(112, 252, 97)\n",
      "    6 -> rgb(199, 236, 54)\n",
      "    7 -> rgb(247, 184, 54)\n",
      "    8 -> rgb(244, 105, 24)\n",
      "    9 -> rgb(199, 42, 4)\n",
      "   10 -> rgb(122, 4, 2)\n",
      "\n",
      "✓ Saved interactive HTML to: pca3d_index/gpt2_index_pca3d_layers.html\n"
     ]
    }
   ],
   "source": [
    "## from __future__ import annotations\n",
    "\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, GPT2TokenizerFast\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.colors as pc\n",
    "\n",
    "# =============================== CONFIG ===============================\n",
    "CSV_PATH      = \"en_ewt-ud-train_sentences.csv\"  # sentence_id, tokens (list[str]), index (list[int]) or similar\n",
    "BASELINE      = \"gpt2\"              # e.g. \"bert-base-uncased\" or \"gpt2\"\n",
    "WORD_REP_MODE = \"last\"                          # BERT: {\"first\",\"last\",\"mean\"}; GPT-2: {\"last\",\"mean\"}\n",
    "\n",
    "INDEX_COL          = \"index\"                     # token positions per sentence (list[int])\n",
    "INDEX_MAX_CLASS    = 10                          # bucket to [1..10], with 10 meaning 10+\n",
    "INCLUDE_ZERO_CLASS = True                        # set True if your positions are 0-based and you want a \"0\" class\n",
    "\n",
    "# Plot subsampling per class (to keep the browser snappy)\n",
    "PCA_MAX_PER_CLASS  = None                        # None → all; try 2000–5000 if needed\n",
    "\n",
    "# Output\n",
    "OUT_DIR   = Path(\"pca3d_index\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "HTML_OUT  = OUT_DIR / f\"{BASELINE.replace('/','_')}_index_pca3d_layers.html\"\n",
    "\n",
    "# Throughput / device\n",
    "BATCH_SIZE = 2\n",
    "RAND_SEED  = 42\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# --------- Plotly font sizes (BIGGER TEXT) ----------\n",
    "PLOT_FONT            = 20   # global default\n",
    "PLOT_TITLE_FONT      = 24\n",
    "PLOT_AXIS_TITLE_FONT = 18\n",
    "PLOT_AXIS_TICK_FONT  = 20\n",
    "PLOT_LEGEND_FONT     = 20\n",
    "PLOT_SLIDER_FONT     = 20\n",
    "PLOT_HOVER_FONT      = 30\n",
    "\n",
    "\n",
    "# =============================== HELPERS ===============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _num_hidden_layers(model) -> int:\n",
    "    n = getattr(model.config, \"num_hidden_layers\", None)\n",
    "    if n is None: n = getattr(model.config, \"n_layer\", None)  # GPT-2\n",
    "    if n is None: raise ValueError(\"Cannot determine num_hidden_layers\")\n",
    "    return int(n)\n",
    "\n",
    "def _hidden_size(model) -> int:\n",
    "    d = getattr(model.config, \"hidden_size\", None)\n",
    "    if d is None: d = getattr(model.config, \"n_embd\", None)  # GPT-2\n",
    "    if d is None: raise ValueError(\"Cannot determine hidden size\")\n",
    "    return int(d)\n",
    "\n",
    "# --------- Load index classes from CSV ----------\n",
    "def _pick_index_col(df: pd.DataFrame) -> str:\n",
    "    for cand in [INDEX_COL, \"token_index\", \"positions\", \"position\", \"idx\", \"INDEX\", \"Index\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    raise ValueError(f\"No index column found. Tried: {INDEX_COL}, token_index, positions, position, idx, INDEX, Index.\")\n",
    "\n",
    "def load_index_from_column(csv_path: str,\n",
    "                           index_max: int = INDEX_MAX_CLASS,\n",
    "                           include_zero: bool = INCLUDE_ZERO_CLASS):\n",
    "    \"\"\"\n",
    "    Build token-level rows with index_class in {1..index_max} (index_max means 'index_max+').\n",
    "    If include_zero=True, keep class '0' as well (for 0-based positions).\n",
    "    \"\"\"\n",
    "    df_all = pd.read_csv(csv_path)\n",
    "    idx_col = _pick_index_col(df_all)\n",
    "    df = df_all[[\"sentence_id\",\"tokens\", idx_col]].copy()\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df.tokens  = df.tokens.apply(_to_list)\n",
    "    df[idx_col] = df[idx_col].apply(_to_list)\n",
    "\n",
    "    rows = []\n",
    "    for sid, toks, idxs in df[[\"sentence_id\",\"tokens\", idx_col]].itertuples(index=False):\n",
    "        L = min(len(toks), len(idxs))\n",
    "        for wid in range(L):\n",
    "            try:\n",
    "                k = int(idxs[wid])\n",
    "            except Exception:\n",
    "                continue\n",
    "            if k == 0 and not include_zero:\n",
    "                # skip zero if we only want 1..N\n",
    "                continue\n",
    "            if k < 0:\n",
    "                continue\n",
    "            if k > index_max:\n",
    "                k = index_max  # bucket as index_max+\n",
    "            rows.append((sid, wid, str(k), toks[wid]))\n",
    "    df_tok = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\",\"index_class\",\"word\"])\n",
    "    if df_tok.empty:\n",
    "        raise ValueError(\"No token rows constructed—check that your 'index' column contains integer lists.\")\n",
    "    df_sent = df[[\"sentence_id\",\"tokens\"]].drop_duplicates(\"sentence_id\")\n",
    "    return df_sent, df_tok\n",
    "\n",
    "def sample_per_class(df_tok: pd.DataFrame, per_class_cap: int | None) -> pd.DataFrame:\n",
    "    \"\"\"Optional per-class subsample for plotting.\"\"\"\n",
    "    if per_class_cap is None:\n",
    "        return df_tok.reset_index(drop=True)\n",
    "    picks = []\n",
    "    for c, sub in df_tok.groupby(\"index_class\", sort=False):\n",
    "        n = min(len(sub), per_class_cap)\n",
    "        picks.append(sub.sample(n, random_state=RAND_SEED, replace=False))\n",
    "    return pd.concat(picks, ignore_index=True)\n",
    "\n",
    "# --------- Robust model/tokenizer loader (BERT + GPT-2) ----------\n",
    "def _load_tok_and_model(model_id: str):\n",
    "    tried = []\n",
    "    cands = [model_id]\n",
    "    # Helpful alt namespace for GPT‑2\n",
    "    if model_id.lower() in {\"gpt2\", \"gpt-2\"}:\n",
    "        for alt in [\"openai-community/gpt2\", \"gpt2\"]:\n",
    "            if alt not in cands:\n",
    "                cands.append(alt)\n",
    "\n",
    "    last_err = None\n",
    "    for mid in cands:\n",
    "        try:\n",
    "            if \"gpt2\" in mid.lower():\n",
    "                tok = GPT2TokenizerFast.from_pretrained(mid, add_prefix_space=True)\n",
    "            else:\n",
    "                tok = AutoTokenizer.from_pretrained(mid, use_fast=True, add_prefix_space=True)\n",
    "            # Right padding; PAD=EOS for GPT‑2\n",
    "            if getattr(tok, \"padding_side\", None) != \"right\":\n",
    "                tok.padding_side = \"right\"\n",
    "            if tok.pad_token is None and getattr(tok, \"eos_token\", None) is not None:\n",
    "                tok.pad_token = tok.eos_token\n",
    "\n",
    "            mdl = AutoModel.from_pretrained(mid, output_hidden_states=True)\n",
    "            if getattr(mdl.config, \"pad_token_id\", None) is None and tok.pad_token_id is not None:\n",
    "                mdl.config.pad_token_id = tok.pad_token_id\n",
    "            mdl = mdl.eval().to(device)\n",
    "            if device == \"cuda\":\n",
    "                mdl.half()\n",
    "            return tok, mdl, mid\n",
    "        except Exception as e:\n",
    "            tried.append((mid, repr(e))); last_err = e\n",
    "            continue\n",
    "    raise RuntimeError(\"Failed to load tokenizer/model. Attempts:\\n\" + \"\\n\".join(f\" - {m}: {err}\" for m, err in tried)) from last_err\n",
    "\n",
    "# --------- Embed selected tokens ----------\n",
    "def embed_subset(df_sent: pd.DataFrame,\n",
    "                 subset_df: pd.DataFrame,\n",
    "                 baseline: str = BASELINE,\n",
    "                 word_rep_mode: str = WORD_REP_MODE,\n",
    "                 batch_size: int = BATCH_SIZE) -> Tuple[np.ndarray, List[str], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Return:\n",
    "      reps   : (L, N, D) float16\n",
    "      words  : list[str] length N (for hover)\n",
    "      filled : boolean mask (N,) — True where token got embedded\n",
    "    \"\"\"\n",
    "    df_sent[\"sentence_id\"]   = df_sent[\"sentence_id\"].astype(str)\n",
    "    subset_df[\"sentence_id\"] = subset_df[\"sentence_id\"].astype(str)\n",
    "\n",
    "    # sid -> list[(global_idx, word_id)]\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(subset_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(str(sid), []).append((gidx, int(wid)))\n",
    "\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_sent[df_sent.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    tokzr, model, resolved = _load_tok_and_model(baseline)\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "        enc_kwargs[\"add_prefix_space\"] = True\n",
    "\n",
    "    L = _num_hidden_layers(model) + 1\n",
    "    D = _hidden_size(model)\n",
    "    N = len(subset_df)\n",
    "\n",
    "    reps   = np.zeros((L, N, D), np.float16)\n",
    "    words  = [\"\"] * N\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{resolved} (embed subset)\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out    = model(**enc_t)\n",
    "            h      = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L,B,T,D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                # map word_id -> token positions\n",
    "                mp: Dict[int, List[int]] = {}\n",
    "                wids = enc_be.word_ids(b)\n",
    "                if wids is None:\n",
    "                    raise RuntimeError(\"Fast tokenizer required (word_ids() unavailable).\")\n",
    "                for tidx, wid in enumerate(wids):\n",
    "                    if wid is not None:\n",
    "                        mp.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                toks_for_sent = df_sel.loc[sid, \"tokens\"]\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = mp.get(wid)\n",
    "                    if not toks:\n",
    "                        continue\n",
    "                    if word_rep_mode == \"first\":\n",
    "                        vec = h[:, b, toks[0], :]\n",
    "                    elif word_rep_mode == \"last\":\n",
    "                        vec = h[:, b, toks[-1], :]\n",
    "                    elif word_rep_mode == \"mean\":\n",
    "                        vec = h[:, b, toks, :].mean(axis=1)\n",
    "                    else:\n",
    "                        raise ValueError(\"WORD_REP_MODE must be one of {'first','last','mean'}.\")\n",
    "                    reps[:, gidx, :] = vec.astype(np.float16, copy=False)\n",
    "                    words[gidx] = str(toks_for_sent[wid])\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    if (~filled).any():\n",
    "        # drop unfilled rows to keep arrays consistent\n",
    "        reps  = reps[:, filled]\n",
    "        words = [w for w, f in zip(words, filled) if f]\n",
    "    return reps, words, filled\n",
    "\n",
    "# --------- PCA (3D) without extra deps ----------\n",
    "def _pca3d_layer(X: np.ndarray, n_components: int = 3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    PCA to 3D via SVD of centered X; returns (n, 3).\n",
    "    \"\"\"\n",
    "    X = X.astype(np.float32, copy=False)\n",
    "    Xc = X - X.mean(0, keepdims=True)\n",
    "    U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "    Y = (U[:, :n_components] * S[:n_components]).astype(np.float32, copy=False)\n",
    "    return Y\n",
    "\n",
    "# --------- Distinct color for each class ----------\n",
    "def _make_distinct_palette(classes: List[str], scale: str = \"Turbo\") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Return {class -> color} as rgb() strings. We sample a Plotly colorscale evenly\n",
    "    so every class has a distinct color (works well even for many classes).\n",
    "    \"\"\"\n",
    "    n = len(classes)\n",
    "    if n <= 1:\n",
    "        return {classes[0]: \"rgb(0,0,0)\"} if n == 1 else {}\n",
    "    colorscale = pc.get_colorscale(scale)  # e.g., \"Turbo\", \"Viridis\"\n",
    "    ts = np.linspace(0.0, 1.0, n)\n",
    "    sampled = pc.sample_colorscale(colorscale, ts.tolist())\n",
    "    return {cls: col for cls, col in zip(classes, sampled)}\n",
    "\n",
    "# --------- Plotly: PCA‑3D per layer, traces per class ----------\n",
    "def pca3d_by_index_and_plot(reps: np.ndarray,\n",
    "                            words: List[str],\n",
    "                            classes_arr: np.ndarray,\n",
    "                            all_classes: List[str],\n",
    "                            model_tag: str,\n",
    "                            html_out: Path):\n",
    "    \"\"\"\n",
    "    Build one trace per (layer, class); slider toggles layers.\n",
    "    \"\"\"\n",
    "    L, N, D = reps.shape\n",
    "    print(f\"PCA plotting on {N:,} tokens across {L} layers...\")\n",
    "\n",
    "    # PCA per layer\n",
    "    Y_layers = [ _pca3d_layer(reps[l]) for l in range(L) ]\n",
    "\n",
    "    # Colors\n",
    "    cmap = _make_distinct_palette(all_classes, scale=\"Turbo\")\n",
    "    print(\"\\nPalette (class -> color):\")\n",
    "    for k in all_classes:\n",
    "        print(f\"  {k:>3} -> {cmap[k]}\")\n",
    "\n",
    "    traces = []\n",
    "    # add traces in layer-major order: for each layer, add one trace per class\n",
    "    for l in range(L):\n",
    "        Y = Y_layers[l]\n",
    "        show_legend = (l == 0)  # only show legend for layer 0\n",
    "        for c in all_classes:\n",
    "            mask = (classes_arr == c)\n",
    "            x = Y[mask, 0] if np.any(mask) else []\n",
    "            y = Y[mask, 1] if np.any(mask) else []\n",
    "            z = Y[mask, 2] if np.any(mask) else []\n",
    "            hovertxt = [f\"{w} | idx={c}\" for w in (np.array(words)[mask] if np.any(mask) else [])]\n",
    "\n",
    "            traces.append(\n",
    "                go.Scatter3d(\n",
    "                    x=x, y=y, z=z,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=2, opacity=0.75, color=cmap[c]),\n",
    "                    name=str(c),\n",
    "                    hovertext=hovertxt,\n",
    "                    # IMPORTANT: no Python %-format on strings containing %{...}\n",
    "                    hovertemplate=\"<b>%{hovertext}</b><br>\"\n",
    "                                  \"x=%{x:.3f}<br>y=%{y:.3f}<br>z=%{z:.3f}<extra></extra>\",\n",
    "                    visible=(l == 0),\n",
    "                    showlegend=show_legend\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Slider steps: turn on the L * |classes| block for layer l\n",
    "    n_per_layer = len(all_classes)\n",
    "    steps = []\n",
    "    for l in range(L):\n",
    "        vis = [False] * (L * n_per_layer)\n",
    "        start = l * n_per_layer\n",
    "        vis[start:start + n_per_layer] = [True] * n_per_layer\n",
    "\n",
    "        steps.append(dict(\n",
    "            method=\"update\",\n",
    "            args=[\n",
    "                {\"visible\": vis},\n",
    "                {\"title\": {\n",
    "                    \"text\": f\"{model_tag} • PCA 3D by index • Layer {l} (drag to rotate)\",\n",
    "                    \"font\": {\"size\": PLOT_TITLE_FONT},\n",
    "                }},\n",
    "            ],\n",
    "            label=str(l),\n",
    "        ))\n",
    "\n",
    "    sliders = [dict(\n",
    "        active=0,\n",
    "        steps=steps,\n",
    "        currentvalue={\"prefix\": \"Layer: \", \"font\": {\"size\": PLOT_SLIDER_FONT}},\n",
    "        font={\"size\": PLOT_SLIDER_FONT},   # step label font\n",
    "        pad={\"t\": 10}\n",
    "    )]\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title={\n",
    "            \"text\": f\"{model_tag} • PCA 3D by index • Layer 0 (drag to rotate)\",\n",
    "            \"font\": {\"size\": PLOT_TITLE_FONT},\n",
    "        },\n",
    "        font={\"size\": PLOT_FONT},\n",
    "        hoverlabel={\"font\": {\"size\": PLOT_HOVER_FONT}},\n",
    "\n",
    "        scene=dict(\n",
    "            xaxis=dict(\n",
    "                title=dict(text=\"PC1\", font=dict(size=PLOT_AXIS_TITLE_FONT)),\n",
    "                tickfont=dict(size=PLOT_AXIS_TICK_FONT),\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title=dict(text=\"PC2\", font=dict(size=PLOT_AXIS_TITLE_FONT)),\n",
    "                tickfont=dict(size=PLOT_AXIS_TICK_FONT),\n",
    "            ),\n",
    "            zaxis=dict(\n",
    "                title=dict(text=\"PC3\", font=dict(size=PLOT_AXIS_TITLE_FONT)),\n",
    "                tickfont=dict(size=PLOT_AXIS_TICK_FONT),\n",
    "            ),\n",
    "            aspectmode=\"data\",\n",
    "        ),\n",
    "\n",
    "        margin=dict(l=0, r=0, b=0, t=60),\n",
    "        sliders=sliders,\n",
    "\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            title=dict(text=\"index class\", font=dict(size=PLOT_LEGEND_FONT)),\n",
    "            font=dict(size=PLOT_LEGEND_FONT),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "   # fig.show()\n",
    "    fig.write_html(str(html_out), include_plotlyjs=\"cdn\")\n",
    "    print(\"\\n✓ Saved interactive HTML to:\", html_out)\n",
    "\n",
    "# =============================== DRIVER ===============================\n",
    "def run_pca3d_index():\n",
    "    # 1) Load tokens + index classes\n",
    "    df_sent, idx_df = load_index_from_column(\n",
    "        CSV_PATH, index_max=INDEX_MAX_CLASS, include_zero=INCLUDE_ZERO_CLASS\n",
    "    )\n",
    "\n",
    "    # 2) Optional per-class subsample for plotting\n",
    "    raw_df = sample_per_class(idx_df, PCA_MAX_PER_CLASS)\n",
    "    classes = sorted(raw_df.index_class.unique(), key=lambda s: int(s))\n",
    "    print(f\"✓ plotting subset — {len(raw_df):,} tokens across index classes {classes}\")\n",
    "\n",
    "    # 3) Embed once\n",
    "    reps, words, filled = embed_subset(df_sent, raw_df, BASELINE, WORD_REP_MODE, BATCH_SIZE)\n",
    "    raw_df = raw_df.reset_index(drop=True).loc[filled].reset_index(drop=True)\n",
    "\n",
    "    # Align labels with reps\n",
    "    cls_arr = raw_df.index_class.values.astype(str)\n",
    "\n",
    "    # 4) PCA→3D per layer + Plotly\n",
    "    pca3d_by_index_and_plot(\n",
    "        reps.astype(np.float32, copy=False),\n",
    "        words,\n",
    "        cls_arr,\n",
    "        classes,\n",
    "        model_tag=BASELINE,\n",
    "        html_out=HTML_OUT\n",
    "    )\n",
    "\n",
    "    # Cleanup\n",
    "    del reps\n",
    "    gc.collect()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pca3d_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aae233-9482-42ea-b77f-6fb3e3c0a724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
