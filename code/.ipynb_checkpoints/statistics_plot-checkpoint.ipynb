{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "984eab21",
   "metadata": {},
   "source": [
    "# Dataset feature distributions (plot-only)\n",
    "\n",
    "This notebook is a **reader-facing** version of `statistics.ipynb`: it shows the *idea* of the dataset statistics used in the paper, and **only produces a plot** (no CSV dumps, no result folders).\n",
    "\n",
    "**Input**: a CSV with one row per sentence and list-like columns (e.g. `tokens`, optionally `pos`, `head_dist`, `relation_type`, `arity`).\n",
    "\n",
    "**Output**: a single Matplotlib figure summarizing the distributions used for binning:\n",
    "- token **index** (first 10 positions)\n",
    "- token **length** (capped at 10)\n",
    "- **POS** (top categories)\n",
    "- dependency **head distance** (binned in [-6, 6])\n",
    "- **arity** (capped)\n",
    "- dependency **relation type** (top-K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a3c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import ast\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================== CONFIG ===========================\n",
    "CSV_PATH = os.getenv(\"CSV_PATH\", \"en_ewt-ud-train_sentences.csv\")\n",
    "\n",
    "EXCLUDE_SENT_START = False  # drop word_id == 0 across all stats (optional)\n",
    "\n",
    "MAX_INDEX = 10\n",
    "MAX_LENGTH = 10\n",
    "MIN_HEAD_DIST, MAX_HEAD_DIST = -6, 6\n",
    "MAX_ARITY = 6\n",
    "\n",
    "TOPK_POS = 14\n",
    "TOPK_REL = 12\n",
    "\n",
    "SAVE_FIG = False\n",
    "FIG_PATH = Path(\"dataset_feature_distributions.png\")\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551630b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_list(x):\n",
    "    \"\"\"Parse list-like cells robustly (accept Python-lists serialized as strings).\"\"\"\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip()\n",
    "        if x.startswith('[') and x.endswith(']'):\n",
    "            try:\n",
    "                return ast.literal_eval(x)\n",
    "            except Exception:\n",
    "                return [x]\n",
    "        return [x]\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    return [x]\n",
    "\n",
    "\n",
    "def _pick_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    \"\"\"Return the first column from candidates that exists in df, else None.\"\"\"\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "def expand_to_tokens(csv_path: str, exclude_sent_start: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Expand sentence-level CSV rows into a token-level DataFrame.\n",
    "\n",
    "    Expected columns (flexible names):\n",
    "      - sentence_id\n",
    "      - tokens (list of surface forms)\n",
    "      - optional: pos, index/position, head_dist, relation_type, arity (all list-like)\n",
    "\n",
    "    Missing optional columns are filled with NaN.\n",
    "    \"\"\"\n",
    "    df_all = pd.read_csv(csv_path)\n",
    "\n",
    "    sid_col = _pick_col(df_all, [\"sentence_id\", \"sent_id\", \"sid\", \"SentenceID\"])\n",
    "    tok_col = _pick_col(df_all, [\"tokens\", \"Tokens\", \"words\", \"Words\"])\n",
    "    pos_col = _pick_col(df_all, [\"pos\", \"upos\", \"POS\", \"Pos\"])\n",
    "    idx_col = _pick_col(df_all, [\"index\", \"token_index\", \"position\", \"positions\", \"idx\"])\n",
    "    hd_col  = _pick_col(df_all, [\"head_dist\", \"head_distance\", \"dep_head_dist\", \"gov_dist\"])\n",
    "    rel_col = _pick_col(df_all, [\"relation_type\", \"deprel\", \"dep_rel\", \"relation\", \"rel\"])\n",
    "    ar_col  = _pick_col(df_all, [\"arity\", \"head_arity\", \"dep_arity\"])\n",
    "\n",
    "    if sid_col is None or tok_col is None:\n",
    "        raise ValueError(\n",
    "            f\"CSV must contain sentence id + tokens. Found columns: {list(df_all.columns)[:25]}\"\n",
    "\n",
    "            f\"Tried sentence_id candidates: {['sentence_id','sent_id','sid','SentenceID']}\\n\"\n",
    "            f\"Tried tokens candidates: {['tokens','Tokens','words','Words']}\"\n",
    "        )\n",
    "\n",
    "    # Parse list-like columns\n",
    "    df_all[tok_col] = df_all[tok_col].apply(_to_list)\n",
    "    if pos_col is not None: df_all[pos_col] = df_all[pos_col].apply(_to_list)\n",
    "    if idx_col is not None: df_all[idx_col] = df_all[idx_col].apply(_to_list)\n",
    "    if hd_col  is not None: df_all[hd_col]  = df_all[hd_col].apply(_to_list)\n",
    "    if rel_col is not None: df_all[rel_col] = df_all[rel_col].apply(_to_list)\n",
    "    if ar_col  is not None: df_all[ar_col]  = df_all[ar_col].apply(_to_list)\n",
    "\n",
    "    rows = []\n",
    "    for _, r in df_all.iterrows():\n",
    "        sid = r[sid_col]\n",
    "        toks = r[tok_col]\n",
    "\n",
    "        posl = r[pos_col] if pos_col is not None else None\n",
    "        idxl = r[idx_col] if idx_col is not None else None\n",
    "        hdl  = r[hd_col]  if hd_col  is not None else None\n",
    "        rell = r[rel_col] if rel_col is not None else None\n",
    "        arl  = r[ar_col]  if ar_col  is not None else None\n",
    "\n",
    "        # Make sure all list-like fields align in length\n",
    "        L = len(toks)\n",
    "        for lst in (posl, idxl, hdl, rell, arl):\n",
    "            if lst is not None:\n",
    "                L = min(L, len(lst))\n",
    "\n",
    "        for wid in range(L):\n",
    "            if exclude_sent_start and wid == 0:\n",
    "                continue\n",
    "\n",
    "            token = toks[wid]\n",
    "            pos   = str(posl[wid]).upper() if (posl is not None and posl[wid] is not None) else np.nan\n",
    "\n",
    "            # If no explicit index column, default to 1-based word position\n",
    "            index = (idxl[wid] if idxl is not None else (wid + 1))\n",
    "\n",
    "            hd    = (hdl[wid]  if hdl  is not None else np.nan)\n",
    "            rel   = (rell[wid] if rell is not None else np.nan)\n",
    "            arity = (arl[wid]  if arl  is not None else np.nan)\n",
    "\n",
    "            length = len(str(token))\n",
    "\n",
    "            rows.append((sid, wid, token, pos, index, length, hd, rel, arity))\n",
    "\n",
    "    tok_df = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\n",
    "            \"sentence_id\", \"word_id\", \"word\", \"pos\",\n",
    "            \"index\", \"length\", \"head_dist\", \"relation_type\", \"arity\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    tok_df[\"word_norm\"] = tok_df[\"word\"].astype(str).str.casefold()\n",
    "    return tok_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================== LOAD ===========================\n",
    "print(f\"Reading: {CSV_PATH}\")\n",
    "df_tokens = expand_to_tokens(CSV_PATH, exclude_sent_start=EXCLUDE_SENT_START)\n",
    "print(f\"Expanded to {len(df_tokens):,} token rows.\")\n",
    "\n",
    "# Helper: safe numeric conversion\n",
    "for col in [\"index\", \"length\", \"head_dist\", \"arity\"]:\n",
    "    if col in df_tokens.columns:\n",
    "        df_tokens[col] = pd.to_numeric(df_tokens[col], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7c93c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================== COUNTS ===========================\n",
    "# Numeric features\n",
    "index_counts = (\n",
    "    df_tokens[df_tokens[\"index\"].between(1, MAX_INDEX)][\"index\"]\n",
    "    .value_counts(sort=False)\n",
    "    .reindex(range(1, MAX_INDEX + 1), fill_value=0)\n",
    ")\n",
    "\n",
    "length_counts = (\n",
    "    df_tokens[df_tokens[\"length\"].between(1, MAX_LENGTH)][\"length\"]\n",
    "    .value_counts(sort=False)\n",
    "    .reindex(range(1, MAX_LENGTH + 1), fill_value=0)\n",
    ")\n",
    "\n",
    "head_dist_counts = None\n",
    "if df_tokens[\"head_dist\"].notna().any():\n",
    "    bins = list(range(MIN_HEAD_DIST, MAX_HEAD_DIST + 1))\n",
    "    head_dist_counts = (\n",
    "        df_tokens[df_tokens[\"head_dist\"].between(MIN_HEAD_DIST, MAX_HEAD_DIST)][\"head_dist\"]\n",
    "        .round()\n",
    "        .astype(int)\n",
    "        .value_counts(sort=False)\n",
    "        .reindex(bins, fill_value=0)\n",
    "    )\n",
    "\n",
    "arity_counts = None\n",
    "if df_tokens[\"arity\"].notna().any():\n",
    "    ar_bins = list(range(0, MAX_ARITY + 1))\n",
    "    arity_counts = (\n",
    "        df_tokens[df_tokens[\"arity\"].between(0, MAX_ARITY)][\"arity\"]\n",
    "        .round()\n",
    "        .astype(int)\n",
    "        .value_counts(sort=False)\n",
    "        .reindex(ar_bins, fill_value=0)\n",
    "    )\n",
    "\n",
    "# Categorical features\n",
    "pos_counts = df_tokens[\"pos\"].dropna().value_counts().head(TOPK_POS)\n",
    "rel_counts = df_tokens[\"relation_type\"].dropna().value_counts().head(TOPK_REL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90a6660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================== PLOT ===========================\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 9), constrained_layout=True)\n",
    "ax_pos, ax_rel, ax_idx, ax_len, ax_hd, ax_ar = axes.flatten()\n",
    "\n",
    "# POS (horizontal)\n",
    "ax_pos.barh(list(reversed(pos_counts.index.tolist())), list(reversed(pos_counts.values.tolist())))\n",
    "ax_pos.set_title(f\"POS (top {TOPK_POS})\")\n",
    "ax_pos.set_xlabel(\"# tokens\")\n",
    "\n",
    "# Relation types (horizontal)\n",
    "ax_rel.barh(list(reversed(rel_counts.index.tolist())), list(reversed(rel_counts.values.tolist())))\n",
    "ax_rel.set_title(f\"Dependency relation (top {TOPK_REL})\")\n",
    "ax_rel.set_xlabel(\"# tokens\")\n",
    "\n",
    "# Index\n",
    "ax_idx.bar(index_counts.index.astype(int), index_counts.values)\n",
    "ax_idx.set_title(f\"Token index (1..{MAX_INDEX})\")\n",
    "ax_idx.set_xlabel(\"Index in sentence\")\n",
    "ax_idx.set_ylabel(\"# tokens\")\n",
    "\n",
    "# Length\n",
    "ax_len.bar(length_counts.index.astype(int), length_counts.values)\n",
    "ax_len.set_title(f\"Token length (1..{MAX_LENGTH} chars)\")\n",
    "ax_len.set_xlabel(\"Length\")\n",
    "ax_len.set_ylabel(\"# tokens\")\n",
    "\n",
    "# Head distance\n",
    "if head_dist_counts is None:\n",
    "    ax_hd.text(0.5, 0.5, \"No head_dist column found\", ha=\"center\", va=\"center\")\n",
    "    ax_hd.set_axis_off()\n",
    "else:\n",
    "    ax_hd.bar(head_dist_counts.index.astype(int), head_dist_counts.values)\n",
    "    ax_hd.set_title(f\"Head distance ({MIN_HEAD_DIST}..{MAX_HEAD_DIST})\")\n",
    "    ax_hd.set_xlabel(\"Signed distance to head\")\n",
    "    ax_hd.set_ylabel(\"# tokens\")\n",
    "\n",
    "# Arity\n",
    "if arity_counts is None:\n",
    "    ax_ar.text(0.5, 0.5, \"No arity column found\", ha=\"center\", va=\"center\")\n",
    "    ax_ar.set_axis_off()\n",
    "else:\n",
    "    ax_ar.bar(arity_counts.index.astype(int), arity_counts.values)\n",
    "    ax_ar.set_title(f\"Arity (0..{MAX_ARITY})\")\n",
    "    ax_ar.set_xlabel(\"# dependents (capped)\")\n",
    "    ax_ar.set_ylabel(\"# tokens\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if SAVE_FIG:\n",
    "    FIG_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(FIG_PATH, dpi=200)\n",
    "    print(f\"âœ“ Saved figure to {FIG_PATH.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
