{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fccdd2f-009b-403c-a55e-4cd573675633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "import numpy as np, pandas as pd, torch\n",
    "import torch.utils.data as torchdata\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel, BertModel, AutoConfig\n",
    "from IsoScore import IsoScore\n",
    "from dadapy import Data\n",
    "from skdim.id import MLE, MOM, TLE, CorrInt, FisherS, lPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091e40a1-a8b7-4760-bfe5-a867493dcd28",
   "metadata": {},
   "source": [
    "# Bert POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35060cf-4645-45b2-b952-d46616a6b8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f500151a380>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ldomenichelli/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "HAS_DADAPY = False\n",
    "try:\n",
    "    from dadapy import Data  \n",
    "    HAS_DADAPY = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "HAS_SKDIM = False\n",
    "try:\n",
    "    from skdim.id import (\n",
    "        MOM, TLE, CorrInt, FisherS, lPCA,\n",
    "        MLE, DANCo, ESS, MiND_ML, MADA, KNN\n",
    "    )\n",
    "    HAS_SKDIM = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from isoscore import IsoScore\n",
    "    _HAS_ISOSCORE = True\n",
    "except Exception:\n",
    "    _HAS_ISOSCORE = False\n",
    "    class _IsoScoreFallback:\n",
    "        @staticmethod\n",
    "        def IsoScore(X: np.ndarray) -> float:\n",
    "            C = np.cov(X.T, ddof=0)\n",
    "            ev = np.linalg.eigvalsh(C)\n",
    "            if ev.mean() <= 0 or ev[-1] <= 0:\n",
    "                return 0.0\n",
    "            # mean/peak eigenvalue ratio in [0,1]; higher ≈ more isotropic\n",
    "            return float(np.clip(ev.mean() / ev[-1], 0.0, 1.0))\n",
    "    IsoScore = _IsoScoreFallback()\n",
    "\n",
    "CSV_PATH   = \"en_ewt-ud-train_sentences.csv\"\n",
    "BASELINE   = \"bert-base-uncased\"\n",
    "WORD_REP_MODE = \"first\"     \n",
    "EXCLUDE_POS = {\"X\", \"SYM\", \"PART\", \"INTJ\"}\n",
    "RAW_MAX_PER_POS = int(1e12)         \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "N_BOOTSTRAP_FAST   = 50         \n",
    "N_BOOTSTRAP_HEAVY  = 200          \n",
    "\n",
    "FAST_BS_MAX_SAMP_PER_POS  = int(1e12)   \n",
    "HEAVY_BS_MAX_SAMP_PER_POS = 5000       \n",
    "# GRIDE multi-scale max neighbor rank\n",
    "DADAPY_GRID_RANGE_MAX = 64             # 32–128 is typical\n",
    "RAND_SEED=42\n",
    "PLOT_DIR     = Path(\"results_POS\"); PLOT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CSV_DIR      = Path(\"tables_POS\") / \"pos_bootstrap\"; CSV_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Throughput: start higher than 1 unless GPU is tiny\n",
    "BATCH_SIZE = 1                         # try 8 → 16 → 32; back off if OOM\n",
    "\n",
    "# Reproducibility & device\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Seaborn style\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "\n",
    "# =============================== HELPERS ===============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _center(X: np.ndarray) -> np.ndarray:\n",
    "    return X - X.mean(0, keepdims=True)\n",
    "\n",
    "def _eigvals_from_X(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Eigenvalues of covariance up to a constant via SVD of centered X (descending).\"\"\"\n",
    "    Xc = _center(X.astype(np.float32, copy=False))\n",
    "    try:\n",
    "        _, S, _ = np.linalg.svd(Xc, full_matrices=False)\n",
    "        lam = (S**2).astype(np.float64)\n",
    "        lam.sort()\n",
    "        return lam[::-1]\n",
    "    except Exception:\n",
    "        return np.array([], dtype=np.float64)\n",
    "\n",
    "def _jitter_unique(X: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"Add tiny noise if there are duplicate rows (helps NN-based estimators).\"\"\"\n",
    "    try:\n",
    "        if np.unique(X, axis=0).shape[0] < X.shape[0]:\n",
    "            X = X + np.random.normal(scale=eps, size=X.shape).astype(X.dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return X\n",
    "\n",
    "# ========= Per-subsample single-value compute functions (used inside bootstrap) =========\n",
    "# --- Isotropy (fast) ---\n",
    "def _iso_once(X: np.ndarray) -> float:\n",
    "    return float(IsoScore.IsoScore(X))\n",
    "\n",
    "def _spect_once(X: np.ndarray) -> float:\n",
    "    ev = np.linalg.eigvalsh(np.cov(X.T, ddof=0))\n",
    "    return float(ev[-1] / (ev.mean() + 1e-9))\n",
    "\n",
    "def _rand_once(X: np.ndarray, K: int = 2000) -> float:\n",
    "    n = X.shape[0]\n",
    "    if n < 2: return np.nan\n",
    "    rng = np.random.default_rng()\n",
    "    K_eff = min(K, (n*(n-1))//2)\n",
    "    i = rng.integers(0, n, size=K_eff)\n",
    "    j = rng.integers(0, n, size=K_eff)\n",
    "    same = i == j\n",
    "    if same.any():\n",
    "        j[same] = rng.integers(0, n, size=same.sum())\n",
    "    A, B = X[i], X[j]\n",
    "    num = np.sum(A*B, axis=1)\n",
    "    den = (np.linalg.norm(A, axis=1)*np.linalg.norm(B, axis=1) + 1e-9)\n",
    "    return float(np.mean(np.abs(num/den)))\n",
    "\n",
    "def _sf_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    gm = np.exp(np.mean(np.log(lam + EPS)))\n",
    "    am = float(lam.mean() + EPS)\n",
    "    return float(gm / am)\n",
    "\n",
    "def _pfI_once(X: np.ndarray) -> float:\n",
    "    n, d = X.shape\n",
    "    if n < 2: return np.nan\n",
    "    rng = np.random.default_rng()\n",
    "    U = rng.standard_normal((PFI_DIRS, d)).astype(np.float32)\n",
    "    U /= np.linalg.norm(U, axis=1, keepdims=True) + 1e-9\n",
    "    S = U @ X.T\n",
    "    m = np.max(S, axis=1, keepdims=True)\n",
    "    logZ = (m + np.log(np.sum(np.exp(S - m), axis=1, keepdims=True))).ravel()\n",
    "    lo = np.percentile(logZ, PFI_Q_LO)\n",
    "    hi = np.percentile(logZ, PFI_Q_HI)\n",
    "    return float(np.exp(lo - hi))  # ≈ min Z / max Z (robust)\n",
    "\n",
    "def _vmf_kappa_once(X: np.ndarray) -> float:\n",
    "    if X.shape[0] < 2: return np.nan\n",
    "    Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
    "    R = np.linalg.norm(Xn.mean(axis=0))\n",
    "    d = Xn.shape[1]\n",
    "    if R < 1e-9: return 0.0\n",
    "    # standard closed-form approximation\n",
    "    return float(max(R * (d - R**2) / (1.0 - R**2 + 1e-9), 0.0))\n",
    "\n",
    "# --- Linear ID (fast) ---\n",
    "def _pcaXX_once(X: np.ndarray, var_ratio: float) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    c = np.cumsum(lam); thr = c[-1] * var_ratio\n",
    "    return float(np.searchsorted(c, thr) + 1)\n",
    "\n",
    "def _pca95_once(X: np.ndarray) -> float:\n",
    "    return _pcaXX_once(X, 0.95)\n",
    "\n",
    "def _pca99_once(X: np.ndarray) -> float:\n",
    "    return _pcaXX_once(X, 0.99)\n",
    "\n",
    "def _erank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    p = lam / (lam.sum() + EPS)\n",
    "    H = -(p * np.log(p + EPS)).sum()\n",
    "    return float(np.exp(H))\n",
    "\n",
    "def _pr_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    s1 = lam.sum(); s2 = (lam**2).sum()\n",
    "    return float((s1**2) / (s2 + EPS))\n",
    "\n",
    "def _stable_rank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    return float(lam.sum() / (lam.max() + EPS))\n",
    "\n",
    "# --- Non-linear (heavy) ---\n",
    "def _dadapy_twonn_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    id_est, _, _ = d.compute_id_2NN()\n",
    "    return float(id_est)\n",
    "\n",
    "def _dadapy_gride_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    d.compute_distances(maxk=DADAPY_GRID_RANGE_MAX)\n",
    "    ids, _, _ = d.return_id_scaling_gride(range_max=DADAPY_GRID_RANGE_MAX)\n",
    "    return float(ids[-1])\n",
    "\n",
    "def _skdim_factory(name: str):\n",
    "    \"\"\"Return a factory that builds a fresh skdim estimator each call, or None.\"\"\"\n",
    "    if not HAS_SKDIM: return None\n",
    "    mapping = {\n",
    "        \"mom\": MOM, \"tle\": TLE, \"corrint\": CorrInt, \"fishers\": FisherS,\n",
    "        \"lpca\": lPCA, \"lpca99\": lPCA,\n",
    "        \"mle\": MLE, \"danco\": DANCo,  \"mind_ml\": MiND_ML,\n",
    "        \"mada\": MADA, \"knn\": KNN,\n",
    "    }\n",
    "    cls = mapping.get(name)\n",
    "    if cls is None: return None\n",
    "\n",
    "    def _builder():\n",
    "        if name == \"lpca\":      # FO variant\n",
    "            return cls(ver=\"FO\")\n",
    "        elif name == \"lpca99\":  # ratio (0.99) variant\n",
    "            return cls(ver=\"ratio\", alphaRatio=0.99)\n",
    "        else:\n",
    "            return cls()\n",
    "    return _builder\n",
    "\n",
    "def _skdim_once_builder(name: str) -> Callable[[np.ndarray], float] | None:\n",
    "    build = _skdim_factory(name)\n",
    "    if build is None: return None\n",
    "\n",
    "    def _once(X: np.ndarray) -> float:\n",
    "        est = build()\n",
    "        est.fit(_jitter_unique(X))\n",
    "        return float(getattr(est, \"dimension_\", np.nan))\n",
    "    return _once\n",
    "\n",
    "\n",
    "# =============================== DATA ===============================\n",
    "def load_word_df(csv_path: str, exclude_pos: set[str] = EXCLUDE_POS):\n",
    "    df = pd.read_csv(csv_path, usecols=[\"sentence_id\",\"tokens\",\"pos\"])\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)  # keep string IDs\n",
    "    df.tokens = df.tokens.apply(_to_list); df.pos = df.pos.apply(_to_list)\n",
    "\n",
    "    rows = []\n",
    "    for sid, toks, poss in df[[\"sentence_id\",\"tokens\",\"pos\"]].itertuples(index=False):\n",
    "        for wid, (tok, p) in enumerate(zip(toks, poss)):\n",
    "            if p not in exclude_pos:\n",
    "                rows.append((sid, wid, p, tok))\n",
    "    word_df = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\",\"pos\",\"word\"])\n",
    "    return df, word_df\n",
    "\n",
    "def sample_raw(word_df: pd.DataFrame, per_pos_cap: int = RAW_MAX_PER_POS) -> pd.DataFrame:\n",
    "    \"\"\"Per-POS cap without frequency matching.\"\"\"\n",
    "    picks = []\n",
    "    for p, sub in word_df.groupby(\"pos\", sort=False):\n",
    "        n = min(len(sub), per_pos_cap)\n",
    "        picks.append(sub.sample(n, random_state=RAND_SEED, replace=False))\n",
    "    return pd.concat(picks, ignore_index=True)\n",
    "\n",
    "\n",
    "def make_class_palette(classes: List[str]) -> Dict[str, Tuple[float, float, float]]:\n",
    "    \"\"\"\n",
    "    Return a deterministic mapping {class -> RGB tuple} with as many distinct\n",
    "    qualitative colors as needed. Up to ~60 unique colors without reuse.\n",
    "    \"\"\"\n",
    "    # Try three matplotlib tab palettes first (20 + 20 + 20)\n",
    "    base_colors: List[Tuple[float, float, float]] = []\n",
    "    for name in (\"tab20\", \"tab20b\", \"tab20c\"):\n",
    "        try:\n",
    "            base_colors.extend(sns.color_palette(name, 20))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # If classes exceed our pool, fall back to evenly spaced hues\n",
    "    if len(base_colors) < len(classes):\n",
    "        base_colors = list(sns.color_palette(\"husl\", len(classes)))  # evenly spaced hues\n",
    "\n",
    "    # Deterministic order (sorted) -> stable color assignment\n",
    "    ordered = list(sorted(classes))\n",
    "    return {cls: base_colors[i % len(base_colors)] for i, cls in enumerate(ordered)}\n",
    "\n",
    "# =============================== EMBEDDING ===============================\n",
    "def embed_subset(df_all_sentences: pd.DataFrame,\n",
    "                 subset_df: pd.DataFrame,\n",
    "                 baseline: str = BASELINE,\n",
    "                 word_rep_mode: str = WORD_REP_MODE,\n",
    "                 batch_size: int = BATCH_SIZE) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return reps (L,N,D) and filled mask (N,) for the selected tokens.\"\"\"\n",
    "    df_all_sentences[\"sentence_id\"] = df_all_sentences[\"sentence_id\"].astype(str)\n",
    "    subset_df[\"sentence_id\"] = subset_df[\"sentence_id\"].astype(str)\n",
    "\n",
    "    # sid -> list[(global_idx, word_id)]\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(subset_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(str(sid), []).append((gidx, int(wid)))\n",
    "\n",
    "    # Materialize in the exact order we will batch\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_all_sentences[df_all_sentences.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    tokzr = AutoTokenizer.from_pretrained(baseline, use_fast=True, add_prefix_space=True)\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
    "    if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "        enc_kwargs[\"add_prefix_space\"] = True\n",
    "\n",
    "    model = AutoModel.from_pretrained(baseline, output_hidden_states=True).eval().to(device)\n",
    "    if device == \"cuda\":\n",
    "        model.half()\n",
    "\n",
    "    L = model.config.num_hidden_layers + 1\n",
    "    D = model.config.hidden_size\n",
    "    N = len(subset_df)\n",
    "\n",
    "    reps   = np.zeros((L, N, D), np.float16)\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{baseline} (embed subset)\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L,B,T,D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                # word_id -> token positions for this item\n",
    "                mp = {}\n",
    "                for tidx, wid in enumerate(enc_be.word_ids(b)):\n",
    "                    if wid is not None:\n",
    "                        mp.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = mp.get(wid)\n",
    "                    if not toks: continue\n",
    "                    if word_rep_mode == \"first\":\n",
    "                        vec = h[:, b, toks[0], :]\n",
    "                    else:\n",
    "                        vec = h[:, b, toks, :].mean(axis=1)\n",
    "                    reps[:, gidx, :] = vec.astype(np.float16, copy=False)\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            # free batch buffers\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    missing = int((~filled).sum())\n",
    "    if missing:\n",
    "        print(f\"⚠ Missing vectors for {missing} of {N} sampled words\")\n",
    "    del model; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    return reps, filled\n",
    "\n",
    "\n",
    "# =============================== BOOTSTRAP CORE ===============================\n",
    "def _bs_layer_loop(rep_sub: np.ndarray, M: int, n_reps: int, compute_once: Callable[[np.ndarray], float]):\n",
    "    \"\"\"Bootstrap: sample M with replacement and apply compute_once(X_layer) -> scalar for each layer.\"\"\"\n",
    "    L, N, D = rep_sub.shape\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    A = np.full((n_reps, L), np.nan, np.float32)\n",
    "    for r in range(n_reps):\n",
    "        idx = rng.integers(0, N, size=M)\n",
    "        for l in range(L):\n",
    "            X = rep_sub[l, idx].astype(np.float32, copy=False)\n",
    "            try:\n",
    "                A[r, l] = float(compute_once(X))\n",
    "            except Exception:\n",
    "                A[r, l] = np.nan\n",
    "    mu = np.nanmean(A, axis=0).astype(np.float32)\n",
    "    lo = np.nanpercentile(A, 2.5, axis=0).astype(np.float32)\n",
    "    hi = np.nanpercentile(A, 97.5, axis=0).astype(np.float32)\n",
    "    return mu, lo, hi\n",
    "\n",
    "# fast metric registry (name -> callable(X)->float)\n",
    "FAST_ONCE: Dict[str, Callable[[np.ndarray], float]] = {\n",
    "    \"iso\": _iso_once,\n",
    "    \"spect\": _spect_once,\n",
    "    \"rand\": _rand_once,\n",
    "    \"sf\": _sf_once,\n",
    "    \"vmf_kappa\": _vmf_kappa_once,\n",
    "    \"erank\": _erank_once,\n",
    "    \"pr\": _pr_once,\n",
    "    \"stable_rank\": _stable_rank_once,\n",
    "}\n",
    "\n",
    "# heavy metric registry\n",
    "HEAVY_ONCE: Dict[str, Callable[[np.ndarray], float] | None] = {\n",
    "    \"twonn\": _dadapy_twonn_once,\n",
    "    \"gride\": _dadapy_gride_once,\n",
    "    \"mom\":   _skdim_once_builder(\"mom\"),\n",
    "    \"tle\":   _skdim_once_builder(\"tle\"),\n",
    "    \"corrint\": _skdim_once_builder(\"corrint\"),\n",
    "    \"fishers\": _skdim_once_builder(\"fishers\"),\n",
    "    \"lpca\":  _skdim_once_builder(\"lpca\"),\n",
    "    \"lpca95\": _skdim_once_builder(\"lpca95\"),\n",
    "    \"lpca99\": _skdim_once_builder(\"lpca99\"),\n",
    "    \"mle\":   _skdim_once_builder(\"mle\"),\n",
    "    \"mada\":  _skdim_once_builder(\"mada\"),\n",
    "    \"knn\":   _skdim_once_builder(\"knn\"),\n",
    "}\n",
    "\n",
    "LABELS = {\n",
    "    # Isotropy\n",
    "    \"iso\":\"IsoScore\",\"spect\":\"Spectral Ratio\",\"rand\":\"RandCos |μ|\",\n",
    "    \"sf\":\"Spectral Flatness\",\"vmf_kappa\":\"vMF κ\",\n",
    "    # Linear ID\n",
    "    \"erank\":\"Effective Rank\",\"pr\":\"Participation Ratio\",\"stable_rank\":\"Stable Rank\",\n",
    "    \"lpca95\":\"lPCA95\",\"lpca99\":\"lPCA99\",\"lpca\":\"lPCA FO\",\n",
    "    # Non-linear\n",
    "    \"twonn\":\"TwoNN ID\",\"gride\":\"GRIDE\",\n",
    "    \"mom\":\"MOM\",\"tle\":\"TLE\",\"corrint\":\"CorrInt\",\n",
    "    \"fishers\":\"FisherS\",\n",
    "    \"mle\":\"MLE\",\"mada\":\"MADA\",\"knn\":\"KNN\",\n",
    "}\n",
    "\n",
    "# Choose plotting order\n",
    "PLOT_ORDER = (\n",
    "    \"iso\",\"sf\",\"vmf_kappa\",\"spect\",\"rand\",\n",
    "    \"erank\",\"pr\",\"stable_rank\",\"lpca95\",\"lpca99\",\"lpca\",\n",
    "    \"twonn\",\"gride\",\"mom\",\"tle\",\"corrint\",\"fishers\",\n",
    "    \"mle\",\"mada\",\"knn\"\n",
    ")\n",
    "\n",
    "# metrics you want to compute (you can prune this list to reduce runtime)\n",
    "#ALL_METRICS = list(PLOT_ORDER)\n",
    "ALL_METRICS = [\"gride\"]\n",
    "\n",
    "\n",
    "# =============================== SAVE / PLOT ===============================\n",
    "def save_metric_csv_all_pos(metric: str,\n",
    "                            pos_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                            layers: np.ndarray,\n",
    "                            baseline: str,\n",
    "                            subset_name: str = \"raw\"):\n",
    "    rows = []\n",
    "    for p, stats in pos_to_stats.items():\n",
    "        mu, lo, hi, n = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\"), stats.get(\"n\", np.nan)\n",
    "        for l, val in enumerate(mu):\n",
    "            rows.append({\n",
    "                \"subset\": subset_name, \"model\": baseline, \"feature\": \"pos\",\n",
    "                \"class\": p, \"metric\": metric, \"layer\": int(layers[l]),\n",
    "                \"mean\": float(val) if np.isfinite(val) else np.nan,\n",
    "                \"ci_low\": float(lo[l]) if isinstance(lo, np.ndarray) and np.isfinite(lo[l]) else np.nan,\n",
    "                \"ci_high\": float(hi[l]) if isinstance(hi, np.ndarray) and np.isfinite(hi[l]) else np.nan,\n",
    "                \"n_tokens\": int(stats.get(\"n\", 0)), \"word_rep_mode\": WORD_REP_MODE,\n",
    "                \"source_csv\": Path(CSV_PATH).name,\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    out = CSV_DIR / f\"pos_{subset_name}_{metric}_{baseline}.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "\n",
    "def plot_metric_with_ci(pos_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                        layers: np.ndarray, metric: str, title: str, out_path: Path,\n",
    "                        palette: Dict[str, Tuple[float, float, float]] | None = None):\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    for p, stats in pos_to_stats.items():\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        if mu is None or np.all(np.isnan(mu)): \n",
    "            continue\n",
    "        color = palette.get(p) if isinstance(palette, dict) else None\n",
    "        plt.plot(layers, mu, label=p, lw=1.8, color=color)\n",
    "        if isinstance(lo, np.ndarray) and isinstance(hi, np.ndarray) and not np.all(np.isnan(lo)):\n",
    "            plt.fill_between(layers, lo, hi, alpha=0.15, color=color)\n",
    "    plt.xlabel(\"Layer\"); plt.ylabel(LABELS.get(metric, metric.upper())); plt.title(title)\n",
    "\n",
    "    # Make the legend compact if there are many classes\n",
    "    n_classes = len(pos_to_stats)\n",
    "    ncol = 3 if n_classes > 12 else 2\n",
    "    plt.legend(ncol=ncol, fontsize=\"small\", title=\"POS\", frameon=False)\n",
    "\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=220); plt.close()\n",
    "\n",
    "\n",
    "\n",
    "# =============================== DRIVER ===============================\n",
    "def run_pos_pipeline():\n",
    "    # 1) Load\n",
    "    df_all, word_df = load_word_df(CSV_PATH, EXCLUDE_POS)\n",
    "    POS_TAGS = sorted(word_df.pos.unique())\n",
    "    palette = make_class_palette(POS_TAGS)\n",
    "    print(f\"✓ corpus ready — {len(word_df):,} tokens across {len(POS_TAGS)} POS\")\n",
    "    print(f\"• DADApy: {'available' if HAS_DADAPY else 'missing'}  • scikit-dimension: {'available' if HAS_SKDIM else 'missing'}\")\n",
    "\n",
    "    # 2) Raw sampling only (no frequency match)\n",
    "    raw_df = sample_raw(word_df, RAW_MAX_PER_POS)\n",
    "    print(\"Sample sizes per POS (raw cap):\")\n",
    "    print(raw_df.pos.value_counts().to_dict())\n",
    "\n",
    "    # 3) Embed once\n",
    "    reps, filled = embed_subset(df_all, raw_df, BASELINE, WORD_REP_MODE, BATCH_SIZE)\n",
    "    raw_df = raw_df.reset_index(drop=True).loc[filled].reset_index(drop=True)\n",
    "    pos_arr = raw_df.pos.values\n",
    "    L = reps.shape[0]; layers = np.arange(L)\n",
    "    print(f\"✓ embedded {len(raw_df):,} tokens  • layers={L}\")\n",
    "\n",
    "    # 4) Metric-by-metric loop (incremental outputs)\n",
    "    for metric in ALL_METRICS:\n",
    "        print(f\"\\n→ Computing metric: {metric} …\")\n",
    "\n",
    "        # Decide registry & bootstrap settings\n",
    "        if metric in FAST_ONCE:\n",
    "            compute_once = FAST_ONCE[metric]\n",
    "            n_bs = N_BOOTSTRAP_FAST\n",
    "            Mcap = FAST_BS_MAX_SAMP_PER_POS\n",
    "        else:\n",
    "            compute_once = HEAVY_ONCE.get(metric)\n",
    "            n_bs = N_BOOTSTRAP_HEAVY\n",
    "            Mcap = HEAVY_BS_MAX_SAMP_PER_POS\n",
    "\n",
    "        if compute_once is None:\n",
    "            print(f\"  (skipping {metric}: estimator unavailable)\")\n",
    "            continue\n",
    "\n",
    "        # Per-POS bootstrap\n",
    "        metric_results: Dict[str, Dict[str, np.ndarray]] = {}\n",
    "        for p in POS_TAGS:\n",
    "            idx = np.where(pos_arr == p)[0]\n",
    "            if idx.size < 3:\n",
    "                continue\n",
    "            sub = reps[:, idx]  # (L, n_p, D)\n",
    "            Np = sub.shape[1]\n",
    "            M = min(Mcap, Np)\n",
    "\n",
    "            mu, lo, hi = _bs_layer_loop(sub, M, n_bs, compute_once)\n",
    "            metric_results[p] = {\"mean\": mu, \"lo\": lo, \"hi\": hi, \"n\": int(Np)}\n",
    "\n",
    "        # Save + plot immediately for this metric\n",
    "        save_metric_csv_all_pos(metric, metric_results, layers, BASELINE, subset_name=\"raw\")\n",
    "        plot_metric_with_ci(metric_results, layers, metric,\n",
    "                    title=f\"{LABELS.get(metric, metric.upper())} • {BASELINE}\",\n",
    "                    out_path=PLOT_DIR / f\"raw_{metric}_{BASELINE}.png\",\n",
    "                    palette=palette)\n",
    "\n",
    "        print(f\"  ✓ saved: CSV= tables/pos_bootstrap/pos_raw_{metric}_{BASELINE}.csv  \"\n",
    "              f\"plot= AO_POS/raw_{metric}_{BASELINE}.png\")\n",
    "\n",
    "        # light cleanup for safety\n",
    "        del metric_results; gc.collect()\n",
    "        if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    # Cleanup\n",
    "    del reps; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    print(\"\\n✓ done (incremental outputs produced per metric).\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pos_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d295474-92fd-4d2c-a50f-8a8a0fe3251b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ corpus ready — 194,916 tokens across POS=['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_151991/4249279576.py:155: FutureWarning:\n",
      "\n",
      "`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "\n",
      "openai-community/gpt2 (embed subset): 100%|█| 5034/5034 [01:05<00:00, 77.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved interactive HTML to: pca3d_pos_classes/gpt2_pca3d_pos_classes.html\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt  # only for color palettes\n",
    "\n",
    "CSV_PATH   = \"en_ewt-ud-train_sentences.csv\"   \n",
    "\n",
    "BASELINE      = \"gpt2\"          \n",
    "WORD_REP_MODE = \"last\"                        \n",
    "\n",
    "# Plotting / sampling\n",
    "PCA_PER_CLASS_MAX_POINTS = 3000              \n",
    "\n",
    "# Throughput + device\n",
    "BATCH_SIZE = 2\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Output\n",
    "OUT_DIR = Path(\"pca3d_pos_classes\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "HTML_OUT = OUT_DIR / f\"{BASELINE.replace('/','_')}_pca3d_pos_classes.html\"\n",
    "\n",
    "# =============================== HELPERS ===============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _num_hidden_layers(model) -> int:\n",
    "    n = getattr(model.config, \"num_hidden_layers\", None)\n",
    "    if n is None: n = getattr(model.config, \"n_layer\", None)\n",
    "    if n is None: raise ValueError(\"Cannot determine number of hidden layers\")\n",
    "    return int(n)\n",
    "\n",
    "def _hidden_size(model) -> int:\n",
    "    d = getattr(model.config, \"hidden_size\", None)\n",
    "    if d is None: d = getattr(model.config, \"n_embd\", None)\n",
    "    if d is None: raise ValueError(\"Cannot determine hidden size\")\n",
    "    return int(d)\n",
    "\n",
    "def _load_tok_and_model(model_id: str):\n",
    "    \"\"\"\n",
    "    Robust loader (BERT/GPT‑2):\n",
    "      - fast tokenizer for .word_ids()\n",
    "      - right padding\n",
    "      - set PAD=EOS for GPT‑like models\n",
    "    \"\"\"\n",
    "    tried = []\n",
    "    def _try(mid: str):\n",
    "        tok = AutoTokenizer.from_pretrained(mid, use_fast=True, add_prefix_space=True)\n",
    "        if getattr(tok, \"padding_side\", None) != \"right\":\n",
    "            tok.padding_side = \"right\"\n",
    "        if tok.pad_token is None and getattr(tok, \"eos_token\", None) is not None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "        mdl = AutoModel.from_pretrained(mid, output_hidden_states=True)\n",
    "        if getattr(mdl.config, \"pad_token_id\", None) is None and tok.pad_token_id is not None:\n",
    "            mdl.config.pad_token_id = tok.pad_token_id\n",
    "        return tok, mdl\n",
    "\n",
    "    order = [model_id]\n",
    "    if model_id.lower() in {\"gpt2\", \"gpt-2\"}:\n",
    "        order += [\"openai-community/gpt2\", \"gpt2\"]  # tolerate both namespaces\n",
    "\n",
    "    last_err = None\n",
    "    for mid in order:\n",
    "        try:\n",
    "            tok, mdl = _try(mid)\n",
    "            mdl = mdl.eval().to(device)\n",
    "            if device == \"cuda\": mdl.half()\n",
    "            return tok, mdl, mid\n",
    "        except Exception as e:\n",
    "            tried.append((mid, repr(e))); last_err = e\n",
    "\n",
    "    raise RuntimeError(\"Could not load tokenizer/model. Attempts:\\n\" +\n",
    "                       \"\\n\".join(f\" - {m}: {err}\" for m, err in tried)) from last_err\n",
    "\n",
    "# =============================== DATA (POS classes) ===============================\n",
    "def load_pos_tokens(csv_path: str, exclude: set[str] | None = None):\n",
    "    \"\"\"\n",
    "    Expects per-sentence: tokens (list[str]) and pos (list[str]).\n",
    "    Returns:\n",
    "      df_sent: sentence_id, tokens\n",
    "      df_tok : per-token rows with columns [sentence_id, word_id, pos, word]\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, usecols=[\"sentence_id\",\"tokens\",\"pos\"])\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df.tokens = df.tokens.apply(_to_list)\n",
    "    df.pos    = df.pos.apply(_to_list)\n",
    "    rows = []\n",
    "    for sid, toks, poss in df[[\"sentence_id\",\"tokens\",\"pos\"]].itertuples(index=False):\n",
    "        L = min(len(toks), len(poss))\n",
    "        for wid in range(L):\n",
    "            p = str(poss[wid])\n",
    "            if exclude and p in exclude:\n",
    "                continue\n",
    "            rows.append((sid, wid, p, str(toks[wid])))\n",
    "    df_tok  = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\",\"pos\",\"word\"])\n",
    "    df_sent = df[[\"sentence_id\",\"tokens\"]].drop_duplicates(\"sentence_id\")\n",
    "    if df_tok.empty:\n",
    "        raise ValueError(\"No token rows constructed—check POS column content.\")\n",
    "    return df_sent, df_tok\n",
    "\n",
    "# =============================== EMBEDDING ===============================\n",
    "def embed_subset(df_sent: pd.DataFrame,\n",
    "                 subset_df: pd.DataFrame,\n",
    "                 baseline: str = BASELINE,\n",
    "                 word_rep_mode: str = WORD_REP_MODE,\n",
    "                 batch_size: int = BATCH_SIZE) -> Tuple[np.ndarray, np.ndarray, str]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      reps   (L, N, D)\n",
    "      filled (N,)\n",
    "      model_tag (resolved id)\n",
    "    \"\"\"\n",
    "    df_sent[\"sentence_id\"]   = df_sent[\"sentence_id\"].astype(str)\n",
    "    subset_df[\"sentence_id\"] = subset_df[\"sentence_id\"].astype(str)\n",
    "\n",
    "    # sid -> list[(global_idx, word_id)]\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(subset_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(str(sid), []).append((gidx, int(wid)))\n",
    "\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_sent[df_sent.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    tokzr, model, model_id = _load_tok_and_model(baseline)\n",
    "\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "        enc_kwargs[\"add_prefix_space\"] = True\n",
    "\n",
    "    L = _num_hidden_layers(model) + 1\n",
    "    D = _hidden_size(model)\n",
    "    N = len(subset_df)\n",
    "\n",
    "    reps   = np.zeros((L, N, D), np.float16)\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{model_id} (embed subset)\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L,B,T,D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                # map word_id -> token positions\n",
    "                mp: Dict[int, List[int]] = {}\n",
    "                wids = enc_be.word_ids(b)\n",
    "                if wids is None:\n",
    "                    raise RuntimeError(\"Fast tokenizer required (word_ids() unavailable).\")\n",
    "                for tidx, wid in enumerate(wids):\n",
    "                    if wid is not None:\n",
    "                        mp.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = mp.get(wid)\n",
    "                    if not toks: continue\n",
    "                    if word_rep_mode == \"first\":\n",
    "                        vec = h[:, b, toks[0], :]\n",
    "                    elif word_rep_mode == \"last\":\n",
    "                        vec = h[:, b, toks[-1], :]\n",
    "                    elif word_rep_mode == \"mean\":\n",
    "                        vec = h[:, b, toks, :].mean(axis=1)\n",
    "                    else:\n",
    "                        raise ValueError(\"WORD_REP_MODE must be {'first','last','mean'}\")\n",
    "                    reps[:, gidx, :] = vec.astype(np.float16, copy=False)\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    missing = int((~filled).sum())\n",
    "    if missing:\n",
    "        print(f\"⚠ Missing vectors for {missing} of {N} tokens\")\n",
    "    del model; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    return reps, filled, model_id\n",
    "\n",
    "# =============================== COLORS ===============================\n",
    "def _class_palette(classes: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Distinct hex colors for many POS tags: use tab20+tab20b+tab20c (~60 colors), then hsv fallback.\n",
    "    \"\"\"\n",
    "    colors = []\n",
    "    for name in (\"tab20\", \"tab20b\", \"tab20c\"):\n",
    "        try:\n",
    "            colors.extend(plt.get_cmap(name).colors)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if len(colors) < len(classes):\n",
    "        hsv = plt.get_cmap(\"hsv\")\n",
    "        colors = [hsv(i/len(classes)) for i in range(len(classes))]\n",
    "    to_hex = lambda rgb: \"#{:02x}{:02x}{:02x}\".format(int(rgb[0]*255), int(rgb[1]*255), int(rgb[2]*255))\n",
    "    ordered = list(sorted(classes))  # alphabetical for stability\n",
    "    return {cls: to_hex(colors[i % len(colors)]) for i, cls in enumerate(ordered)}\n",
    "\n",
    "# =============================== PCA + PLOTLY BY CLASS ===============================\n",
    "def pca3d_layers_by_class(reps: np.ndarray,\n",
    "                          words: List[str],\n",
    "                          class_arr: np.ndarray,\n",
    "                          classes: List[str],\n",
    "                          model_tag: str,\n",
    "                          html_out: Path):\n",
    "    \"\"\"\n",
    "    reps: (L, N, D)\n",
    "    words: list[str] length N (hover text)\n",
    "    class_arr: array length N with POS labels (strings)\n",
    "    classes: list of unique labels\n",
    "    \"\"\"\n",
    "    L, N, D = reps.shape\n",
    "    classes_sorted = list(sorted(classes))\n",
    "    palette = _class_palette(classes_sorted)\n",
    "\n",
    "    # Build consistent subset across layers: sample up to cap per class\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    sel_idx: List[int] = []\n",
    "    for c in classes_sorted:\n",
    "        idx_c = np.where(class_arr == c)[0]\n",
    "        if PCA_PER_CLASS_MAX_POINTS is not None and len(idx_c) > PCA_PER_CLASS_MAX_POINTS:\n",
    "            idx_c = rng.choice(idx_c, size=PCA_PER_CLASS_MAX_POINTS, replace=False)\n",
    "        sel_idx.extend(idx_c.tolist())\n",
    "    sel_idx = np.array(sel_idx, dtype=np.int64)\n",
    "    if sel_idx.size == 0:\n",
    "        raise ValueError(\"No points selected for PCA plotting (check POS tags and caps).\")\n",
    "\n",
    "    # Per-class local positions within sel_idx\n",
    "    cls_pos: Dict[str, np.ndarray] = {c: np.where(class_arr[sel_idx] == c)[0] for c in classes_sorted}\n",
    "\n",
    "    reps_sel = reps[:, sel_idx, :].astype(np.float32, copy=False)\n",
    "    words_sel = [words[i] for i in sel_idx]\n",
    "\n",
    "    # PCA per layer\n",
    "    Y_layers: List[np.ndarray] = []\n",
    "    for l in range(L):\n",
    "        X = reps_sel[l]  # (n_sel, D)\n",
    "        Xc = X - X.mean(0, keepdims=True)\n",
    "        pca = PCA(n_components=3, random_state=RAND_SEED)\n",
    "        Y = pca.fit_transform(Xc)  # (n_sel, 3)\n",
    "        Y_layers.append(Y)\n",
    "\n",
    "    # Build traces: (layer, class) grid\n",
    "    traces = []\n",
    "    for l in range(L):\n",
    "        Y = Y_layers[l]\n",
    "        for c in classes_sorted:\n",
    "            pos = cls_pos[c]\n",
    "            if pos.size == 0:\n",
    "                continue\n",
    "            # customdata for stable hover: [class, layer]\n",
    "            custom = np.column_stack([np.full(pos.size, c, dtype=object),\n",
    "                                      np.full(pos.size, l, dtype=int)])\n",
    "            traces.append(\n",
    "                go.Scatter3d(\n",
    "                    x=Y[pos, 0], y=Y[pos, 1], z=Y[pos, 2],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=2, opacity=0.75, color=palette[c]),\n",
    "                    text=[words_sel[i] for i in pos],\n",
    "                    customdata=custom,\n",
    "                    hovertemplate=(\n",
    "                        \"<b>%{text}</b>\"\n",
    "                        \"<br>POS=%{customdata[0]} • layer=%{customdata[1]}\"\n",
    "                        \"<br>x=%{x:.3f}<br>y=%{y:.3f}<br>z=%{z:.3f}\"\n",
    "                        \"<extra></extra>\"\n",
    "                    ),\n",
    "                    name=f\"{c}\",\n",
    "                    visible=(l == 0),\n",
    "                    showlegend=(l == 0)  # legend only once\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Slider toggles visibility of all class traces for a given layer\n",
    "    traces_per_layer = len(traces) // L if L > 0 else 0\n",
    "    steps = []\n",
    "    for l in range(L):\n",
    "        vis = [False]*len(traces)\n",
    "        start = l * traces_per_layer\n",
    "        for k in range(traces_per_layer):\n",
    "            if start + k < len(traces):\n",
    "                vis[start + k] = True\n",
    "        steps.append(dict(\n",
    "            method=\"update\",\n",
    "            args=[{\"visible\": vis},\n",
    "                  {\"title\": f\"{model_tag} • PCA 3D by POS • layer {l} (drag to rotate)\"}],\n",
    "            label=str(l),\n",
    "        ))\n",
    "\n",
    "    sliders = [dict(\n",
    "        active=0,\n",
    "        steps=steps,\n",
    "        currentvalue={\"prefix\": \"Layer: \"},\n",
    "        pad={\"t\": 10}\n",
    "    )]\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title=f\"{model_tag} • PCA 3D by POS • layer 0 (drag to rotate)\",\n",
    "        scene=dict(xaxis_title=\"PC1\", yaxis_title=\"PC2\", zaxis_title=\"PC3\", aspectmode=\"data\"),\n",
    "        margin=dict(l=0, r=0, b=0, t=40),\n",
    "        sliders=sliders,\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "    fig.write_html(str(html_out), include_plotlyjs=\"cdn\")\n",
    "    print(\"✓ Saved interactive HTML to:\", html_out)\n",
    "\n",
    "# =============================== DRIVER ===============================\n",
    "def run_pca3d_pos_classes():\n",
    "    # 1) Load POS tokens\n",
    "    #    (Optionally exclude certain POS by passing exclude={\"X\",\"SYM\",\"PART\",\"INTJ\"} etc.)\n",
    "    df_sent, df_tok = load_pos_tokens(CSV_PATH, exclude=None)\n",
    "    pos_tags = sorted(df_tok.pos.unique().tolist())\n",
    "    print(f\"✓ corpus ready — {len(df_tok):,} tokens across POS={pos_tags}\")\n",
    "\n",
    "    # 2) (Optional) cap per POS for speed happens inside the PCA function; here we keep all\n",
    "    subset_df = df_tok[[\"sentence_id\",\"word_id\",\"pos\",\"word\"]].copy()\n",
    "\n",
    "    # 3) Embed once\n",
    "    reps, filled, resolved_model = embed_subset(df_sent, subset_df, BASELINE, WORD_REP_MODE, BATCH_SIZE)\n",
    "    subset_df = subset_df.reset_index(drop=True).loc[filled].reset_index(drop=True)\n",
    "\n",
    "    # 4) Collect hover text + labels\n",
    "    words = subset_df[\"word\"].astype(str).tolist()\n",
    "    cls_arr = subset_df[\"pos\"].astype(str).values\n",
    "    classes = sorted(subset_df[\"pos\"].unique().tolist())\n",
    "\n",
    "    # 5) PCA+Plotly\n",
    "    pca3d_layers_by_class(reps, words, cls_arr, classes, model_tag=resolved_model, html_out=HTML_OUT)\n",
    "\n",
    "    # Cleanup\n",
    "    del reps; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pca3d_pos_classes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427b03cf-3263-427b-8110-b678ac13846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# ============== Optional deps (gracefully skipped if not installed) ==============\n",
    "HAS_DADAPY = False\n",
    "try:\n",
    "    from dadapy import Data  # DADApy ID estimators (TwoNN, GRIDE)\n",
    "    HAS_DADAPY = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "HAS_SKDIM = False\n",
    "try:\n",
    "    from skdim.id import (\n",
    "        MOM, TLE, CorrInt, FisherS, lPCA,\n",
    "        MLE, DANCo, ESS, MiND_ML, MADA, KNN\n",
    "    )\n",
    "    HAS_SKDIM = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# IsoScore: use library if available, else a simple monotone fallback\n",
    "try:\n",
    "    from isoscore import IsoScore\n",
    "    _HAS_ISOSCORE = True\n",
    "except Exception:\n",
    "    _HAS_ISOSCORE = False\n",
    "    class _IsoScoreFallback:\n",
    "        @staticmethod\n",
    "        def IsoScore(X: np.ndarray) -> float:\n",
    "            C = np.cov(X.T, ddof=0)\n",
    "            ev = np.linalg.eigvalsh(C)\n",
    "            if ev.mean() <= 0 or ev[-1] <= 0:\n",
    "                return 0.0\n",
    "            # mean/peak eigenvalue ratio in [0,1]; higher ≈ more isotropic\n",
    "            return float(np.clip(ev.mean() / (ev[-1] + 1e-12), 0.0, 1.0))\n",
    "    IsoScore = _IsoScoreFallback()\n",
    "\n",
    "# =============================== CONFIG ===============================\n",
    "CSV_PATH        = \"en_ewt-ud-train_sentences.csv\"\n",
    "BASELINE        = \"gpt2\"                 # ← GPT‑2\n",
    "WORD_REP_MODE   = \"last\"                 # ← {\"last\",\"mean\"} word representation\n",
    "EXCLUDE_POS     = {\"X\", \"SYM\", \"PART\", \"INTJ\"}\n",
    "\n",
    "# Sampling cap per POS for plotting (increase if you want)\n",
    "\n",
    "RAW_MAX_PER_POS = int(1e12)         # effectively no cap\n",
    "\n",
    "# Bootstrap replicates\n",
    "N_BOOTSTRAP_FAST   = 50         # good CIs without going overboard\n",
    "N_BOOTSTRAP_HEAVY  = 200            # keep heavy bootstrap moderate\n",
    "\n",
    "# Per-replicate sample size M (min(cap, N_pos))\n",
    "FAST_BS_MAX_SAMP_PER_POS  = int(1e12)   # => M = N_pos (the classic bootstrap uses M=N)\n",
    "HEAVY_BS_MAX_SAMP_PER_POS = 5000        # 1000–5000 is a practical range for TwoNN/GRIDE/skdim\n",
    "\n",
    "DADAPY_GRID_RANGE_MAX     = 64\n",
    "\n",
    "# Runtime / output\n",
    "BATCH_SIZE   = 8\n",
    "RAND_SEED    = 42\n",
    "PLOT_DIR     = Path(\"AO_POS\"); PLOT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CSV_DIR      = Path(\"tables\") / \"pos_bootstrap\"; CSV_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Isotropy extras\n",
    "PFI_DIRS = 256\n",
    "PFI_Q_LO = 5.0\n",
    "PFI_Q_HI = 95.0\n",
    "EPS      = 1e-12\n",
    "\n",
    "# Repro & device\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Seaborn style\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "# =============================== HELPERS ===============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _center(X: np.ndarray) -> np.ndarray:\n",
    "    return X - X.mean(0, keepdims=True)\n",
    "\n",
    "def _eigvals_from_X(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Eigenvalues of covariance up to a constant via SVD of centered X (descending).\"\"\"\n",
    "    Xc = _center(X.astype(np.float32, copy=False))\n",
    "    try:\n",
    "        _, S, _ = np.linalg.svd(Xc, full_matrices=False)\n",
    "        lam = (S**2).astype(np.float64)\n",
    "        lam.sort()\n",
    "        return lam[::-1]\n",
    "    except Exception:\n",
    "        return np.array([], dtype=np.float64)\n",
    "\n",
    "def _jitter_unique(X: np.ndarray, eps: float = 1e-7) -> np.ndarray:\n",
    "    \"\"\"Add tiny noise if there are duplicate rows (helps NN-based estimators).\"\"\"\n",
    "    try:\n",
    "        if np.unique(X, axis=0).shape[0] < X.shape[0]:\n",
    "            X = X + np.random.normal(scale=eps, size=X.shape).astype(X.dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return X\n",
    "\n",
    "# ========= Single-shot compute functions used inside bootstrap =========\n",
    "# --- Isotropy (fast) ---\n",
    "def _iso_once(X: np.ndarray) -> float:\n",
    "    return float(IsoScore.IsoScore(X))\n",
    "\n",
    "def _spect_once(X: np.ndarray) -> float:\n",
    "    ev = np.linalg.eigvalsh(np.cov(X.T, ddof=0))\n",
    "    return float(ev[-1] / (ev.mean() + 1e-9))\n",
    "\n",
    "def _rand_once(X: np.ndarray, K: int = 2000) -> float:\n",
    "    n = X.shape[0]\n",
    "    if n < 2: return np.nan\n",
    "    rng = np.random.default_rng()\n",
    "    K_eff = min(K, (n*(n-1))//2)\n",
    "    i = rng.integers(0, n, size=K_eff)\n",
    "    j = rng.integers(0, n, size=K_eff)\n",
    "    same = i == j\n",
    "    if same.any():\n",
    "        j[same] = rng.integers(0, n, size=same.sum())\n",
    "    A, B = X[i], X[j]\n",
    "    num = np.sum(A*B, axis=1)\n",
    "    den = (np.linalg.norm(A, axis=1)*np.linalg.norm(B, axis=1) + 1e-9)\n",
    "    return float(np.mean(np.abs(num/den)))\n",
    "\n",
    "def _sf_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    gm = np.exp(np.mean(np.log(lam + EPS)))\n",
    "    am = float(lam.mean() + EPS)\n",
    "    return float(gm / am)\n",
    "\n",
    "def _pfI_once(X: np.ndarray) -> float:\n",
    "    n, d = X.shape\n",
    "    if n < 2: return np.nan\n",
    "    rng = np.random.default_rng()\n",
    "    U = rng.standard_normal((PFI_DIRS, d)).astype(np.float32)\n",
    "    U /= np.linalg.norm(U, axis=1, keepdims=True) + 1e-9\n",
    "    S = U @ X.T\n",
    "    m = np.max(S, axis=1, keepdims=True)\n",
    "    logZ = (m + np.log(np.sum(np.exp(S - m), axis=1, keepdims=True))).ravel()\n",
    "    lo = np.percentile(logZ, PFI_Q_LO)\n",
    "    hi = np.percentile(logZ, PFI_Q_HI)\n",
    "    return float(np.exp(lo - hi))  # ≈ min Z / max Z (robust)\n",
    "\n",
    "def _vmf_kappa_once(X: np.ndarray) -> float:\n",
    "    if X.shape[0] < 2: return np.nan\n",
    "    Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
    "    R = np.linalg.norm(Xn.mean(axis=0))\n",
    "    d = Xn.shape[1]\n",
    "    if R < 1e-9: return 0.0\n",
    "    return float(max(R * (d - R**2) / (1.0 - R**2 + 1e-9), 0.0))\n",
    "\n",
    "# --- Linear ID (fast) ---\n",
    "def _pcaXX_once(X: np.ndarray, var_ratio: float) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    c = np.cumsum(lam); thr = c[-1] * var_ratio\n",
    "    return float(np.searchsorted(c, thr) + 1)\n",
    "\n",
    "def _pca95_once(X: np.ndarray) -> float:\n",
    "    return _pcaXX_once(X, 0.95)\n",
    "\n",
    "def _pca99_once(X: np.ndarray) -> float:\n",
    "    return _pcaXX_once(X, 0.99)\n",
    "\n",
    "def _erank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    p = lam / (lam.sum() + EPS)\n",
    "    H = -(p * np.log(p + EPS)).sum()\n",
    "    return float(np.exp(H))\n",
    "\n",
    "def _pr_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    s1 = lam.sum(); s2 = (lam**2).sum()\n",
    "    return float((s1**2) / (s2 + EPS))\n",
    "\n",
    "def _stable_rank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    return float(lam.sum() / (lam.max() + EPS))\n",
    "\n",
    "# --- Non-linear (heavy) ---\n",
    "def _dadapy_twonn_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    id_est, _, _ = d.compute_id_2NN()\n",
    "    return float(id_est)\n",
    "\n",
    "def _dadapy_gride_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    d.compute_distances(maxk=DADAPY_GRID_RANGE_MAX)\n",
    "    ids, _, _ = d.return_id_scaling_gride(range_max=DADAPY_GRID_RANGE_MAX)\n",
    "    return float(ids[-1])\n",
    "\n",
    "def _skdim_factory(name: str):\n",
    "    \"\"\"Return a factory that builds a fresh skdim estimator each call, or None.\"\"\"\n",
    "    if not HAS_SKDIM: return None\n",
    "    mapping = {\n",
    "        \"mom\": MOM, \"tle\": TLE, \"corrint\": CorrInt, \"fishers\": FisherS,\n",
    "        \"lpca\": lPCA, \"lpca99\": lPCA,\n",
    "        \"mle\": MLE, \"danco\": DANCo, \"mind_ml\": MiND_ML,\n",
    "        \"mada\": MADA, \"knn\": KNN, \"ess\": ESS,\n",
    "    }\n",
    "    cls = mapping.get(name)\n",
    "    if cls is None: return None\n",
    "\n",
    "    def _builder():\n",
    "        if name == \"lpca\":      # FO variant\n",
    "            return cls(ver=\"FO\")\n",
    "        elif name == \"lpca99\":  # ratio (0.99) variant\n",
    "            return cls(ver=\"ratio\", alphaRatio=0.99)\n",
    "        else:\n",
    "            return cls()\n",
    "    return _builder\n",
    "\n",
    "def _skdim_once_builder(name: str) -> Callable[[np.ndarray], float] | None:\n",
    "    build = _skdim_factory(name)\n",
    "    if build is None: return None\n",
    "\n",
    "    def _once(X: np.ndarray) -> float:\n",
    "        est = build()\n",
    "        est.fit(_jitter_unique(X))\n",
    "        return float(getattr(est, \"dimension_\", np.nan))\n",
    "    return _once\n",
    "\n",
    "# =============================== DATA ===============================\n",
    "def load_word_df(csv_path: str, exclude_pos: set[str] = EXCLUDE_POS):\n",
    "    df = pd.read_csv(csv_path, usecols=[\"sentence_id\",\"tokens\",\"pos\"])\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df.tokens = df.tokens.apply(_to_list); df.pos = df.pos.apply(_to_list)\n",
    "\n",
    "    rows = []\n",
    "    for sid, toks, poss in df[[\"sentence_id\",\"tokens\",\"pos\"]].itertuples(index=False):\n",
    "        for wid, (tok, p) in enumerate(zip(toks, poss)):\n",
    "            if p not in exclude_pos:\n",
    "                rows.append((sid, wid, p, tok))\n",
    "    word_df = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\",\"pos\",\"word\"])\n",
    "    return df, word_df\n",
    "\n",
    "def sample_raw(word_df: pd.DataFrame, per_pos_cap: int = RAW_MAX_PER_POS) -> pd.DataFrame:\n",
    "    \"\"\"Per-POS cap without frequency matching.\"\"\"\n",
    "    picks = []\n",
    "    for p, sub in word_df.groupby(\"pos\", sort=False):\n",
    "        n = min(len(sub), per_pos_cap)\n",
    "        picks.append(sub.sample(n, random_state=RAND_SEED, replace=False))\n",
    "    return pd.concat(picks, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================== EMBEDDING (GPT‑2) ===============================\n",
    "def _num_hidden_layers(model) -> int:\n",
    "    \"\"\"Works for GPT‑2 (n_layer) and BERT-like (num_hidden_layers).\"\"\"\n",
    "    n = getattr(model.config, \"num_hidden_layers\", None)\n",
    "    if n is None:\n",
    "        n = getattr(model.config, \"n_layer\", None)\n",
    "    if n is None:\n",
    "        raise ValueError(\"Cannot determine number of hidden layers from model.config\")\n",
    "    return int(n)\n",
    "\n",
    "def _hidden_size(model) -> int:\n",
    "    d = getattr(model.config, \"hidden_size\", None)\n",
    "    if d is None:\n",
    "        d = getattr(model.config, \"n_embd\", None)  # GPT‑2\n",
    "    if d is None:\n",
    "        raise ValueError(\"Cannot determine hidden size from model.config\")\n",
    "    return int(d)\n",
    "\n",
    "def embed_subset(df_all_sentences: pd.DataFrame,\n",
    "                 subset_df: pd.DataFrame,\n",
    "                 baseline: str = BASELINE,\n",
    "                 word_rep_mode: str = WORD_REP_MODE,\n",
    "                 batch_size: int = BATCH_SIZE) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Return reps (L,N,D) and filled mask (N,) for the selected tokens.\n",
    "    Uses word_ids() to map subword tokens back to original word indices.\n",
    "    For GPT‑2, we (1) set add_prefix_space=True and (2) map pad_token→eos_token.\n",
    "    \"\"\"\n",
    "    df_all_sentences[\"sentence_id\"] = df_all_sentences[\"sentence_id\"].astype(str)\n",
    "    subset_df[\"sentence_id\"] = subset_df[\"sentence_id\"].astype(str)\n",
    "\n",
    "    # sid -> list[(global_idx, word_id)]\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(subset_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(str(sid), []).append((gidx, int(wid)))\n",
    "\n",
    "    # Materialize in batch order\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_all_sentences[df_all_sentences.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    # GPT‑2 fast tokenizer with prefix space (needed with is_split_into_words)\n",
    "    tokzr = AutoTokenizer.from_pretrained(baseline, use_fast=True, add_prefix_space=True)\n",
    "\n",
    "    # Ensure padding works: GPT‑2 has no pad token by default\n",
    "    if tokzr.pad_token is None:\n",
    "        tokzr.pad_token = tokzr.eos_token\n",
    "\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
    "    # (Fast tokenizers support `word_ids`; we already set add_prefix_space=True above)\n",
    "\n",
    "    # Model\n",
    "    model = AutoModel.from_pretrained(baseline, output_hidden_states=True).eval().to(device)\n",
    "    # Make sure model's pad id is set (decoder-only models will ignore attention on padding anyway)\n",
    "    if getattr(model.config, \"pad_token_id\", None) is None and tokzr.pad_token_id is not None:\n",
    "        model.config.pad_token_id = tokzr.pad_token_id\n",
    "    if device == \"cuda\":\n",
    "        model.half()\n",
    "\n",
    "    L = _num_hidden_layers(model) + 1   # include embedding layer\n",
    "    D = _hidden_size(model)\n",
    "    N = len(subset_df)\n",
    "\n",
    "    reps   = np.zeros((L, N, D), np.float32)   # keep float32 to avoid duplicate-row issues in KNN metrics\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{baseline} (embed subset)\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)            # fast tokenizer\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            # hidden_states: tuple len=L of (B,T,D)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L,B,T,D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                # word_id -> token positions for this item\n",
    "                mp: Dict[int, List[int]] = {}\n",
    "                # word_ids(b) works only with fast tokenizers\n",
    "                for tidx, wid in enumerate(enc_be.word_ids(b)):\n",
    "                    if wid is not None:\n",
    "                        mp.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = mp.get(wid)\n",
    "                    if not toks:\n",
    "                        continue\n",
    "                    if word_rep_mode == \"last\":\n",
    "                        vec = h[:, b, toks[-1], :]          # last subtoken\n",
    "                    elif word_rep_mode == \"mean\":\n",
    "                        vec = h[:, b, toks, :].mean(axis=1) # mean over subtokens\n",
    "                    else:  # fallback to last\n",
    "                        vec = h[:, b, toks[-1], :]\n",
    "                    reps[:, gidx, :] = vec.astype(np.float32, copy=False)\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            # free batch buffers\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    missing = int((~filled).sum())\n",
    "    if missing:\n",
    "        print(f\"⚠ Missing vectors for {missing} of {N} sampled words\")\n",
    "    del model; gc.collect()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    return reps, filled\n",
    "\n",
    "# =============================== BOOTSTRAP CORE ===============================\n",
    "def _bs_layer_loop(rep_sub: np.ndarray, M: int, n_reps: int, compute_once: Callable[[np.ndarray], float]):\n",
    "    \"\"\"Bootstrap: sample M with replacement and apply compute_once(X_layer) -> scalar for each layer.\"\"\"\n",
    "    L, N, D = rep_sub.shape\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    A = np.full((n_reps, L), np.nan, np.float32)\n",
    "    for r in range(n_reps):\n",
    "        idx = rng.integers(0, N, size=M)\n",
    "        for l in range(L):\n",
    "            X = rep_sub[l, idx].astype(np.float32, copy=False)\n",
    "            try:\n",
    "                A[r, l] = float(compute_once(X))\n",
    "            except Exception:\n",
    "                A[r, l] = np.nan\n",
    "    mu = np.nanmean(A, axis=0).astype(np.float32)\n",
    "    lo = np.nanpercentile(A, 2.5, axis=0).astype(np.float32)\n",
    "    hi = np.nanpercentile(A, 97.5, axis=0).astype(np.float32)\n",
    "    return mu, lo, hi\n",
    "\n",
    "# fast metric registry\n",
    "FAST_ONCE: Dict[str, Callable[[np.ndarray], float]] = {\n",
    "    \"iso\": _iso_once,\n",
    "    \"spect\": _spect_once,\n",
    "    \"rand\": _rand_once,\n",
    "    \"sf\": _sf_once,\n",
    "    \"pfI\": _pfI_once,\n",
    "    \"vmf_kappa\": _vmf_kappa_once,\n",
    "    \"erank\": _erank_once,\n",
    "    \"pr\": _pr_once,\n",
    "    \"stable_rank\": _stable_rank_once,\n",
    "    \"pca95\": _pca95_once,\n",
    "    \"pca99\": _pca99_once,\n",
    "}\n",
    "\n",
    "# heavy metric registry\n",
    "HEAVY_ONCE: Dict[str, Callable[[np.ndarray], float] | None] = {\n",
    "    \"twonn\": _dadapy_twonn_once,\n",
    "    \"gride\": _dadapy_gride_once,\n",
    "    \"mom\":   _skdim_once_builder(\"mom\"),\n",
    "    \"tle\":   _skdim_once_builder(\"tle\"),\n",
    "    \"corrint\": _skdim_once_builder(\"corrint\"),\n",
    "    \"fishers\": _skdim_once_builder(\"fishers\"),\n",
    "    \"lpca\":  _skdim_once_builder(\"lpca\"),\n",
    "    \"lpca95\": _skdim_once_builder(\"lpca99\"),\n",
    "    \"lpca99\": _skdim_once_builder(\"lpca99\"),\n",
    "    \"mle\":   _skdim_once_builder(\"mle\"),\n",
    "    \"danco\": _skdim_once_builder(\"danco\"),\n",
    "    \"ess\":   _skdim_once_builder(\"ess\"),\n",
    "    \"mada\":  _skdim_once_builder(\"mada\"),\n",
    "    \"knn\":   _skdim_once_builder(\"knn\"),\n",
    "}\n",
    "\n",
    "LABELS = {\n",
    "    # Isotropy\n",
    "    \"iso\":\"IsoScore\",\"sf\":\"Spectral Flatness\",\"pfI\":\"Partition Isotropy I\",\n",
    "    \"vmf_kappa\":\"vMF κ\",\"spect\":\"Spectral Ratio\",\"rand\":\"RandCos |μ|\",\n",
    "    # Linear ID\n",
    "    \"erank\":\"Effective Rank\",\"pr\":\"Participation Ratio\",\"stable_rank\":\"Stable Rank\",\n",
    "    # Non-linear\n",
    "    \"twonn\":\"TwoNN ID\",\"gride\":\"GRIDE\",\n",
    "    \"mom\":\"MOM\",\"tle\":\"TLE\",\"corrint\":\"CorrInt\",\"fishers\":\"FisherS\",\n",
    "    \"lpca\":\"lPCA FO\",\"lpca99\":\"lPCA 0.99\",\"lpca95\":\"lPCA 0.95\", \"mle\":\"MLE\",\"danco\":\"DANCo\",\n",
    "    \"ess\":\"ESS\",\"mada\":\"MADA\",\"knn\":\"KNN\",\n",
    "}\n",
    "\n",
    "# Choose plotting order\n",
    "PLOT_ORDER = (\n",
    "    \"iso\",\"sf\",\"pfI\",\"vmf_kappa\",\"spect\",\"rand\",\n",
    "    \"erank\",\"pr\",\"stable_rank\",\"pca95\",\"pca99\",\n",
    "    \"twonn\",\"gride\",\"mom\",\"tle\",\"corrint\",\"fishers\",\"lpca\",\"lpca99\",\n",
    "    \"mle\",\"danco\",\"ess\",\"mada\",\"knn\"\n",
    ")\n",
    "#ALL_METRICS = list(PLOT_ORDER)\n",
    "ALL_METRICS =[\"gride\"]\n",
    "\n",
    "# =============================== SAVE / PLOT ===============================\n",
    "def save_metric_csv_all_pos(metric: str,\n",
    "                            pos_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                            layers: np.ndarray,\n",
    "                            baseline: str,\n",
    "                            subset_name: str = \"raw\"):\n",
    "    rows = []\n",
    "    for p, stats in pos_to_stats.items():\n",
    "        mu, lo, hi, n = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\"), stats.get(\"n\", np.nan)\n",
    "        for l, val in enumerate(mu):\n",
    "            rows.append({\n",
    "                \"subset\": subset_name, \"model\": baseline, \"feature\": \"pos\",\n",
    "                \"class\": p, \"metric\": metric, \"layer\": int(layers[l]),\n",
    "                \"mean\": float(val) if np.isfinite(val) else np.nan,\n",
    "                \"ci_low\": float(lo[l]) if isinstance(lo, np.ndarray) and np.isfinite(lo[l]) else np.nan,\n",
    "                \"ci_high\": float(hi[l]) if isinstance(hi, np.ndarray) and np.isfinite(hi[l]) else np.nan,\n",
    "                \"n_tokens\": int(stats.get(\"n\", 0)), \"word_rep_mode\": WORD_REP_MODE,\n",
    "                \"source_csv\": Path(CSV_PATH).name,\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    out = CSV_DIR / f\"pos_{subset_name}_{metric}_{baseline}.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "\n",
    "def plot_metric_with_ci(pos_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                        layers: np.ndarray, metric: str, title: str, out_path: Path):\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    for p, stats in pos_to_stats.items():\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        if mu is None or np.all(np.isnan(mu)): continue\n",
    "        plt.plot(layers, mu, label=p, lw=1.8)\n",
    "        if isinstance(lo, np.ndarray) and isinstance(hi, np.ndarray) and not np.all(np.isnan(lo)):\n",
    "            plt.fill_between(layers, lo, hi, alpha=0.15)\n",
    "    plt.xlabel(\"Layer\"); plt.ylabel(LABELS.get(metric, metric.upper())); plt.title(title)\n",
    "    plt.legend(ncol=2, fontsize=\"small\", title=\"POS\", frameon=False)\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=220); plt.close()\n",
    "\n",
    "# =============================== DRIVER ===============================\n",
    "def run_pos_pipeline():\n",
    "    # 1) Load\n",
    "    df_all, word_df = load_word_df(CSV_PATH, EXCLUDE_POS)\n",
    "    POS_TAGS = sorted(word_df.pos.unique())\n",
    "    print(f\"✓ corpus ready — {len(word_df):,} tokens across {len(POS_TAGS)} POS\")\n",
    "    print(f\"• DADApy: {'available' if HAS_DADAPY else 'missing'}  • scikit-dimension: {'available' if HAS_SKDIM else 'missing'}\")\n",
    "\n",
    "    # 2) Raw sampling only (no frequency match)\n",
    "    raw_df = sample_raw(word_df, RAW_MAX_PER_POS)\n",
    "    print(\"Sample sizes per POS (raw cap):\")\n",
    "    print(raw_df.pos.value_counts().to_dict())\n",
    "\n",
    "    # 3) Embed once (GPT‑2, last/mean subtoken per word)\n",
    "    reps, filled = embed_subset(df_all, raw_df, BASELINE, WORD_REP_MODE, BATCH_SIZE)\n",
    "    raw_df = raw_df.reset_index(drop=True).loc[filled].reset_index(drop=True)\n",
    "    pos_arr = raw_df.pos.values\n",
    "    L = reps.shape[0]; layers = np.arange(L)\n",
    "    print(f\"✓ embedded {len(raw_df):,} tokens  • layers={L}\")\n",
    "\n",
    "    # 4) Metric-by-metric loop (incremental outputs)\n",
    "    for metric in ALL_METRICS:\n",
    "        print(f\"\\n→ Computing metric: {metric} …\")\n",
    "\n",
    "        # Decide registry & bootstrap settings\n",
    "        if metric in FAST_ONCE:\n",
    "            compute_once = FAST_ONCE[metric]\n",
    "            n_bs = N_BOOTSTRAP_FAST\n",
    "            Mcap = FAST_BS_MAX_SAMP_PER_POS\n",
    "        else:\n",
    "            compute_once = HEAVY_ONCE.get(metric)\n",
    "            n_bs = N_BOOTSTRAP_HEAVY\n",
    "            Mcap = HEAVY_BS_MAX_SAMP_PER_POS\n",
    "\n",
    "        if compute_once is None:\n",
    "            print(f\"  (skipping {metric}: estimator unavailable)\")\n",
    "            continue\n",
    "\n",
    "        # Per-POS bootstrap\n",
    "        metric_results: Dict[str, Dict[str, np.ndarray]] = {}\n",
    "        for p in POS_TAGS:\n",
    "            idx = np.where(pos_arr == p)[0]\n",
    "            if idx.size < 3:\n",
    "                continue\n",
    "            sub = reps[:, idx]  # (L, n_p, D)\n",
    "            Np = sub.shape[1]\n",
    "            M = min(Mcap, Np)\n",
    "\n",
    "            mu, lo, hi = _bs_layer_loop(sub, M, n_bs, compute_once)\n",
    "            metric_results[p] = {\"mean\": mu, \"lo\": lo, \"hi\": hi, \"n\": int(Np)}\n",
    "\n",
    "        # Save + plot immediately for this metric\n",
    "        save_metric_csv_all_pos(metric, metric_results, layers, BASELINE, subset_name=\"raw\")\n",
    "        plot_metric_with_ci(metric_results, layers, metric,\n",
    "                    title=f\"{LABELS.get(metric, metric.upper())} • {BASELINE}\",\n",
    "                    out_path=PLOT_DIR / f\"raw_{metric}_{BASELINE}.png\")\n",
    "        print(f\"  ✓ saved: CSV= tables/pos_bootstrap/pos_raw_{metric}_{BASELINE}.csv  \"\n",
    "              f\"plot= AO_POS/raw_{metric}_{BASELINE}_{WORD_REP_MODE}.png\")\n",
    "\n",
    "        # light cleanup\n",
    "        del metric_results; gc.collect()\n",
    "        if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    # Cleanup\n",
    "    del reps; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    print(\"\\n✓ done (incremental outputs produced per metric).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pos_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bcd0d2-9981-40d7-999a-85c00c94b667",
   "metadata": {},
   "source": [
    "## No index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70714bf0-9dab-4b30-95e2-41e493eaeaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ corpus ready — 179,495 tokens across 13 POS\n",
      "• DADApy: available  • scikit-dimension: available\n",
      "Sample sizes per POS (raw cap):\n",
      "{'NOUN': 33143, 'PUNCT': 21787, 'VERB': 21495, 'ADP': 17040, 'DET': 14938, 'PRON': 14909, 'ADJ': 12350, 'AUX': 11207, 'PROPN': 10478, 'ADV': 9061, 'CCONJ': 6354, 'SCONJ': 3370, 'NUM': 3363}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openai-community/gpt2 (embed subset): 100%|█| 1258/1258 [00:22<00:00, 55.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ embedded 179,495 tokens  • layers=13\n",
      "\n",
      "→ Computing metric: gride …\n",
      "  ✓ saved: CSV= tables/pos_bootstrap/pos_raw_gride_gpt2.csv  plot= AO_POS/raw_gride_gpt2_last.png\n",
      "\n",
      "✓ done (incremental outputs produced per metric).\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel, GPT2TokenizerFast\n",
    "\n",
    "# ============== Optional deps (gracefully skipped if not installed) ==============\n",
    "HAS_DADAPY = False\n",
    "try:\n",
    "    from dadapy import Data  # DADApy ID estimators (TwoNN, GRIDE)\n",
    "    HAS_DADAPY = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "HAS_SKDIM = False\n",
    "try:\n",
    "    from skdim.id import (\n",
    "        MOM, TLE, CorrInt, FisherS, lPCA,\n",
    "        MLE, DANCo, ESS, MiND_ML, MADA, KNN\n",
    "    )\n",
    "    HAS_SKDIM = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# IsoScore: use library if available, else a simple monotone fallback\n",
    "try:\n",
    "    from isoscore import IsoScore\n",
    "    _HAS_ISOSCORE = True\n",
    "except Exception:\n",
    "    _HAS_ISOSCORE = False\n",
    "    class _IsoScoreFallback:\n",
    "        @staticmethod\n",
    "        def IsoScore(X: np.ndarray) -> float:\n",
    "            C = np.cov(X.T, ddof=0)\n",
    "            ev = np.linalg.eigvalsh(C)\n",
    "            if ev.mean() <= 0 or ev[-1] <= 0:\n",
    "                return 0.0\n",
    "            # mean/peak eigenvalue ratio in [0,1]; higher ≈ more isotropic\n",
    "            return float(np.clip(ev.mean() / (ev[-1] + 1e-12), 0.0, 1.0))\n",
    "    IsoScore = _IsoScoreFallback()\n",
    "\n",
    "# =============================== CONFIG ===============================\n",
    "CSV_PATH        = \"en_ewt-ud-train_sentences.csv\"\n",
    "BASELINE        = \"gpt2\"                 # ← GPT‑2\n",
    "WORD_REP_MODE   = \"last\"                 # ← {\"last\",\"mean\"} word representation\n",
    "EXCLUDE_POS     = {\"X\", \"SYM\", \"PART\", \"INTJ\"}\n",
    "\n",
    "# NEW: drop tokens at 0-based sentence index == 1 (the 2nd token)\n",
    "# If you meant first token (1‑based “1”), change the check wid == 1 → wid == 0 in load_word_df.\n",
    "EXCLUDE_INDEX_1 = True\n",
    "\n",
    "# Sampling cap per POS for plotting (increase if you want)\n",
    "RAW_MAX_PER_POS = int(1e12)         # effectively no cap\n",
    "\n",
    "\n",
    "# Bootstrap replicates\n",
    "N_BOOTSTRAP_FAST   = 50\n",
    "N_BOOTSTRAP_HEAVY  = 100\n",
    "\n",
    "# Per-replicate sample size M (min(cap, N_pos))\n",
    "FAST_BS_MAX_SAMP_PER_POS  = int(1e12)\n",
    "HEAVY_BS_MAX_SAMP_PER_POS = 5000\n",
    "\n",
    "DADAPY_GRID_RANGE_MAX     = 64\n",
    "\n",
    "# Runtime / output\n",
    "BATCH_SIZE   = 8\n",
    "RAND_SEED    = 42\n",
    "PLOT_DIR     = Path(\"AO_POS\"); PLOT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CSV_DIR      = Path(\"tables\") / \"pos_bootstrap\"; CSV_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Isotropy extras\n",
    "PFI_DIRS = 256\n",
    "PFI_Q_LO = 5.0\n",
    "PFI_Q_HI = 95.0\n",
    "EPS      = 1e-12\n",
    "\n",
    "# Repro & device\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Seaborn style\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "# =============================== HELPERS ===============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _center(X: np.ndarray) -> np.ndarray:\n",
    "    return X - X.mean(0, keepdims=True)\n",
    "\n",
    "def _eigvals_from_X(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Eigenvalues of covariance up to a constant via SVD of centered X (descending).\"\"\"\n",
    "    Xc = _center(X.astype(np.float32, copy=False))\n",
    "    try:\n",
    "        _, S, _ = np.linalg.svd(Xc, full_matrices=False)\n",
    "        lam = (S**2).astype(np.float64)\n",
    "        lam.sort()\n",
    "        return lam[::-1]\n",
    "    except Exception:\n",
    "        return np.array([], dtype=np.float64)\n",
    "\n",
    "def _jitter_unique(X: np.ndarray, eps: float = 1e-7) -> np.ndarray:\n",
    "    \"\"\"Add tiny noise if there are duplicate rows (helps NN-based estimators).\"\"\"\n",
    "    try:\n",
    "        if np.unique(X, axis=0).shape[0] < X.shape[0]:\n",
    "            X = X + np.random.normal(scale=eps, size=X.shape).astype(X.dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return X\n",
    "\n",
    "# ========= Single-shot compute functions used inside bootstrap =========\n",
    "# --- Isotropy (fast) ---\n",
    "def _iso_once(X: np.ndarray) -> float:\n",
    "    return float(IsoScore.IsoScore(X))\n",
    "\n",
    "def _spect_once(X: np.ndarray) -> float:\n",
    "    ev = np.linalg.eigvalsh(np.cov(X.T, ddof=0))\n",
    "    return float(ev[-1] / (ev.mean() + 1e-9))\n",
    "\n",
    "def _rand_once(X: np.ndarray, K: int = 2000) -> float:\n",
    "    n = X.shape[0]\n",
    "    if n < 2: return np.nan\n",
    "    rng = np.random.default_rng()\n",
    "    K_eff = min(K, (n*(n-1))//2)\n",
    "    i = rng.integers(0, n, size=K_eff)\n",
    "    j = rng.integers(0, n, size=K_eff)\n",
    "    same = i == j\n",
    "    if same.any():\n",
    "        j[same] = rng.integers(0, n, size=same.sum())\n",
    "    A, B = X[i], X[j]\n",
    "    num = np.sum(A*B, axis=1)\n",
    "    den = (np.linalg.norm(A, axis=1)*np.linalg.norm(B, axis=1) + 1e-9)\n",
    "    return float(np.mean(np.abs(num/den)))\n",
    "\n",
    "def _sf_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    gm = np.exp(np.mean(np.log(lam + EPS)))\n",
    "    am = float(lam.mean() + EPS)\n",
    "    return float(gm / am)\n",
    "\n",
    "def _pfI_once(X: np.ndarray) -> float:\n",
    "    n, d = X.shape\n",
    "    if n < 2: return np.nan\n",
    "    rng = np.random.default_rng()\n",
    "    U = rng.standard_normal((PFI_DIRS, d)).astype(np.float32)\n",
    "    U /= np.linalg.norm(U, axis=1, keepdims=True) + 1e-9\n",
    "    S = U @ X.T\n",
    "    m = np.max(S, axis=1, keepdims=True)\n",
    "    logZ = (m + np.log(np.sum(np.exp(S - m), axis=1, keepdims=True))).ravel()\n",
    "    lo = np.percentile(logZ, PFI_Q_LO)\n",
    "    hi = np.percentile(logZ, PFI_Q_HI)\n",
    "    return float(np.exp(lo - hi))  # ≈ min Z / max Z (robust)\n",
    "\n",
    "def _vmf_kappa_once(X: np.ndarray) -> float:\n",
    "    if X.shape[0] < 2: return np.nan\n",
    "    Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
    "    R = np.linalg.norm(Xn.mean(axis=0))\n",
    "    d = Xn.shape[1]\n",
    "    if R < 1e-9: return 0.0\n",
    "    return float(max(R * (d - R**2) / (1.0 - R**2 + 1e-9), 0.0))\n",
    "\n",
    "# --- Linear ID (fast) ---\n",
    "def _pcaXX_once(X: np.ndarray, var_ratio: float) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    c = np.cumsum(lam); thr = c[-1] * var_ratio\n",
    "    return float(np.searchsorted(c, thr) + 1)\n",
    "\n",
    "def _pca95_once(X: np.ndarray) -> float:\n",
    "    return _pcaXX_once(X, 0.95)\n",
    "\n",
    "def _pca99_once(X: np.ndarray) -> float:\n",
    "    return _pcaXX_once(X, 0.99)\n",
    "\n",
    "def _erank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    p = lam / (lam.sum() + EPS)\n",
    "    H = -(p * np.log(p + EPS)).sum()\n",
    "    return float(np.exp(H))\n",
    "\n",
    "def _pr_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    s1 = lam.sum(); s2 = (lam**2).sum()\n",
    "    return float((s1**2) / (s2 + EPS))\n",
    "\n",
    "def _stable_rank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    return float(lam.sum() / (lam.max() + EPS))\n",
    "\n",
    "# --- Non-linear (heavy) ---\n",
    "def _dadapy_twonn_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    id_est, _, _ = d.compute_id_2NN()\n",
    "    return float(id_est)\n",
    "\n",
    "def _dadapy_gride_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    d.compute_distances(maxk=DADAPY_GRID_RANGE_MAX)\n",
    "    ids, _, _ = d.return_id_scaling_gride(range_max=DADAPY_GRID_RANGE_MAX)\n",
    "    return float(ids[-1])\n",
    "\n",
    "def _skdim_factory(name: str):\n",
    "    \"\"\"Return a factory that builds a fresh skdim estimator each call, or None.\"\"\"\n",
    "    if not HAS_SKDIM: return None\n",
    "    mapping = {\n",
    "        \"mom\": MOM, \"tle\": TLE, \"corrint\": CorrInt, \"fishers\": FisherS,\n",
    "        \"lpca\": lPCA, \"lpca99\": lPCA,\n",
    "        \"mle\": MLE, \"danco\": DANCo, \"mind_ml\": MiND_ML,\n",
    "        \"mada\": MADA, \"knn\": KNN, \"ess\": ESS,\n",
    "    }\n",
    "    cls = mapping.get(name)\n",
    "    if cls is None: return None\n",
    "\n",
    "    def _builder():\n",
    "        if name == \"lpca\":      # FO variant\n",
    "            return cls(ver=\"FO\")\n",
    "        elif name == \"lpca99\":  # ratio (0.99) variant\n",
    "            return cls(ver=\"ratio\", alphaRatio=0.99)\n",
    "        else:\n",
    "            return cls()\n",
    "    return _builder\n",
    "\n",
    "def _skdim_once_builder(name: str) -> Callable[[np.ndarray], float] | None:\n",
    "    build = _skdim_factory(name)\n",
    "    if build is None: return None\n",
    "    def _once(X: np.ndarray) -> float:\n",
    "        est = build()\n",
    "        est.fit(_jitter_unique(X))\n",
    "        return float(getattr(est, \"dimension_\", np.nan))\n",
    "    return _once\n",
    "\n",
    "# =============================== DATA ===============================\n",
    "def load_word_df(csv_path: str,\n",
    "                 exclude_pos: set[str] = EXCLUDE_POS,\n",
    "                 exclude_index_1: bool = EXCLUDE_INDEX_1):\n",
    "    \"\"\"\n",
    "    Load sentence rows (tokens, pos), expand to token rows, filter by POS\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, usecols=[\"sentence_id\",\"tokens\",\"pos\"])\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df.tokens = df.tokens.apply(_to_list); df.pos = df.pos.apply(_to_list)\n",
    "\n",
    "    rows = []\n",
    "    for sid, toks, poss in df[[\"sentence_id\",\"tokens\",\"pos\"]].itertuples(index=False):\n",
    "        L = min(len(toks), len(poss))\n",
    "        for wid in range(L):\n",
    "            if exclude_index_1 and wid == 0:\n",
    "                continue\n",
    "            p = poss[wid]\n",
    "            if p in exclude_pos:\n",
    "                continue\n",
    "            rows.append((sid, wid, p, toks[wid]))\n",
    "\n",
    "    word_df = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\",\"pos\",\"word\"])\n",
    "\n",
    "    # Extra safety: enforce the filter even if logic above changes later\n",
    "    if exclude_index_1 and not word_df.empty:\n",
    "        word_df = word_df[word_df.word_id !=0].reset_index(drop=True)\n",
    "\n",
    "    return df, word_df\n",
    "\n",
    "def sample_raw(word_df: pd.DataFrame, per_pos_cap: int = RAW_MAX_PER_POS) -> pd.DataFrame:\n",
    "    \"\"\"Per-POS cap without frequency matching.\"\"\"\n",
    "    picks = []\n",
    "    for p, sub in word_df.groupby(\"pos\", sort=False):\n",
    "        n = min(len(sub), per_pos_cap)\n",
    "        picks.append(sub.sample(n, random_state=RAND_SEED, replace=False))\n",
    "    return pd.concat(picks, ignore_index=True)\n",
    "\n",
    "# =============================== MODEL LOADING (robust for GPT‑2) ===============================\n",
    "def _load_tok_and_model(baseline: str):\n",
    "    \"\"\"\n",
    "    Robust loader:\n",
    "    - GPT‑2: prefer GPT2TokenizerFast; ensure right padding; set pad_token=eos_token if missing.\n",
    "    - Try both 'gpt2' and 'openai-community/gpt2' (some envs only have one).\n",
    "    - Set model.config.pad_token_id if missing.\n",
    "    \"\"\"\n",
    "    candidates = [baseline]\n",
    "    b = baseline.lower()\n",
    "    if \"gpt2\" in b:\n",
    "        if baseline != \"openai-community/gpt2\":\n",
    "            candidates.append(\"openai-community/gpt2\")\n",
    "        if baseline != \"gpt2\":\n",
    "            candidates.append(\"gpt2\")\n",
    "\n",
    "    last_err = None\n",
    "    for mid in candidates:\n",
    "        try:\n",
    "            if \"gpt2\" in mid.lower():\n",
    "                tok = GPT2TokenizerFast.from_pretrained(mid, add_prefix_space=True)\n",
    "            else:\n",
    "                tok = AutoTokenizer.from_pretrained(mid, use_fast=True, add_prefix_space=True)\n",
    "\n",
    "            if getattr(tok, \"padding_side\", None) != \"right\":\n",
    "                tok.padding_side = \"right\"\n",
    "            if tok.pad_token is None and getattr(tok, \"eos_token\", None) is not None:\n",
    "                tok.pad_token = tok.eos_token\n",
    "\n",
    "            mdl = AutoModel.from_pretrained(mid, output_hidden_states=True)\n",
    "            if getattr(mdl.config, \"pad_token_id\", None) is None and tok.pad_token_id is not None:\n",
    "                mdl.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "            mdl = mdl.eval().to(device)\n",
    "            if device == \"cuda\":\n",
    "                mdl.half()\n",
    "            return tok, mdl, mid\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "\n",
    "    raise RuntimeError(f\"Failed to load tokenizer/model for {candidates}. Last error: {last_err}\")\n",
    "\n",
    "# =============================== EMBEDDING ===============================\n",
    "def _num_hidden_layers(model) -> int:\n",
    "    n = getattr(model.config, \"num_hidden_layers\", None)\n",
    "    if n is None: n = getattr(model.config, \"n_layer\", None)\n",
    "    if n is None: raise ValueError(\"Cannot determine number of hidden layers from model.config\")\n",
    "    return int(n)\n",
    "\n",
    "def _hidden_size(model) -> int:\n",
    "    d = getattr(model.config, \"hidden_size\", None)\n",
    "    if d is None: d = getattr(model.config, \"n_embd\", None)  # GPT‑2\n",
    "    if d is None: raise ValueError(\"Cannot determine hidden size from model.config\")\n",
    "    return int(d)\n",
    "\n",
    "def embed_subset(df_all_sentences: pd.DataFrame,\n",
    "                 subset_df: pd.DataFrame,\n",
    "                 baseline: str = BASELINE,\n",
    "                 word_rep_mode: str = WORD_REP_MODE,\n",
    "                 batch_size: int = BATCH_SIZE) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Return reps (L,N,D) and filled mask (N,) for the selected tokens.\n",
    "    Uses word_ids() to map subword tokens back to original word indices.\n",
    "    \"\"\"\n",
    "    df_all_sentences[\"sentence_id\"] = df_all_sentences[\"sentence_id\"].astype(str)\n",
    "    subset_df[\"sentence_id\"] = subset_df[\"sentence_id\"].astype(str)\n",
    "\n",
    "    # sid -> list[(global_idx, word_id)]\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(subset_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(str(sid), []).append((gidx, int(wid)))\n",
    "\n",
    "    # Materialize in batch order\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_all_sentences[df_all_sentences.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    tokzr, model, model_id = _load_tok_and_model(baseline)\n",
    "\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
    "    if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "        enc_kwargs[\"add_prefix_space\"] = True\n",
    "\n",
    "    L = _num_hidden_layers(model) + 1   # include embedding layer\n",
    "    D = _hidden_size(model)\n",
    "    N = len(subset_df)\n",
    "\n",
    "    reps   = np.zeros((L, N, D), np.float32)  # use float32 (stable for some estimators)\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{model_id} (embed subset)\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)            # fast tokenizer\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            # hidden_states: tuple len=L of (B,T,D)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L,B,T,D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                # word_id -> token positions for this item\n",
    "                mp: Dict[int, List[int]] = {}\n",
    "                wids = enc_be.word_ids(b)\n",
    "                if wids is None:\n",
    "                    raise RuntimeError(\"Fast tokenizer required; word_ids() returned None.\")\n",
    "                for tidx, wid in enumerate(wids):\n",
    "                    if wid is not None:\n",
    "                        mp.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = mp.get(wid)\n",
    "                    if not toks:\n",
    "                        continue\n",
    "                    if word_rep_mode == \"last\":\n",
    "                        vec = h[:, b, toks[-1], :]          # last subtoken\n",
    "                    elif word_rep_mode == \"mean\":\n",
    "                        vec = h[:, b, toks, :].mean(axis=1) # mean over subtokens\n",
    "                    else:  # fallback to last for decoder models\n",
    "                        vec = h[:, b, toks[-1], :]\n",
    "                    reps[:, gidx, :] = vec.astype(np.float32, copy=False)\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    missing = int((~filled).sum())\n",
    "    if missing:\n",
    "        print(f\"⚠ Missing vectors for {missing} of {N} sampled words\")\n",
    "    del model; gc.collect()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    return reps, filled\n",
    "\n",
    "# =============================== BOOTSTRAP CORE ===============================\n",
    "def _bs_layer_loop(rep_sub: np.ndarray, M: int, n_reps: int, compute_once: Callable[[np.ndarray], float]):\n",
    "    \"\"\"Bootstrap: sample M with replacement and apply compute_once(X_layer) -> scalar for each layer.\"\"\"\n",
    "    L, N, D = rep_sub.shape\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    A = np.full((n_reps, L), np.nan, np.float32)\n",
    "    for r in range(n_reps):\n",
    "        idx = rng.integers(0, N, size=M)\n",
    "        for l in range(L):\n",
    "            X = rep_sub[l, idx].astype(np.float32, copy=False)\n",
    "            try:\n",
    "                A[r, l] = float(compute_once(X))\n",
    "            except Exception:\n",
    "                A[r, l] = np.nan\n",
    "    mu = np.nanmean(A, axis=0).astype(np.float32)\n",
    "    lo = np.nanpercentile(A, 2.5, axis=0).astype(np.float32)\n",
    "    hi = np.nanpercentile(A, 97.5, axis=0).astype(np.float32)\n",
    "    return mu, lo, hi\n",
    "\n",
    "# fast metric registry\n",
    "FAST_ONCE: Dict[str, Callable[[np.ndarray], float]] = {\n",
    "    \"iso\": _iso_once,\n",
    "    \"spect\": _spect_once,\n",
    "    \"rand\": _rand_once,\n",
    "    \"sf\": _sf_once,\n",
    "    \"pfI\": _pfI_once,\n",
    "    \"vmf_kappa\": _vmf_kappa_once,\n",
    "    \"erank\": _erank_once,\n",
    "    \"pr\": _pr_once,\n",
    "    \"stable_rank\": _stable_rank_once,\n",
    "    \"pca95\": _pca95_once,\n",
    "    \"pca99\": _pca99_once,\n",
    "}\n",
    "\n",
    "# heavy metric registry\n",
    "HEAVY_ONCE: Dict[str, Callable[[np.ndarray], float] | None] = {\n",
    "    \"twonn\": _dadapy_twonn_once,\n",
    "    \"gride\": _dadapy_gride_once,\n",
    "    \"mom\":   _skdim_once_builder(\"mom\"),\n",
    "    \"tle\":   _skdim_once_builder(\"tle\"),\n",
    "    \"corrint\": _skdim_once_builder(\"corrint\"),\n",
    "    \"fishers\": _skdim_once_builder(\"fishers\"),\n",
    "    \"lpca\":  _skdim_once_builder(\"lpca\"),\n",
    "    \"lpca95\": _skdim_once_builder(\"lpca99\"),\n",
    "    \"lpca99\": _skdim_once_builder(\"lpca99\"),\n",
    "    \"mle\":   _skdim_once_builder(\"mle\"),\n",
    "    \"danco\": _skdim_once_builder(\"danco\"),\n",
    "    \"ess\":   _skdim_once_builder(\"ess\"),\n",
    "    \"mada\":  _skdim_once_builder(\"mada\"),\n",
    "    \"knn\":   _skdim_once_builder(\"knn\"),\n",
    "}\n",
    "\n",
    "LABELS = {\n",
    "    # Isotropy\n",
    "    \"iso\":\"IsoScore\",\"sf\":\"Spectral Flatness\",\"pfI\":\"Partition Isotropy I\",\n",
    "    \"vmf_kappa\":\"vMF κ\",\"spect\":\"Spectral Ratio\",\"rand\":\"RandCos |μ|\",\n",
    "    # Linear ID\n",
    "    \"erank\":\"Effective Rank\",\"pr\":\"Participation Ratio\",\"stable_rank\":\"Stable Rank\",\n",
    "    # Non-linear\n",
    "    \"twonn\":\"TwoNN ID\",\"gride\":\"GRIDE\",\n",
    "    \"mom\":\"MOM\",\"tle\":\"TLE\",\"corrint\":\"CorrInt\",\"fishers\":\"FisherS\",\n",
    "    \"lpca\":\"lPCA FO\",\"lpca99\":\"lPCA 0.99\",\"lpca95\":\"lPCA 0.95\", \"mle\":\"MLE\",\"danco\":\"DANCo\",\n",
    "    \"ess\":\"ESS\",\"mada\":\"MADA\",\"knn\":\"KNN\",\n",
    "}\n",
    "\n",
    "# Choose plotting order\n",
    "PLOT_ORDER = (\n",
    "    \"iso\",\"sf\",\"pfI\",\"vmf_kappa\",\"spect\",\"rand\",\n",
    "    \"erank\",\"pr\",\"stable_rank\",\"pca95\",\"pca99\",\n",
    "    \"twonn\",\"gride\",\"mom\",\"tle\",\"corrint\",\"fishers\",\"lpca\",\"lpca99\",\n",
    "    \"mle\",\"danco\",\"ess\",\"mada\",\"knn\"\n",
    ")\n",
    "#ALL_METRICS = list(PLOT_ORDER)\n",
    "ALL_METRICS = [\"gride\"]\n",
    "\n",
    "# =============================== SAVE / PLOT ===============================\n",
    "def save_metric_csv_all_pos(metric: str,\n",
    "                            pos_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                            layers: np.ndarray,\n",
    "                            baseline: str,\n",
    "                            subset_name: str = \"raw\"):\n",
    "    rows = []\n",
    "    for p, stats in pos_to_stats.items():\n",
    "        mu, lo, hi, n = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\"), stats.get(\"n\", np.nan)\n",
    "        for l, val in enumerate(mu):\n",
    "            rows.append({\n",
    "                \"subset\": subset_name, \"model\": baseline, \"feature\": \"pos\",\n",
    "                \"class\": p, \"metric\": metric, \"layer\": int(layers[l]),\n",
    "                \"mean\": float(val) if np.isfinite(val) else np.nan,\n",
    "                \"ci_low\": float(lo[l]) if isinstance(lo, np.ndarray) and np.isfinite(lo[l]) else np.nan,\n",
    "                \"ci_high\": float(hi[l]) if isinstance(hi, np.ndarray) and np.isfinite(hi[l]) else np.nan,\n",
    "                \"n_tokens\": int(stats.get(\"n\", 0)), \"word_rep_mode\": WORD_REP_MODE,\n",
    "                \"source_csv\": Path(CSV_PATH).name,\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    out = CSV_DIR / f\"pos_{subset_name}_{metric}_{baseline}.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "\n",
    "def plot_metric_with_ci(pos_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                        layers: np.ndarray, metric: str, title: str, out_path: Path):\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    for p, stats in pos_to_stats.items():\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        if mu is None or np.all(np.isnan(mu)): continue\n",
    "        plt.plot(layers, mu, label=p, lw=1.8)\n",
    "        if isinstance(lo, np.ndarray) and isinstance(hi, np.ndarray) and not np.all(np.isnan(lo)):\n",
    "            plt.fill_between(layers, lo, hi, alpha=0.15)\n",
    "    plt.xlabel(\"Layer\"); plt.ylabel(LABELS.get(metric, metric.upper())); plt.title(title)\n",
    "    plt.legend(ncol=2, fontsize=\"small\", title=\"POS\", frameon=False)\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=220); plt.close()\n",
    "\n",
    "# =============================== DRIVER ===============================\n",
    "def run_pos_pipeline():\n",
    "    # 1) Load\n",
    "    df_all, word_df = load_word_df(CSV_PATH, EXCLUDE_POS, EXCLUDE_INDEX_1)\n",
    "    POS_TAGS = sorted(word_df.pos.unique())\n",
    "    print(f\"✓ corpus ready — {len(word_df):,} tokens across {len(POS_TAGS)} POS\")\n",
    "    print(f\"• DADApy: {'available' if HAS_DADAPY else 'missing'}  • scikit-dimension: {'available' if HAS_SKDIM else 'missing'}\")\n",
    "\n",
    "    # 2) Raw sampling only (no frequency match)\n",
    "    raw_df = sample_raw(word_df, RAW_MAX_PER_POS)\n",
    "    print(\"Sample sizes per POS (raw cap):\")\n",
    "    print(raw_df.pos.value_counts().to_dict())\n",
    "\n",
    "    # 3) Embed once (GPT‑2, last/mean subtoken per word)\n",
    "    reps, filled = embed_subset(df_all, raw_df, BASELINE, WORD_REP_MODE, BATCH_SIZE)\n",
    "    raw_df = raw_df.reset_index(drop=True).loc[filled].reset_index(drop=True)\n",
    "    pos_arr = raw_df.pos.values\n",
    "    L = reps.shape[0]; layers = np.arange(L)\n",
    "    print(f\"✓ embedded {len(raw_df):,} tokens  • layers={L}\")\n",
    "\n",
    "    # 4) Metric-by-metric loop (incremental outputs)\n",
    "    for metric in ALL_METRICS:\n",
    "        print(f\"\\n→ Computing metric: {metric} …\")\n",
    "\n",
    "        # Decide registry & bootstrap settings\n",
    "        if metric in FAST_ONCE:\n",
    "            compute_once = FAST_ONCE[metric]\n",
    "            n_bs = N_BOOTSTRAP_FAST\n",
    "            Mcap = FAST_BS_MAX_SAMP_PER_POS\n",
    "        else:\n",
    "            compute_once = HEAVY_ONCE.get(metric)\n",
    "            n_bs = N_BOOTSTRAP_HEAVY\n",
    "            Mcap = HEAVY_BS_MAX_SAMP_PER_POS\n",
    "\n",
    "        if compute_once is None:\n",
    "            print(f\"  (skipping {metric}: estimator unavailable)\")\n",
    "            continue\n",
    "\n",
    "        # Per-POS bootstrap\n",
    "        metric_results: Dict[str, Dict[str, np.ndarray]] = {}\n",
    "        for p in POS_TAGS:\n",
    "            idx = np.where(pos_arr == p)[0]\n",
    "            if idx.size < 3:\n",
    "                continue\n",
    "            sub = reps[:, idx]  # (L, n_p, D)\n",
    "            Np = sub.shape[1]\n",
    "            M = min(Mcap, Np)\n",
    "\n",
    "            mu, lo, hi = _bs_layer_loop(sub, M, n_bs, compute_once)\n",
    "            metric_results[p] = {\"mean\": mu, \"lo\": lo, \"hi\": hi, \"n\": int(Np)}\n",
    "\n",
    "        # Save + plot immediately for this metric\n",
    "        save_metric_csv_all_pos(metric, metric_results, layers, BASELINE, subset_name=\"raw\")\n",
    "        plot_metric_with_ci(metric_results, layers, metric,\n",
    "                    title=f\"{LABELS.get(metric, metric.upper())} • {BASELINE}\",\n",
    "                    out_path=PLOT_DIR / f\"raw_{metric}_{BASELINE}.png\")\n",
    "        print(f\"  ✓ saved: CSV= tables/pos_bootstrap/pos_raw_{metric}_{BASELINE}.csv  \"\n",
    "              f\"plot= AO_POS/raw_{metric}_{BASELINE}_{WORD_REP_MODE}.png\")\n",
    "\n",
    "        del metric_results; gc.collect()\n",
    "        if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    # Cleanup\n",
    "    del reps; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    print(\"\\n✓ done (incremental outputs produced per metric).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pos_pipeline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caa200d-8026-4ca2-9fb9-3c862bbdbf19",
   "metadata": {},
   "source": [
    "# all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c20f884-6005-412c-b061-2105b56abe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "HAS_DADAPY = False\n",
    "try:\n",
    "    from dadapy import Data  # DADApy ID estimators (TwoNN, GRIDE)\n",
    "    HAS_DADAPY = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "HAS_SKDIM = False\n",
    "try:\n",
    "    # include the fuller set of estimators now in use\n",
    "    from skdim.id import (\n",
    "        MOM, TLE, CorrInt, FisherS, lPCA,\n",
    "        MLE, DANCo, ESS, MiND_ML, MADA, KNN\n",
    "    )\n",
    "    HAS_SKDIM = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# IsoScore: use library if available, else a simple monotone fallback\n",
    "try:\n",
    "    from isoscore import IsoScore\n",
    "    _HAS_ISOSCORE = True\n",
    "except Exception:\n",
    "    _HAS_ISOSCORE = False\n",
    "    class _IsoScoreFallback:\n",
    "        @staticmethod\n",
    "        def IsoScore(X: np.ndarray) -> float:\n",
    "            C = np.cov(X.T, ddof=0)\n",
    "            ev = np.linalg.eigvalsh(C)\n",
    "            if ev.mean() <= 0 or ev[-1] <= 0:\n",
    "                return 0.0\n",
    "            # mean/peak eigenvalue ratio in [0,1]; higher ≈ more isotropic\n",
    "            return float(np.clip(ev.mean() / ev[-1], 0.0, 1.0))\n",
    "    IsoScore = _IsoScoreFallback()\n",
    "\n",
    "# =============================== CONFIG ===============================\n",
    "CSV_PATH    = \"it_isdt-ud-train_sentences.csv\"\n",
    "MODELS      = [\"bert-base-uncased\", \"gpt2\"]  # compare both\n",
    "REP_MODES   = [\"first\", \"mean\", \"last\"]      # extract these\n",
    "EXCLUDE_POS = {\"X\", \"SYM\", \"PART\", \"INTJ\"}   # optional exclusions\n",
    "\n",
    "# Pooled tokens cap (set to None to use ALL tokens)\n",
    "ALL_TOKEN_CAP            = None          # e.g., 80_000 for safety\n",
    "BATCH_SIZE               = 1\n",
    "RAND_SEED                = 42\n",
    "\n",
    "# Bootstrap replicates\n",
    "N_BOOTSTRAP_FAST         = 2\n",
    "N_BOOTSTRAP_HEAVY        = 2\n",
    "\n",
    "# Per-replicate sample size M for pooled tokens (min(cap, N_all))\n",
    "FAST_BS_MAX_SAMP         = 10000         # for fast metrics\n",
    "HEAVY_BS_MAX_SAMP        = 10_000          # for TwoNN/GRIDE/skdim (tune for speed)\n",
    "\n",
    "# GRIDE multi-scale max neighbor rank\n",
    "DADAPY_GRID_RANGE_MAX    = 64\n",
    "\n",
    "# Isotropy Monte-Carlo directions / quantiles and epsilon\n",
    "PFI_DIRS   = 128\n",
    "PFI_Q_LO   = 5.0\n",
    "PFI_Q_HI   = 95.0\n",
    "EPS        = 1e-12\n",
    "\n",
    "# Output\n",
    "OUT_DIR     = Path(\"metrics_all_token\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOT_DIR    = OUT_DIR / \"plots\";      PLOT_DIR.mkdir(exist_ok=True)\n",
    "CSV_DIR     = OUT_DIR / \"tables\";     CSV_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Repro & device\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Style\n",
    "sns.set_style(\"darkgrid\"); plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "MODEL_LABEL = {\n",
    "    \"bert-base-uncased\": \"BERT-base\",\n",
    "    \"gpt2\": \"GPT-2\",\n",
    "}\n",
    "\n",
    "def _model_tag(name: str) -> str:\n",
    "    return name.split(\"/\")[-1].replace(\":\", \"_\")\n",
    "\n",
    "# =============================== HELPERS ===============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _center(X: np.ndarray) -> np.ndarray:\n",
    "    return X - X.mean(0, keepdims=True)\n",
    "\n",
    "def _eigvals_from_X(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Eigenvalues of covariance up to a constant via SVD of centered X (descending).\"\"\"\n",
    "    Xc = _center(X.astype(np.float32, copy=False))\n",
    "    try:\n",
    "        _, S, _ = np.linalg.svd(Xc, full_matrices=False)\n",
    "        lam = (S**2).astype(np.float64)\n",
    "        lam.sort()\n",
    "        return lam[::-1]\n",
    "    except Exception:\n",
    "        return np.array([], dtype=np.float64)\n",
    "\n",
    "def _jitter_unique(X: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"Add tiny noise if there are duplicate rows (helps NN-based estimators).\"\"\"\n",
    "    try:\n",
    "        if np.unique(X, axis=0).shape[0] < X.shape[0]:\n",
    "            X = X + np.random.normal(scale=eps, size=X.shape).astype(X.dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return X\n",
    "\n",
    "# ========= Per-subsample single-value compute functions (used inside bootstrap) =========\n",
    "# --- Isotropy (fast) ---\n",
    "def _iso_once(X: np.ndarray) -> float:\n",
    "    return float(IsoScore.IsoScore(X))\n",
    "\n",
    "def _spect_once(X: np.ndarray) -> float:\n",
    "    ev = np.linalg.eigvalsh(np.cov(X.T, ddof=0))\n",
    "    return float(ev[-1] / (ev.mean() + EPS))\n",
    "\n",
    "def _rand_once(X: np.ndarray, K: int = 2000) -> float:\n",
    "    n = X.shape[0]\n",
    "    if n < 2: return np.nan\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    K_eff = min(K, (n*(n-1))//2)\n",
    "    i = rng.integers(0, n, size=K_eff)\n",
    "    j = rng.integers(0, n, size=K_eff)\n",
    "    same = i == j\n",
    "    if same.any():\n",
    "        j[same] = rng.integers(0, n, size=same.sum())\n",
    "    A, B = X[i], X[j]\n",
    "    num = np.sum(A*B, axis=1)\n",
    "    den = (np.linalg.norm(A, axis=1)*np.linalg.norm(B, axis=1) + 1e-9)\n",
    "    return float(np.mean(np.abs(num/den)))\n",
    "\n",
    "def _sf_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    gm = np.exp(np.mean(np.log(lam + EPS)))\n",
    "    am = float(lam.mean() + EPS)\n",
    "    return float(gm / am)\n",
    "\n",
    "def _pfI_once(X: np.ndarray) -> float:\n",
    "    n, d = X.shape\n",
    "    if n < 2: return np.nan\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    U = rng.standard_normal((PFI_DIRS, d)).astype(np.float32)\n",
    "    U /= np.linalg.norm(U, axis=1, keepdims=True) + 1e-9\n",
    "    S = U @ X.T\n",
    "    m = np.max(S, axis=1, keepdims=True)\n",
    "    logZ = (m + np.log(np.sum(np.exp(S - m), axis=1, keepdims=True))).ravel()\n",
    "    lo = np.percentile(logZ, PFI_Q_LO)\n",
    "    hi = np.percentile(logZ, PFI_Q_HI)\n",
    "    return float(np.exp(lo - hi))  # ≈ min Z / max Z (robust)\n",
    "\n",
    "def _vmf_kappa_once(X: np.ndarray) -> float:\n",
    "    if X.shape[0] < 2: return np.nan\n",
    "    Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-9)\n",
    "    R = np.linalg.norm(Xn.mean(axis=0))\n",
    "    d = Xn.shape[1]\n",
    "    if R < 1e-9: return 0.0\n",
    "    # standard closed-form approximation\n",
    "    return float(max(R * (d - R**2) / (1.0 - R**2 + 1e-9), 0.0))\n",
    "\n",
    "# --- Linear ID (fast) ---\n",
    "def _pcaXX_once(X: np.ndarray, var_ratio: float) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    c = np.cumsum(lam); thr = c[-1] * var_ratio\n",
    "    return float(np.searchsorted(c, thr) + 1)\n",
    "\n",
    "def _pca95_once(X: np.ndarray) -> float:\n",
    "    return _pcaXX_once(X, 0.95)\n",
    "\n",
    "def _pca99_once(X: np.ndarray) -> float:\n",
    "    return _pcaXX_once(X, 0.99)\n",
    "\n",
    "def _erank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    p = lam / (lam.sum() + EPS)\n",
    "    H = -(p * np.log(p + EPS)).sum()\n",
    "    return float(np.exp(H))\n",
    "\n",
    "def _pr_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    s1 = lam.sum(); s2 = (lam**2).sum()\n",
    "    return float((s1**2) / (s2 + EPS))\n",
    "\n",
    "def _stable_rank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    return float(lam.sum() / (lam.max() + EPS))\n",
    "\n",
    "# --- Non-linear (heavy) ---\n",
    "def _dadapy_twonn_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    id_est, _, _ = d.compute_id_2NN()\n",
    "    return float(id_est)\n",
    "\n",
    "def _dadapy_gride_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    d.compute_distances(maxk=DADAPY_GRID_RANGE_MAX)\n",
    "    ids, _, _ = d.return_id_scaling_gride(range_max=DADAPY_GRID_RANGE_MAX)\n",
    "    return float(ids[-1])\n",
    "\n",
    "def _skdim_factory(name: str):\n",
    "    \"\"\"Return a factory that builds a fresh skdim estimator each call, or None.\"\"\"\n",
    "    if not HAS_SKDIM: return None\n",
    "    mapping = {\n",
    "        \"mom\": MOM, \"tle\": TLE, \"corrint\": CorrInt, \"fishers\": FisherS,\n",
    "        \"lpca\": lPCA, \"lpca99\": lPCA,\n",
    "        \"mle\": MLE, \"danco\": DANCo,  \"mind_ml\": MiND_ML,\n",
    "        \"mada\": MADA, \"knn\": KNN,\n",
    "    }\n",
    "    cls = mapping.get(name)\n",
    "    if cls is None: return None\n",
    "\n",
    "    def _builder():\n",
    "        if name == \"lpca\":      # FO variant\n",
    "            return cls(ver=\"FO\")\n",
    "        elif name == \"lpca99\":  # ratio (0.99) variant\n",
    "            return cls(ver=\"ratio\", alphaRatio=0.99)\n",
    "        else:\n",
    "            return cls()\n",
    "    return _builder\n",
    "\n",
    "def _skdim_once_builder(name: str) -> Callable[[np.ndarray], float] | None:\n",
    "    build = _skdim_factory(name)\n",
    "    if build is None: return None\n",
    "\n",
    "    def _once(X: np.ndarray) -> float:\n",
    "        est = build(); est.fit(_jitter_unique(X))\n",
    "        return float(getattr(est, \"dimension_\", np.nan))\n",
    "    return _once\n",
    "\n",
    "# =============================== DATA ===============================\n",
    "def load_word_df(csv_path: str, exclude_pos: set[str] = EXCLUDE_POS):\n",
    "    df = pd.read_csv(csv_path, usecols=[\"sentence_id\",\"tokens\",\"pos\"])\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)  # keep string IDs\n",
    "    df.tokens = df.tokens.apply(_to_list); df.pos = df.pos.apply(_to_list)\n",
    "\n",
    "    rows = []\n",
    "    for sid, toks, poss in df[[\"sentence_id\",\"tokens\",\"pos\"]].itertuples(index=False):\n",
    "        for wid, (tok, p) in enumerate(zip(toks, poss)):\n",
    "            if (exclude_pos is None) or (p not in exclude_pos):\n",
    "                rows.append((sid, wid))\n",
    "    word_df = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\"])\n",
    "    return df, word_df\n",
    "\n",
    "def sample_all(word_df: pd.DataFrame, cap: int | None) -> pd.DataFrame:\n",
    "    if cap is None or len(word_df) <= cap:\n",
    "        return word_df.reset_index(drop=True)\n",
    "    return word_df.sample(cap, random_state=RAND_SEED).reset_index(drop=True)\n",
    "\n",
    "# =============================== EMBEDDING ===============================\n",
    "def embed_subset(df_all_sentences: pd.DataFrame,\n",
    "                 subset_df: pd.DataFrame,\n",
    "                 baseline: str,\n",
    "                 word_rep_mode: str,\n",
    "                 batch_size: int = BATCH_SIZE) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return reps (L,N,D) and filled mask (N,) for the selected tokens.\"\"\"\n",
    "    df_all_sentences[\"sentence_id\"] = df_all_sentences[\"sentence_id\"].astype(str)\n",
    "    subset_df = subset_df.copy()\n",
    "    subset_df[\"sentence_id\"] = subset_df[\"sentence_id\"].astype(str)\n",
    "\n",
    "    # sid -> list[(global_idx, word_id)]\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(subset_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(str(sid), []).append((gidx, int(wid)))\n",
    "\n",
    "    # Materialize in the exact order we will batch\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_all_sentences[df_all_sentences.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    tokzr = AutoTokenizer.from_pretrained(baseline, use_fast=True,  add_prefix_space=True)\n",
    "    # GPT-2 has no PAD token → use EOS for padding\n",
    "    if tokzr.pad_token is None:\n",
    "        tokzr.pad_token = tokzr.eos_token\n",
    "\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "        enc_kwargs[\"add_prefix_space\"] = True\n",
    "\n",
    "    model = AutoModel.from_pretrained(baseline, output_hidden_states=True).eval().to(device)\n",
    "    if getattr(model.config, \"pad_token_id\", None) is None and tokzr.pad_token_id is not None:\n",
    "        model.config.pad_token_id = tokzr.pad_token_id\n",
    "    if device == \"cuda\": model.half()\n",
    "\n",
    "    L = model.config.num_hidden_layers + 1\n",
    "    D = model.config.hidden_size\n",
    "    N = len(subset_df)\n",
    "\n",
    "    reps   = np.zeros((L, N, D), np.float16)\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    tag = _model_tag(baseline)\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{tag}:{word_rep_mode} (embed all)\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L,B,T,D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                # word_id -> token positions for this item\n",
    "                mp = {}\n",
    "                for tidx, wid in enumerate(enc_be.word_ids(b)):\n",
    "                    if wid is not None:\n",
    "                        mp.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = mp.get(wid);\n",
    "                    if not toks: continue\n",
    "                    if word_rep_mode == \"first\":\n",
    "                        vec = h[:, b, toks[0], :]\n",
    "                    elif word_rep_mode == \"last\":\n",
    "                        vec = h[:, b, toks[-1], :]\n",
    "                    else:  # \"mean\"\n",
    "                        vec = h[:, b, toks, :].mean(axis=1)\n",
    "                    reps[:, gidx, :] = vec.astype(np.float16, copy=False)\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            # free batch buffers\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    missing = int((~filled).sum())\n",
    "    if missing:\n",
    "        print(f\"⚠ Missing vectors for {missing} of {N} tokens\")\n",
    "    del model; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    return reps[:, filled], filled\n",
    "\n",
    "# =============================== BOOTSTRAP CORE ===============================\n",
    "def _bs_layer_loop(rep_sub: np.ndarray, M: int, n_reps: int, compute_once: Callable[[np.ndarray], float]):\n",
    "    \"\"\"Bootstrap: sample M with replacement and apply compute_once(X_layer) -> scalar for each layer.\"\"\"\n",
    "    L, N, D = rep_sub.shape\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    A = np.full((n_reps, L), np.nan, np.float32)\n",
    "    for r in range(n_reps):\n",
    "        idx = rng.integers(0, N, size=M)\n",
    "        for l in range(L):\n",
    "            X = rep_sub[l, idx].astype(np.float32, copy=False)\n",
    "            try:\n",
    "                A[r, l] = float(compute_once(X))\n",
    "            except Exception:\n",
    "                A[r, l] = np.nan\n",
    "    mu = np.nanmean(A, axis=0).astype(np.float32)\n",
    "    lo = np.nanpercentile(A, 2.5, axis=0).astype(np.float32)\n",
    "    hi = np.nanpercentile(A, 97.5, axis=0).astype(np.float32)\n",
    "    return mu, lo, hi\n",
    "\n",
    "# fast metric registry (name -> callable(X)->float)\n",
    "FAST_ONCE: Dict[str, Callable[[np.ndarray], float]] = {\n",
    "    \"iso\": _iso_once,\n",
    "    \"spect\": _spect_once,\n",
    "    \"rand\": _rand_once,\n",
    "    \"sf\": _sf_once,\n",
    "    \"vmf_kappa\": _vmf_kappa_once,\n",
    "    \"erank\": _erank_once,\n",
    "    \"pr\": _pr_once,\n",
    "    \"stable_rank\": _stable_rank_once,\n",
    "\n",
    "}\n",
    "\n",
    "# heavy metric registry\n",
    "HEAVY_ONCE: Dict[str, Callable[[np.ndarray], float] | None] = {\n",
    "    \"twonn\":   _dadapy_twonn_once,\n",
    "    \"gride\":   _dadapy_gride_once,\n",
    "    \"mom\":     _skdim_once_builder(\"mom\"),\n",
    "    \"tle\":     _skdim_once_builder(\"tle\"),\n",
    "    \"corrint\": _skdim_once_builder(\"corrint\"),\n",
    "    \"fishers\": _skdim_once_builder(\"fishers\"),\n",
    "    \"lpca\":    _skdim_once_builder(\"lpca\"),\n",
    "    \"lpca95\":    _skdim_once_builder(\"lpca95\"),\n",
    "    \"lpca99\":  _skdim_once_builder(\"lpca99\"),\n",
    "    \"mle\":     _skdim_once_builder(\"mle\"),\n",
    "    \"mada\":    _skdim_once_builder(\"mada\"),\n",
    "}\n",
    "\n",
    "LABELS = {\n",
    "    # Isotropy\n",
    "    \"iso\":\"IsoScore\",\"spect\":\"Spectral Ratio\",\"rand\":\"RandCos |μ|\",\n",
    "    \"sf\":\"Spectral Flatness\",\"vmf_kappa\":\"vMF κ\",\n",
    "    # Linear ID\n",
    "    \"erank\":\"Effective Rank\",\"pr\":\"Participation Ratio\",\"stable_rank\":\"Stable Rank\", \"lpca99\":\"lPCA 0.95\",  \"lpca99\":\"lPCA 0.99\", \"lpca\":\"lPCA (FO)\",\n",
    "    # Non-linear\n",
    "    \"twonn\":\"TwoNN ID\",\"gride\":\"GRIDE\",\n",
    "    \"mom\":\"MOM\",\"tle\":\"TLE\",\"corrint\":\"CorrInt\",\n",
    "    \"fishers\":\"FisherS\",,\"mle\":\"MLE\",\"mind_ml\":\"MiND-ML\",\n",
    "    \"mada\":\"MADA\",\"knn\":\"KNN\",\n",
    "}\n",
    "\n",
    "# Choose plotting order\n",
    "PLOT_ORDER = (\n",
    "    \"iso\",\"sf\",\"pfI\",\"vmf_kappa\",\"spect\",\"rand\",\n",
    "    \"erank\",\"pr\",\"stable_rank\"\n",
    "    \"twonn\",\"gride\",\"mom\",\"tle\",\"corrint\",\"fishers\",\"lpca\",\"lpca99\", \"lpca95\", \n",
    "    \"mle\",\"mind_ml\",\"mada\",\"knn\"\n",
    ")\n",
    "\n",
    "ALL_METRICS = [m for m in PLOT_ORDER if (m in FAST_ONCE or HEAVY_ONCE.get(m) is not None)]\n",
    "\n",
    "# =============================== SAVE / PLOT ===============================\n",
    "def save_metric_csv_alltokens(metric: str,\n",
    "                              results: Dict[tuple, Dict[str, np.ndarray]],\n",
    "                              layers: np.ndarray):\n",
    "    \"\"\"\n",
    "    results key: (model, rep_mode) -> {\"mean\",\"lo\",\"hi\",\"n\"}\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for (model, mode), stats in results.items():\n",
    "        mu, lo, hi, n = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\"), stats.get(\"n\", np.nan)\n",
    "        tag = _model_tag(model)\n",
    "        for l, val in enumerate(mu):\n",
    "            rows.append({\n",
    "                \"model\": model, \"model_tag\": tag,\n",
    "                \"word_rep_mode\": mode, \"metric\": metric, \"layer\": int(layers[l]),\n",
    "                \"mean\": float(val) if np.isfinite(val) else np.nan,\n",
    "                \"ci_low\": float(lo[l]) if isinstance(lo, np.ndarray) and np.isfinite(lo[l]) else np.nan,\n",
    "                \"ci_high\": float(hi[l]) if isinstance(hi, np.ndarray) and np.isfinite(hi[l]) else np.nan,\n",
    "                \"n_tokens\": int(stats.get(\"n\", 0)),\n",
    "                \"source_csv\": Path(CSV_PATH).name,\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    out = CSV_DIR / f\"alltokens_{metric}.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "\n",
    "def plot_metric_compare(results: Dict[tuple, Dict[str, np.ndarray]],\n",
    "                        layers: np.ndarray, metric: str):\n",
    "    \"\"\"\n",
    "    Plot all (model,rep) curves together with CI.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5.5))\n",
    "    for (model, mode), stats in results.items():\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        label = f\"{MODEL_LABEL.get(model, model)} • {mode}\"\n",
    "        if mu is None or np.all(np.isnan(mu)): continue\n",
    "        plt.plot(layers, mu, label=label, lw=1.8)\n",
    "        if isinstance(lo, np.ndarray) and isinstance(hi, np.ndarray) and not np.all(np.isnan(lo)):\n",
    "            plt.fill_between(layers, lo, hi, alpha=0.15)\n",
    "    plt.xlabel(\"Layer\")\n",
    "    plt.ylabel(LABELS.get(metric, metric.upper()))\n",
    "    plt.title(f\"{LABELS.get(metric, metric.upper())} • all tokens\")\n",
    "    plt.legend(ncol=2, fontsize=\"small\", frameon=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOT_DIR / f\"alltokens_{metric}.png\", dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "def plot_metric_deltas(results: Dict[tuple, Dict[str, np.ndarray]],\n",
    "                       layers: np.ndarray, metric: str):\n",
    "    \"\"\"\n",
    "    Optional: plot (first - mean) and (last - mean) per model.\n",
    "    Uses difference of bootstrap means (no CI; for exact CIs you'd need paired resamples).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(9, 4.8))\n",
    "    for model in MODELS:\n",
    "        base = results.get((model, \"mean\"))\n",
    "        if base is None: continue\n",
    "        base_mu = base[\"mean\"]\n",
    "        for mode in (\"first\", \"last\"):\n",
    "            cur = results.get((model, mode))\n",
    "            if cur is None: continue\n",
    "            diff = cur[\"mean\"] - base_mu\n",
    "            label = f\"{MODEL_LABEL.get(model, model)} • {mode} − mean\"\n",
    "            plt.plot(layers, diff, label=label, lw=2)\n",
    "    plt.axhline(0.0, ls=\"--\", lw=1, alpha=0.6)\n",
    "    plt.xlabel(\"Layer\")\n",
    "    plt.ylabel(f\"Δ {LABELS.get(metric, metric.upper())}\")\n",
    "    plt.title(f\"Representation deltas (first/last − mean) • {LABELS.get(metric, metric.upper())}\")\n",
    "    plt.legend(ncol=2, fontsize=\"small\", frameon=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOT_DIR / f\"alltokens_{metric}_deltas.png\", dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "# =============================== METRIC COMPUTE ===============================\n",
    "def _compute_metric_bootstrap(rep: np.ndarray, metric: str) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    rep: (L, N, D)\n",
    "    \"\"\"\n",
    "    N = rep.shape[1]\n",
    "    if metric in FAST_ONCE:\n",
    "        M = min(FAST_BS_MAX_SAMP, N)\n",
    "        n_bs = N_BOOTSTRAP_FAST\n",
    "        compute_once = FAST_ONCE[metric]\n",
    "    else:\n",
    "        compute_once = HEAVY_ONCE.get(metric)\n",
    "        if compute_once is None:\n",
    "            return (np.full(rep.shape[0], np.nan),)*3\n",
    "        M = min(HEAVY_BS_MAX_SAMP, N)\n",
    "        n_bs = N_BOOTSTRAP_HEAVY\n",
    "    mu, lo, hi = _bs_layer_loop(rep, M, n_bs, compute_once)\n",
    "    return mu, lo, hi\n",
    "\n",
    "def compute_all_metrics_for_rep(rep: np.ndarray) -> Dict[str, Dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Returns: metric -> {\"mean\": mu, \"lo\": lo, \"hi\": hi, \"n\": N}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for metric in ALL_METRICS:\n",
    "        mu, lo, hi = _compute_metric_bootstrap(rep, metric)\n",
    "        results[metric] = {\"mean\": mu, \"lo\": lo, \"hi\": hi, \"n\": int(rep.shape[1])}\n",
    "    return results\n",
    "\n",
    "# =============================== DRIVER ===============================\n",
    "def run_alltokens_compare():\n",
    "    # 1) Load & pick pooled tokens\n",
    "    df_all, word_df = load_word_df(CSV_PATH, EXCLUDE_POS)\n",
    "    all_df = sample_all(word_df, ALL_TOKEN_CAP)\n",
    "    print(f\"✓ pooled tokens: {len(all_df):,}\")\n",
    "\n",
    "    # 2) For each (model, rep_mode): embed, compute metrics per layer\n",
    "    all_layers = None\n",
    "    combined: Dict[str, Dict[tuple, Dict[str, np.ndarray]]] = {m:{} for m in ALL_METRICS}\n",
    "\n",
    "    for model in MODELS:\n",
    "        for mode in REP_MODES:\n",
    "            reps, filled = embed_subset(df_all, all_df, model, mode, BATCH_SIZE)\n",
    "\n",
    "            L = reps.shape[0]\n",
    "            if all_layers is None:\n",
    "                all_layers = np.arange(L)\n",
    "            print(f\"→ {model} • {mode}: reps shape = {reps.shape}\")\n",
    "\n",
    "            per_metric = compute_all_metrics_for_rep(reps)\n",
    "            for metric, stats in per_metric.items():\n",
    "                combined[metric][(model, mode)] = stats\n",
    "\n",
    "            # free\n",
    "            del reps; gc.collect()\n",
    "            if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    # 3) Save & plot per metric\n",
    "    for metric, results in combined.items():\n",
    "        save_metric_csv_alltokens(metric, results, all_layers)\n",
    "        plot_metric_compare(results, all_layers, metric)\n",
    "        plot_metric_deltas(results, all_layers, metric)\n",
    "\n",
    "    print(\"✓ done; outputs in:\", OUT_DIR.resolve())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_alltokens_compare()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e33698-8e57-4d93-9219-8bb2b9493af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "openai-community/gpt2: embed (last): 100%|██| 2517/2517 [00:41<00:00, 60.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved interactive HTML to: pca3d_all_tokens/gpt2_pca3d_layers.html\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, GPT2TokenizerFast\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Plotly for interactive 3D\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# =============================== CONFIG ===============================\n",
    "CSV_PATH   = \"en_ewt-ud-train_sentences.csv\"  # columns: sentence_id (str), tokens (list[str])\n",
    "MODEL_ID   = \"gpt2\"                           # e.g., \"gpt2\", \"bert-base-uncased\"\n",
    "REP_MODE   = \"last\"                           # \"first\" | \"last\" | \"mean\"\n",
    "BATCH_SIZE = 4\n",
    "RAND_SEED  = 42\n",
    "\n",
    "# For plotting (subsample to keep the browser smooth)\n",
    "PCA_MAX_POINTS = None                         # None = plot all tokens\n",
    "\n",
    "# Output\n",
    "OUT_DIR  = Path(\"pca3d_all_tokens\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "HTML_OUT = OUT_DIR / f\"{MODEL_ID.replace('/','_')}_pca3d_layers.html\"\n",
    "\n",
    "# Repro + device\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# =============================== PLOT SETTINGS ===============================\n",
    "# Bigger fonts: set axes to 17 (what you asked)\n",
    "AXIS_TITLE_FONT_SIZE = 24\n",
    "AXIS_TICK_FONT_SIZE  = 20\n",
    "GLOBAL_FONT_SIZE     = 20\n",
    "SLIDER_FONT_SIZE     = 20\n",
    "HOVER_FONT_SIZE      = 20\n",
    "TITLE_FONT_SIZE      = 20\n",
    "\n",
    "\n",
    "\n",
    "MARKER_SIZE    = 2\n",
    "MARKER_OPACITY = 0.70\n",
    "\n",
    "# ---- Color options (easy to change) ----\n",
    "# Choose one:\n",
    "#   COLOR_MODE = \"constant\"   -> single fixed color for all points\n",
    "#   COLOR_MODE = \"per_layer\"  -> different color per layer (edit LAYER_COLORS)\n",
    "#   COLOR_MODE = \"colorscale\" -> gradient color per point (by pc1/pc2/pc3/radius)\n",
    "COLOR_MODE = \"constant\"\n",
    "\n",
    "# If COLOR_MODE == \"constant\"\n",
    "MARKER_COLOR = \"orange\"  # any CSS color: \"red\", \"#ff8800\", \"rgb(255,0,0)\", etc.\n",
    "\n",
    "# If COLOR_MODE == \"per_layer\"\n",
    "LAYER_COLORS = None\n",
    "# Example:\n",
    "# LAYER_COLORS = [\"#636EFA\",\"#EF553B\",\"#00CC96\",\"#AB63FA\",\"#FFA15A\",\"#19D3F3\",\"#FF6692\",\"#B6E880\"]\n",
    "\n",
    "# If COLOR_MODE == \"colorscale\"\n",
    "COLOR_BY   = \"pc1\"       # \"pc1\" | \"pc2\" | \"pc3\" | \"radius\"\n",
    "COLOR_SCALE = \"Turbo\"    # \"Viridis\", \"Cividis\", \"Plasma\", \"Inferno\", \"Turbo\", etc.\n",
    "SHOW_COLORBAR = True\n",
    "\n",
    "# Optional: show the figure window (useful in notebooks / interactive sessions)\n",
    "SHOW_FIG = False\n",
    "\n",
    "# =============================== HELPERS ===============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _num_hidden_layers(model) -> int:\n",
    "    n = getattr(model.config, \"num_hidden_layers\", None)\n",
    "    if n is None: n = getattr(model.config, \"n_layer\", None)\n",
    "    if n is None: raise ValueError(\"Cannot determine num_hidden_layers from model.config\")\n",
    "    return int(n)\n",
    "\n",
    "def _hidden_size(model) -> int:\n",
    "    d = getattr(model.config, \"hidden_size\", None)\n",
    "    if d is None: d = getattr(model.config, \"n_embd\", None)\n",
    "    if d is None: raise ValueError(\"Cannot determine hidden_size from model.config\")\n",
    "    return int(d)\n",
    "\n",
    "def _load_tok_and_model(model_id: str):\n",
    "    \"\"\"\n",
    "    Robust loader:\n",
    "      - For GPT-2, force the fast tokenizer and try both 'gpt2' and 'openai-community/gpt2'.\n",
    "      - Set PAD = EOS for GPT-2-like tokenizers.\n",
    "    \"\"\"\n",
    "    cands = [model_id]\n",
    "    if \"gpt2\" in model_id.lower():\n",
    "        if model_id != \"openai-community/gpt2\": cands.append(\"openai-community/gpt2\")\n",
    "        if model_id != \"gpt2\": cands.append(\"gpt2\")\n",
    "\n",
    "    last_err = None\n",
    "    for mid in cands:\n",
    "        try:\n",
    "            if \"gpt2\" in mid.lower():\n",
    "                tok = GPT2TokenizerFast.from_pretrained(mid, add_prefix_space=True)\n",
    "            else:\n",
    "                tok = AutoTokenizer.from_pretrained(mid, use_fast=True, add_prefix_space=True)\n",
    "\n",
    "            # pad-right and PAD token if missing (common for GPT-2)\n",
    "            if getattr(tok, \"padding_side\", None) != \"right\":\n",
    "                tok.padding_side = \"right\"\n",
    "            if tok.pad_token is None and getattr(tok, \"eos_token\", None) is not None:\n",
    "                tok.pad_token = tok.eos_token\n",
    "\n",
    "            mdl = AutoModel.from_pretrained(mid, output_hidden_states=True)\n",
    "            if getattr(mdl.config, \"pad_token_id\", None) is None and tok.pad_token_id is not None:\n",
    "                mdl.config.pad_token_id = tok.pad_token_id\n",
    "            if device == \"cuda\":\n",
    "                mdl = mdl.half()\n",
    "            return tok, mdl.eval().to(device), mid\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    raise RuntimeError(f\"Failed to load model/tokenizer for {cands}: {last_err}\")\n",
    "\n",
    "def load_word_df(csv_path: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Return (df_all_sentences, token_index_df) for ALL tokens (no labels needed).\"\"\"\n",
    "    df = pd.read_csv(csv_path, usecols=[\"sentence_id\",\"tokens\"])\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df.tokens = df.tokens.apply(_to_list)\n",
    "    rows = []\n",
    "    for sid, toks in df[[\"sentence_id\",\"tokens\"]].itertuples(index=False):\n",
    "        for wid in range(len(toks)):\n",
    "            rows.append((sid, wid))\n",
    "    word_df = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\"])\n",
    "    return df, word_df\n",
    "\n",
    "# =============================== EMBEDDING ===============================\n",
    "def embed_all_tokens(df_all: pd.DataFrame,\n",
    "                     token_df: pd.DataFrame,\n",
    "                     model_id: str,\n",
    "                     rep_mode: str = \"mean\",\n",
    "                     batch_size: int = 4):\n",
    "    \"\"\"\n",
    "    Embed all tokens with per-word piece aggregation.\n",
    "    Returns:\n",
    "      reps: np.ndarray of shape (L, N, D)\n",
    "      words: list[str] length N (token strings for hover)\n",
    "      model_tag: str\n",
    "    \"\"\"\n",
    "    tokzr, model, model_tag = _load_tok_and_model(model_id)\n",
    "\n",
    "    # Build per-sentence index: sid -> list[(global_idx, word_id)]\n",
    "    token_df = token_df.copy()\n",
    "    token_df[\"sentence_id\"] = token_df[\"sentence_id\"].astype(str)\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(token_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(sid, []).append((gidx, int(wid)))\n",
    "\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_all[df_all.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    L = _num_hidden_layers(model) + 1\n",
    "    D = _hidden_size(model)\n",
    "    N = len(token_df)\n",
    "\n",
    "    reps   = np.zeros((L, N, D), np.float16)\n",
    "    words  = [\"\"] * N\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "        enc_kwargs[\"add_prefix_space\"] = True\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{model_tag}: embed ({rep_mode})\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L,B,T,D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                # map word_id -> token positions\n",
    "                mp: Dict[int, List[int]] = {}\n",
    "                wids = enc_be.word_ids(b)\n",
    "                if wids is None:\n",
    "                    raise RuntimeError(\"Fast tokenizer required (word_ids() unavailable).\")\n",
    "                for tidx, wid in enumerate(wids):\n",
    "                    if wid is not None:\n",
    "                        mp.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                toks_for_sent = df_sel.loc[sid, \"tokens\"]\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    tokpos = mp.get(wid)\n",
    "                    if not tokpos:\n",
    "                        continue\n",
    "\n",
    "                    # choose representation per wordpiece policy\n",
    "                    if rep_mode == \"first\":\n",
    "                        vec = h[:, b, tokpos[0], :]              # (L, D)\n",
    "                    elif rep_mode == \"last\":\n",
    "                        vec = h[:, b, tokpos[-1], :]             # (L, D)\n",
    "                    else:  # \"mean\"\n",
    "                        vec = h[:, b, tokpos, :].mean(axis=1)    # (L, D)\n",
    "\n",
    "                    reps[:, gidx, :] = vec.astype(np.float16, copy=False)\n",
    "                    words[gidx] = str(toks_for_sent[wid])\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    if (~filled).any():\n",
    "        missing = int((~filled).sum())\n",
    "        print(f\"⚠ Missing vectors for {missing} tokens (skipped in PCA).\")\n",
    "        reps   = reps[:, filled]\n",
    "        words  = [w for w, f in zip(words, filled) if f]\n",
    "\n",
    "    return reps, words, model_tag\n",
    "\n",
    "# =============================== PCA + PLOTLY ===============================\n",
    "def _layer_color(l: int) -> str:\n",
    "    \"\"\"Pick a per-layer color if LAYER_COLORS is provided; otherwise fall back.\"\"\"\n",
    "    if LAYER_COLORS is not None and l < len(LAYER_COLORS):\n",
    "        return LAYER_COLORS[l]\n",
    "    return MARKER_COLOR\n",
    "\n",
    "def _colorscale_values(Y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute numeric values used for colorscale coloring.\"\"\"\n",
    "    if COLOR_BY == \"pc1\":\n",
    "        return Y[:, 0]\n",
    "    if COLOR_BY == \"pc2\":\n",
    "        return Y[:, 1]\n",
    "    if COLOR_BY == \"pc3\":\n",
    "        return Y[:, 2]\n",
    "    if COLOR_BY == \"radius\":\n",
    "        return np.linalg.norm(Y, axis=1)\n",
    "    raise ValueError(\"COLOR_BY must be one of: 'pc1', 'pc2', 'pc3', 'radius'.\")\n",
    "\n",
    "def pca3d_per_layer_and_plot(reps: np.ndarray, words: List[str], model_tag: str):\n",
    "    \"\"\"\n",
    "    reps: (L, N, D), words: list[str] length N\n",
    "    Creates an interactive 3D Plotly figure with a layer slider.\n",
    "    \"\"\"\n",
    "    L, N, D = reps.shape\n",
    "\n",
    "    # Optional subsampling for plotting\n",
    "    if PCA_MAX_POINTS is None or PCA_MAX_POINTS >= N:\n",
    "        sel_idx = np.arange(N, dtype=np.int64)\n",
    "    else:\n",
    "        sel_idx = np.random.default_rng(RAND_SEED).choice(N, size=PCA_MAX_POINTS, replace=False)\n",
    "        print(f\"⚠ PCA_MAX_POINTS={PCA_MAX_POINTS} < N={N} → plotting a subset.\")\n",
    "\n",
    "    reps_sel  = reps[:, sel_idx, :].astype(np.float32, copy=False)\n",
    "    words_sel = [words[i] for i in sel_idx]\n",
    "\n",
    "    # PCA to 3D for each layer (independently)\n",
    "    Y_layers: List[np.ndarray] = []\n",
    "    for l in range(L):\n",
    "        X = reps_sel[l]  # (n, D)\n",
    "        Xc = X - X.mean(0, keepdims=True)\n",
    "        pca = PCA(n_components=3, random_state=RAND_SEED)\n",
    "        Y = pca.fit_transform(Xc)  # (n, 3)\n",
    "        Y_layers.append(Y)\n",
    "\n",
    "    # Build Plotly figure with a slider to switch layers\n",
    "    traces = []\n",
    "    for l in range(L):\n",
    "        Y = Y_layers[l]\n",
    "\n",
    "        # Choose marker coloring\n",
    "        if COLOR_MODE == \"constant\":\n",
    "            marker = dict(size=MARKER_SIZE, opacity=MARKER_OPACITY, color=MARKER_COLOR)\n",
    "\n",
    "        elif COLOR_MODE == \"per_layer\":\n",
    "            marker = dict(size=MARKER_SIZE, opacity=MARKER_OPACITY, color=_layer_color(l))\n",
    "\n",
    "        elif COLOR_MODE == \"colorscale\":\n",
    "            cvals = _colorscale_values(Y)\n",
    "            marker = dict(\n",
    "                size=MARKER_SIZE,\n",
    "                opacity=MARKER_OPACITY,\n",
    "                color=cvals,\n",
    "                colorscale=COLOR_SCALE,\n",
    "                showscale=SHOW_COLORBAR,\n",
    "                colorbar=dict(\n",
    "                    title=COLOR_BY,\n",
    "                    titlefont=dict(size=AXIS_TITLE_FONT_SIZE),\n",
    "                    tickfont=dict(size=AXIS_TICK_FONT_SIZE),\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"COLOR_MODE must be one of: 'constant', 'per_layer', 'colorscale'.\")\n",
    "\n",
    "        hovertemplate = (\n",
    "            \"<b>%{text}</b><br>\"\n",
    "            \"x=%{x:.3f}<br>y=%{y:.3f}<br>z=%{z:.3f}\"\n",
    "            \"<extra>Layer \" + str(l) + \"</extra>\"\n",
    "        )\n",
    "\n",
    "        traces.append(\n",
    "            go.Scatter3d(\n",
    "                x=Y[:, 0], y=Y[:, 1], z=Y[:, 2],\n",
    "                mode=\"markers\",\n",
    "                marker=marker,\n",
    "                text=words_sel,                 # <-- enables %{text} in hovertemplate\n",
    "                hovertemplate=hovertemplate,\n",
    "                name=f\"Layer {l}\",\n",
    "                visible=(l == 0),\n",
    "                showlegend=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    steps = []\n",
    "    for l in range(L):\n",
    "        vis = [False] * L\n",
    "        vis[l] = True\n",
    "        steps.append(dict(\n",
    "            method=\"update\",\n",
    "            args=[\n",
    "                {\"visible\": vis},\n",
    "                {\"title\": {\n",
    "                    \"text\": f\"{model_tag} • PCA 3D • layer {l} (drag to rotate)\",\n",
    "                    \"font\": {\"size\": TITLE_FONT_SIZE},\n",
    "                }},\n",
    "            ],\n",
    "            label=str(l),\n",
    "        ))\n",
    "\n",
    "    sliders = [dict(\n",
    "        active=0,\n",
    "        steps=steps,\n",
    "        currentvalue={\"prefix\": \"Layer: \", \"font\": {\"size\": SLIDER_FONT_SIZE}},\n",
    "        font={\"size\": SLIDER_FONT_SIZE},\n",
    "        pad={\"t\": 10}\n",
    "    )]\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title={\"text\": f\"{model_tag} • PCA 3D • layer 0 (drag to rotate)\",\n",
    "               \"font\": {\"size\": TITLE_FONT_SIZE}},\n",
    "        font={\"size\": GLOBAL_FONT_SIZE},\n",
    "        hoverlabel={\"font\": {\"size\": HOVER_FONT_SIZE}},\n",
    "\n",
    "        scene=dict(\n",
    "            xaxis=dict(\n",
    "                title=dict(text=\"PC1\", font=dict(size=AXIS_TITLE_FONT_SIZE)),\n",
    "                tickfont=dict(size=AXIS_TICK_FONT_SIZE),\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title=dict(text=\"PC2\", font=dict(size=AXIS_TITLE_FONT_SIZE)),\n",
    "                tickfont=dict(size=AXIS_TICK_FONT_SIZE),\n",
    "            ),\n",
    "            zaxis=dict(\n",
    "                title=dict(text=\"PC3\", font=dict(size=AXIS_TITLE_FONT_SIZE)),\n",
    "                tickfont=dict(size=AXIS_TICK_FONT_SIZE),\n",
    "            ),\n",
    "            aspectmode=\"data\",\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=60),\n",
    "        sliders=sliders,\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "\n",
    "    if SHOW_FIG:\n",
    "        fig.show()\n",
    "\n",
    "    fig.write_html(str(HTML_OUT), include_plotlyjs=\"cdn\")\n",
    "    print(\"✓ Saved interactive HTML to:\", HTML_OUT)\n",
    "\n",
    "# =============================== DRIVER ===============================\n",
    "def run_pca3d_all_tokens():\n",
    "    df_all, token_df = load_word_df(CSV_PATH)\n",
    "    reps, words, tag = embed_all_tokens(\n",
    "        df_all, token_df, MODEL_ID, rep_mode=REP_MODE, batch_size=BATCH_SIZE\n",
    "    )\n",
    "    pca3d_per_layer_and_plot(reps, words, tag)\n",
    "\n",
    "    # Cleanup\n",
    "    del reps\n",
    "    gc.collect()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pca3d_all_tokens()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef4e5116-fd24-4954-ae6b-4245365aba05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ corpus ready — 189,167 tokens across 13 POS\n",
      "• Device=cuda  • DADApy=yes  • IsoScore=fallback\n",
      "• SAFE_MAX_TOKENS_PER_POS=5000  • BATCH_SIZE=1\n",
      "\n",
      "→ POS=ADJ  (sampled 5000 tokens)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-base-uncased embed:  80%|██████████▍  | 2968/3687 [00:21<00:05, 140.26it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 844\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ done.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 844\u001b[0m     \u001b[43mrun_safe_pos_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 742\u001b[0m, in \u001b[0;36mrun_safe_pos_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m→ POS=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  (sampled \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pos_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokens)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 742\u001b[0m reps_pos, filled \u001b[38;5;241m=\u001b[39m \u001b[43membed_subset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokzr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWORD_REP_MODE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    743\u001b[0m pos_df \u001b[38;5;241m=\u001b[39m pos_df\u001b[38;5;241m.\u001b[39mloc[filled]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    744\u001b[0m reps_pos \u001b[38;5;241m=\u001b[39m reps_pos[:, filled]  \u001b[38;5;66;03m# keep only filled vectors\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 406\u001b[0m, in \u001b[0;36membed_subset\u001b[0;34m(df_all_sentences, subset_df, tokzr, model, word_rep_mode, batch_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m enc_be \u001b[38;5;241m=\u001b[39m tokzr(batch_tokens, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menc_kwargs)\n\u001b[1;32m    404\u001b[0m enc_t \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m enc_be\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 406\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menc_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m hs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(out\u001b[38;5;241m.\u001b[39mhidden_states)\u001b[38;5;241m.\u001b[39mdetach()  \u001b[38;5;66;03m# (L,B,T,D)\u001b[39;00m\n\u001b[1;32m    408\u001b[0m hs \u001b[38;5;241m=\u001b[39m hs\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat16)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()       \u001b[38;5;66;03m# store as float16 for safety\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1000\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    998\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1000\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1014\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:650\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    646\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[1;32m    648\u001b[0m layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 650\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    661\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:558\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_value\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.58\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    556\u001b[0m     cache_position: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    557\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 558\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    567\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:488\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_value\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.58\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m     cache_position: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    487\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 488\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    498\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:392\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(current_states)\n\u001b[1;32m    388\u001b[0m         \u001b[38;5;241m.\u001b[39mview(bsz, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_head_size)\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    390\u001b[0m     )\n\u001b[1;32m    391\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 392\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;241m.\u001b[39mview(bsz, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_head_size)\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    395\u001b[0m     )\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;66;03m# save all key/value_layer to cache to be re-used for fast auto-regressive generation\u001b[39;00m\n\u001b[1;32m    399\u001b[0m         cache_position \u001b[38;5;241m=\u001b[39m cache_position \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SAFE POS-by-POS representation-geometry pipeline (BERT + bootstrap + stability)\n",
    "\n",
    "Key safety design:\n",
    "- Embed + compute metrics ONE POS at a time (caps memory).\n",
    "- Avoid global reps[:, idx] slicing that would make giant copies (NumPy advanced indexing).\n",
    "- Keep M capped for heavy kNN metrics (GRIDE), and keep batch sizes small.\n",
    "\n",
    "Outputs:\n",
    "- tables_POS/pos_bootstrap/pos_raw_<metric>_<model>.csv\n",
    "- results_POS/raw_<metric>_<model>.png\n",
    "- results_POS/stability/ stability curves CSV+PNG (optional)\n",
    "- results_POS/run_metadata.json\n",
    "\n",
    "Notes:\n",
    "- For GRIDE, requires dadapy. If missing, the script will skip GRIDE gracefully.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os, gc, ast, json, time, random, inspect, contextlib\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# OPTIONAL DEPENDENCIES\n",
    "# ===========================\n",
    "HAS_DADAPY = False\n",
    "try:\n",
    "    from dadapy import Data\n",
    "    HAS_DADAPY = True\n",
    "except Exception:\n",
    "    HAS_DADAPY = False\n",
    "\n",
    "HAS_SKDIM = False\n",
    "try:\n",
    "    from skdim.id import (\n",
    "        MOM, TLE, CorrInt, FisherS, lPCA,\n",
    "        MLE, DANCo, ESS, MiND_ML, MADA, KNN\n",
    "    )\n",
    "    HAS_SKDIM = True\n",
    "except Exception:\n",
    "    HAS_SKDIM = False\n",
    "\n",
    "# IsoScore is optional; we keep a fallback\n",
    "_HAS_ISOSCORE = False\n",
    "IsoScore = None\n",
    "try:\n",
    "    from isoscore import IsoScore as _IsoScoreObj  # type: ignore\n",
    "    IsoScore = _IsoScoreObj\n",
    "    _HAS_ISOSCORE = True\n",
    "except Exception:\n",
    "    _HAS_ISOSCORE = False\n",
    "\n",
    "if not _HAS_ISOSCORE:\n",
    "    class _IsoScoreFallback:\n",
    "        @staticmethod\n",
    "        def IsoScore(X: np.ndarray) -> float:\n",
    "            C = np.cov(X.T, ddof=0)\n",
    "            ev = np.linalg.eigvalsh(C)\n",
    "            if ev.mean() <= 0 or ev[-1] <= 0:\n",
    "                return 0.0\n",
    "            return float(np.clip(ev.mean() / ev[-1], 0.0, 1.0))\n",
    "    IsoScore = _IsoScoreFallback()\n",
    "\n",
    "def _isoscore_call(X: np.ndarray) -> float:\n",
    "    obj = IsoScore\n",
    "    if obj is None:\n",
    "        return float(\"nan\")\n",
    "    if hasattr(obj, \"IsoScore\") and callable(getattr(obj, \"IsoScore\")):\n",
    "        return float(obj.IsoScore(X))\n",
    "    if callable(obj):\n",
    "        return float(obj(X))\n",
    "    return float(\"nan\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# CONFIG (SAFE DEFAULTS)\n",
    "# ===========================\n",
    "CSV_PATH = \"en_ewt-ud-train_sentences.csv\"\n",
    "BASELINE = \"bert-base-uncased\"\n",
    "\n",
    "# Representation choice for wordpieces\n",
    "WORD_REP_MODE = \"first\"  # \"first\" or \"mean\"\n",
    "\n",
    "# POS filtering\n",
    "EXCLUDE_POS = {\"X\", \"SYM\", \"PART\", \"INTJ\"}\n",
    "\n",
    "# SAFETY: cap how many tokens you embed PER POS (very important)\n",
    "SAFE_MAX_TOKENS_PER_POS = 5000  # <= 5000 recommended if running GRIDE\n",
    "\n",
    "# Bootstrap\n",
    "N_BOOTSTRAP_FAST = 50\n",
    "N_BOOTSTRAP_HEAVY = 200\n",
    "\n",
    "# For heavy metrics, cap the resample size M (critical for kNN methods)\n",
    "HEAVY_BS_MAX_SAMP_PER_POS = 5000\n",
    "\n",
    "# GRIDE: maximum neighbor rank (maxk) for distances and scaling ID\n",
    "DADAPY_GRID_RANGE_MAX = 64\n",
    "\n",
    "# Numerical stability\n",
    "EPS = 1e-9\n",
    "JITTER_EPS = 1e-6\n",
    "\n",
    "# Reproducibility\n",
    "RAND_SEED = 42\n",
    "\n",
    "# Embedding throughput (keep small for safety)\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# What metrics to compute\n",
    "# (You can extend this list; this safe script keeps your default GRIDE example)\n",
    "ALL_METRICS = [\"iso\"]  # e.g. [\"iso\",\"sf\",\"gride\"]\n",
    "\n",
    "# Optional: stability curves (how many tokens M and how many bootstraps B)\n",
    "RUN_STABILITY = True\n",
    "STAB_DIR = Path(\"results_POS\") / \"stability\"\n",
    "STAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "STAB_TOPK_POS = 3              # run stability on top-K most frequent POS\n",
    "STAB_M_MIN = 200\n",
    "STAB_M_MAX = 5000\n",
    "STAB_M_GRID_POINTS = 8\n",
    "STAB_M_REPS = 20\n",
    "STAB_M_REPLACE = False         # recommended for kNN ID stability curves\n",
    "STAB_TOL_CI_HALFWIDTH = 1.0    # GRIDE: “stable within ±1 dim” default\n",
    "STAB_STABLE_FOR = 2\n",
    "\n",
    "RUN_B_CONVERGENCE = True\n",
    "STAB_B_MAX = 400\n",
    "STAB_B_GRID = [25, 50, 100, 200, 300, 400]\n",
    "STAB_B_REPLACE = True          # bootstrap-with-replacement by definition\n",
    "\n",
    "# Output dirs\n",
    "PLOT_DIR = Path(\"results_POS\"); PLOT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CSV_DIR = Path(\"tables_POS\") / \"pos_bootstrap\"; CSV_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Style\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "# Device\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED)\n",
    "np.random.seed(RAND_SEED)\n",
    "torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# HELPERS\n",
    "# ===========================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _center(X: np.ndarray) -> np.ndarray:\n",
    "    return X - X.mean(0, keepdims=True)\n",
    "\n",
    "def _eigvals_from_X(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Eigenvalues of (Xc^T Xc) up to a constant via SVD of centered X.\n",
    "    Pads with zeros to length D so spectral metrics behave consistently.\n",
    "    \"\"\"\n",
    "    Xc = _center(X.astype(np.float32, copy=False))\n",
    "    try:\n",
    "        _, S, _ = np.linalg.svd(Xc, full_matrices=False)\n",
    "        lam = (S**2).astype(np.float64)  # length min(N,D)\n",
    "        D = Xc.shape[1]\n",
    "        if lam.size < D:\n",
    "            lam = np.concatenate([lam, np.zeros(D - lam.size, dtype=np.float64)])\n",
    "        lam.sort()\n",
    "        return lam[::-1]\n",
    "    except Exception:\n",
    "        return np.array([], dtype=np.float64)\n",
    "\n",
    "def _jitter_unique(X: np.ndarray, eps: float = JITTER_EPS) -> np.ndarray:\n",
    "    \"\"\"Add tiny noise if there are duplicate rows (helps NN-based estimators).\"\"\"\n",
    "    try:\n",
    "        if np.unique(X, axis=0).shape[0] < X.shape[0]:\n",
    "            X = X + np.random.normal(scale=eps, size=X.shape).astype(X.dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return X\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# METRICS (single-value on X)\n",
    "# ===========================\n",
    "# --- Isotropy (fast) ---\n",
    "def _iso_once(X: np.ndarray) -> float:\n",
    "    return float(_isoscore_call(X))\n",
    "\n",
    "def _spect_once(X: np.ndarray) -> float:\n",
    "    ev = np.linalg.eigvalsh(np.cov(X.T, ddof=0))\n",
    "    return float(ev[-1] / (ev.mean() + EPS))\n",
    "\n",
    "def _rand_once(X: np.ndarray, K: int = 2000) -> float:\n",
    "    n = X.shape[0]\n",
    "    if n < 2:\n",
    "        return np.nan\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    K_eff = min(K, (n * (n - 1)) // 2)\n",
    "    i = rng.integers(0, n, size=K_eff)\n",
    "    j = rng.integers(0, n, size=K_eff)\n",
    "    same = i == j\n",
    "    if same.any():\n",
    "        j[same] = rng.integers(0, n, size=same.sum())\n",
    "    A, B = X[i], X[j]\n",
    "    num = np.sum(A * B, axis=1)\n",
    "    den = (np.linalg.norm(A, axis=1) * np.linalg.norm(B, axis=1) + EPS)\n",
    "    return float(np.mean(np.abs(num / den)))\n",
    "\n",
    "def _sf_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0:\n",
    "        return np.nan\n",
    "    gm = np.exp(np.mean(np.log(lam + EPS)))\n",
    "    am = float(lam.mean() + EPS)\n",
    "    return float(gm / am)\n",
    "\n",
    "def _vmf_kappa_once(X: np.ndarray) -> float:\n",
    "    if X.shape[0] < 2:\n",
    "        return np.nan\n",
    "    Xn = X / (np.linalg.norm(X, axis=1, keepdims=True) + EPS)\n",
    "    R = np.linalg.norm(Xn.mean(axis=0))\n",
    "    d = Xn.shape[1]\n",
    "    if R < EPS:\n",
    "        return 0.0\n",
    "    return float(max(R * (d - R**2) / (1.0 - R**2 + EPS), 0.0))\n",
    "\n",
    "# --- Linear ID (fast) ---\n",
    "def _erank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0:\n",
    "        return np.nan\n",
    "    p = lam / (lam.sum() + EPS)\n",
    "    H = -(p * np.log(p + EPS)).sum()\n",
    "    return float(np.exp(H))\n",
    "\n",
    "def _pr_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0:\n",
    "        return np.nan\n",
    "    s1 = lam.sum()\n",
    "    s2 = (lam**2).sum()\n",
    "    return float((s1**2) / (s2 + EPS))\n",
    "\n",
    "def _stable_rank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0:\n",
    "        return np.nan\n",
    "    return float(lam.sum() / (lam.max() + EPS))\n",
    "\n",
    "# --- Non-linear (heavy) ---\n",
    "def _dadapy_gride_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY:\n",
    "        return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    d.compute_distances(maxk=DADAPY_GRID_RANGE_MAX)\n",
    "    ids, _, _ = d.return_id_scaling_gride(range_max=DADAPY_GRID_RANGE_MAX)\n",
    "    return float(ids[-1])\n",
    "\n",
    "# Registries\n",
    "FAST_ONCE: Dict[str, Callable[[np.ndarray], float]] = {\n",
    "    \"iso\": _iso_once,\n",
    "    \"spect\": _spect_once,\n",
    "    \"rand\": _rand_once,\n",
    "    \"sf\": _sf_once,\n",
    "    \"vmf_kappa\": _vmf_kappa_once,\n",
    "    \"erank\": _erank_once,\n",
    "    \"pr\": _pr_once,\n",
    "    \"stable_rank\": _stable_rank_once,\n",
    "}\n",
    "\n",
    "HEAVY_ONCE: Dict[str, Optional[Callable[[np.ndarray], float]]] = {\n",
    "    \"gride\": _dadapy_gride_once,\n",
    "}\n",
    "\n",
    "LABELS = {\n",
    "    \"iso\": \"IsoScore\",\n",
    "    \"spect\": \"Spectral Ratio\",\n",
    "    \"rand\": \"RandCos |μ|\",\n",
    "    \"sf\": \"Spectral Flatness\",\n",
    "    \"vmf_kappa\": \"vMF κ\",\n",
    "    \"erank\": \"Effective Rank\",\n",
    "    \"pr\": \"Participation Ratio\",\n",
    "    \"stable_rank\": \"Stable Rank\",\n",
    "    \"gride\": \"GRIDE\",\n",
    "}\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# DATA\n",
    "# ===========================\n",
    "def load_word_df(csv_path: str, exclude_pos: set[str] = EXCLUDE_POS):\n",
    "    df = pd.read_csv(csv_path, usecols=[\"sentence_id\", \"tokens\", \"pos\"])\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df.tokens = df.tokens.apply(_to_list)\n",
    "    df.pos = df.pos.apply(_to_list)\n",
    "\n",
    "    # explode to token-level\n",
    "    rows = []\n",
    "    for sid, toks, poss in df[[\"sentence_id\", \"tokens\", \"pos\"]].itertuples(index=False):\n",
    "        for wid, (tok, p) in enumerate(zip(toks, poss)):\n",
    "            if p not in exclude_pos:\n",
    "                rows.append((sid, wid, p, tok))\n",
    "    word_df = pd.DataFrame(rows, columns=[\"sentence_id\", \"word_id\", \"pos\", \"word\"])\n",
    "    return df, word_df\n",
    "\n",
    "def sample_pos(word_df: pd.DataFrame, pos: str, cap: int) -> pd.DataFrame:\n",
    "    sub = word_df[word_df.pos == pos]\n",
    "    if len(sub) == 0:\n",
    "        return sub\n",
    "    n = min(len(sub), cap)\n",
    "    return sub.sample(n=n, random_state=RAND_SEED, replace=False).reset_index(drop=True)\n",
    "\n",
    "def make_class_palette(classes: List[str]) -> Dict[str, Tuple[float, float, float]]:\n",
    "    base_colors: List[Tuple[float, float, float]] = []\n",
    "    for name in (\"tab20\", \"tab20b\", \"tab20c\"):\n",
    "        try:\n",
    "            base_colors.extend(sns.color_palette(name, 20))\n",
    "        except Exception:\n",
    "            pass\n",
    "    if len(base_colors) < len(classes):\n",
    "        base_colors = list(sns.color_palette(\"husl\", len(classes)))\n",
    "    ordered = list(sorted(classes))\n",
    "    return {cls: base_colors[i % len(base_colors)] for i, cls in enumerate(ordered)}\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# EMBEDDING (POS subset)\n",
    "# ===========================\n",
    "def build_tokenizer_and_model(baseline: str):\n",
    "    # tokenizer (bert-base-uncased may not accept add_prefix_space in some versions)\n",
    "    try:\n",
    "        tokzr = AutoTokenizer.from_pretrained(baseline, use_fast=True, add_prefix_space=True)\n",
    "    except TypeError:\n",
    "        tokzr = AutoTokenizer.from_pretrained(baseline, use_fast=True)\n",
    "\n",
    "    model = AutoModel.from_pretrained(baseline, output_hidden_states=True).eval().to(device)\n",
    "    if device == \"cuda\":\n",
    "        model.half()\n",
    "    return tokzr, model\n",
    "\n",
    "def embed_subset(\n",
    "    df_all_sentences: pd.DataFrame,\n",
    "    subset_df: pd.DataFrame,\n",
    "    tokzr: AutoTokenizer,\n",
    "    model: AutoModel,\n",
    "    word_rep_mode: str = WORD_REP_MODE,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Return reps (L,N,D) float16 and filled mask (N,) for the selected tokens (subset_df rows).\n",
    "    Safe: N is capped per POS, so reps is bounded.\n",
    "    \"\"\"\n",
    "    df_all_sentences[\"sentence_id\"] = df_all_sentences[\"sentence_id\"].astype(str)\n",
    "    subset_df[\"sentence_id\"] = subset_df[\"sentence_id\"].astype(str)\n",
    "\n",
    "    # sid -> list[(global_idx, word_id)]\n",
    "    by_sid: Dict[str, List[Tuple[int, int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(subset_df[[\"sentence_id\", \"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(str(sid), []).append((gidx, int(wid)))\n",
    "\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (\n",
    "        df_all_sentences[df_all_sentences.sentence_id.isin(sids)]\n",
    "        .drop_duplicates(\"sentence_id\")\n",
    "        .set_index(\"sentence_id\")\n",
    "        .loc[sids]\n",
    "    )\n",
    "\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
    "    if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "        enc_kwargs[\"add_prefix_space\"] = True\n",
    "\n",
    "    L = model.config.num_hidden_layers + 1\n",
    "    D = model.config.hidden_size\n",
    "    N = len(subset_df)\n",
    "\n",
    "    reps = np.zeros((L, N, D), np.float16)\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    amp_ctx = torch.autocast(\"cuda\", dtype=torch.float16) if device == \"cuda\" else contextlib.nullcontext()\n",
    "\n",
    "    with torch.inference_mode(), amp_ctx:\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{BASELINE} embed\"):\n",
    "            batch_ids = sids[start: start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t = {k: v.to(device) for k, v in enc_be.items()}\n",
    "\n",
    "            out = model(**enc_t)\n",
    "            hs = torch.stack(out.hidden_states).detach()  # (L,B,T,D)\n",
    "            hs = hs.to(torch.float16).cpu().numpy()       # store as float16 for safety\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                mp: Dict[int, List[int]] = {}\n",
    "                for tidx, wid in enumerate(enc_be.word_ids(b)):\n",
    "                    if wid is not None:\n",
    "                        mp.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = mp.get(wid)\n",
    "                    if not toks:\n",
    "                        continue\n",
    "                    if word_rep_mode == \"first\":\n",
    "                        vec = hs[:, b, toks[0], :]                 # (L,D)\n",
    "                    else:\n",
    "                        vec = hs[:, b, toks, :].mean(axis=1)       # (L,D)\n",
    "                    reps[:, gidx, :] = vec.astype(np.float16, copy=False)\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            # cleanup batch\n",
    "            del enc_be, enc_t, out, hs\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    missing = int((~filled).sum())\n",
    "    if missing:\n",
    "        print(f\"⚠ Missing vectors for {missing} of {N} sampled words\")\n",
    "\n",
    "    return reps, filled\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# BOOTSTRAP (SAFE, SMALL)\n",
    "# ===========================\n",
    "def bs_layer_loop(\n",
    "    reps_pos: np.ndarray,  # (L,N,D) for ONE POS\n",
    "    M: int,\n",
    "    n_reps: int,\n",
    "    compute_once: Callable[[np.ndarray], float],\n",
    "    replace: bool = True,\n",
    "    seed: int = RAND_SEED,\n",
    "    layers_idx: Optional[List[int]] = None,\n",
    "    return_reps: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Safe: reps_pos is bounded by SAFE_MAX_TOKENS_PER_POS.\n",
    "    \"\"\"\n",
    "    L, N, D = reps_pos.shape\n",
    "    if layers_idx is None:\n",
    "        layers_idx = list(range(L))\n",
    "    layers_idx = [int(x) for x in layers_idx]\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    A = np.full((n_reps, len(layers_idx)), np.nan, np.float32)\n",
    "\n",
    "    M_eff = min(int(M), int(N))\n",
    "    for r in range(n_reps):\n",
    "        if replace:\n",
    "            idx = rng.integers(0, N, size=M_eff)\n",
    "        else:\n",
    "            idx = rng.choice(N, size=M_eff, replace=False)\n",
    "\n",
    "        for j, l in enumerate(layers_idx):\n",
    "            X = reps_pos[l, idx].astype(np.float32, copy=False)  # small (M_eff,D)\n",
    "            try:\n",
    "                A[r, j] = float(compute_once(X))\n",
    "            except Exception:\n",
    "                A[r, j] = np.nan\n",
    "\n",
    "    mu = np.nanmean(A, axis=0).astype(np.float32)\n",
    "    lo = np.nanpercentile(A, 2.5, axis=0).astype(np.float32)\n",
    "    hi = np.nanpercentile(A, 97.5, axis=0).astype(np.float32)\n",
    "\n",
    "    if return_reps:\n",
    "        return mu, lo, hi, A\n",
    "    return mu, lo, hi\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# SAVE / PLOT (same style as you had)\n",
    "# ===========================\n",
    "def save_metric_csv_all_pos(metric: str,\n",
    "                            pos_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                            layers: np.ndarray,\n",
    "                            baseline: str,\n",
    "                            subset_name: str = \"raw\"):\n",
    "    rows = []\n",
    "    for p, stats in pos_to_stats.items():\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        n = stats.get(\"n\", 0)\n",
    "        for l, val in enumerate(mu):\n",
    "            rows.append({\n",
    "                \"subset\": subset_name,\n",
    "                \"model\": baseline,\n",
    "                \"feature\": \"pos\",\n",
    "                \"class\": p,\n",
    "                \"metric\": metric,\n",
    "                \"layer\": int(layers[l]),\n",
    "                \"mean\": float(val) if np.isfinite(val) else np.nan,\n",
    "                \"ci_low\": float(lo[l]) if isinstance(lo, np.ndarray) and np.isfinite(lo[l]) else np.nan,\n",
    "                \"ci_high\": float(hi[l]) if isinstance(hi, np.ndarray) and np.isfinite(hi[l]) else np.nan,\n",
    "                \"n_tokens\": int(n),\n",
    "                \"word_rep_mode\": WORD_REP_MODE,\n",
    "                \"source_csv\": Path(CSV_PATH).name,\n",
    "                \"seed\": int(RAND_SEED),\n",
    "                \"safe_max_tokens_per_pos\": int(SAFE_MAX_TOKENS_PER_POS),\n",
    "                \"heavy_cap\": int(HEAVY_BS_MAX_SAMP_PER_POS),\n",
    "                \"gride_range_max\": int(DADAPY_GRID_RANGE_MAX),\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    out = CSV_DIR / f\"pos_{subset_name}_{metric}_{baseline}.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "\n",
    "def plot_metric_with_ci(pos_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                        layers: np.ndarray, metric: str, title: str, out_path: Path,\n",
    "                        palette: Optional[Dict[str, Tuple[float, float, float]]] = None):\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    for p, stats in pos_to_stats.items():\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        if mu is None or np.all(np.isnan(mu)):\n",
    "            continue\n",
    "        color = palette.get(p) if isinstance(palette, dict) else None\n",
    "        plt.plot(layers, mu, label=p, lw=1.8, color=color)\n",
    "        if isinstance(lo, np.ndarray) and isinstance(hi, np.ndarray) and not np.all(np.isnan(lo)):\n",
    "            plt.fill_between(layers, lo, hi, alpha=0.15, color=color)\n",
    "\n",
    "    plt.xlabel(\"Layer\")\n",
    "    plt.ylabel(LABELS.get(metric, metric.upper()))\n",
    "    plt.title(title)\n",
    "\n",
    "    n_classes = len(pos_to_stats)\n",
    "    ncol = 3 if n_classes > 12 else 2\n",
    "    plt.legend(ncol=ncol, fontsize=\"small\", title=\"POS\", frameon=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# STABILITY UTILITIES (M-curve + B-curve)\n",
    "# ===========================\n",
    "def make_M_grid(N: int, min_M=200, max_M=5000, n_points=8) -> List[int]:\n",
    "    max_M = min(int(max_M), int(N))\n",
    "    min_M = min(int(min_M), max_M)\n",
    "    if max_M <= 2:\n",
    "        return [max_M]\n",
    "    if max_M <= min_M:\n",
    "        return [max_M]\n",
    "    Ms = np.unique(np.round(np.geomspace(min_M, max_M, n_points)).astype(int))\n",
    "    Ms = Ms[Ms >= 2]\n",
    "    if Ms.size == 0:\n",
    "        return [max_M]\n",
    "    if Ms[-1] != max_M:\n",
    "        Ms = np.unique(np.r_[Ms, max_M])\n",
    "    return Ms.tolist()\n",
    "\n",
    "def sweep_M(reps_pos: np.ndarray, Ms: List[int], compute_once, n_reps: int, replace: bool, layers_idx: List[int]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    N = reps_pos.shape[1]\n",
    "    for M in Ms:\n",
    "        M_eff = min(int(M), int(N))\n",
    "        mu, lo, hi = bs_layer_loop(reps_pos, M_eff, n_reps, compute_once, replace=replace, layers_idx=layers_idx, seed=RAND_SEED + M_eff)\n",
    "        for j, l in enumerate(layers_idx):\n",
    "            rows.append({\n",
    "                \"M\": int(M_eff),\n",
    "                \"layer\": int(l),\n",
    "                \"mean\": float(mu[j]),\n",
    "                \"ci_low\": float(lo[j]),\n",
    "                \"ci_high\": float(hi[j]),\n",
    "                \"ci_width\": float(hi[j] - lo[j]),\n",
    "                \"n_reps\": int(n_reps),\n",
    "                \"replace\": bool(replace),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def min_M_for_tolerance(dfM: pd.DataFrame, layer: int, tol_ci_halfwidth: float, stable_for: int = 2) -> Optional[int]:\n",
    "    g = dfM[dfM.layer == int(layer)].sort_values(\"M\")\n",
    "    if g.empty:\n",
    "        return None\n",
    "    Ms = g[\"M\"].to_numpy()\n",
    "    hw = (g[\"ci_width\"].to_numpy() / 2.0)\n",
    "    ok = hw <= float(tol_ci_halfwidth)\n",
    "\n",
    "    for i in range(len(Ms)):\n",
    "        if ok[i] and (i + stable_for - 1) < len(Ms) and ok[i:i + stable_for].all():\n",
    "            return int(Ms[i])\n",
    "    return None\n",
    "\n",
    "def plot_M_curve(dfM: pd.DataFrame, metric_label: str, out_path: Path, title: str = \"\"):\n",
    "    plt.figure(figsize=(7.6, 4.4))\n",
    "    for layer, g in dfM.groupby(\"layer\"):\n",
    "        g = g.sort_values(\"M\")\n",
    "        plt.plot(g[\"M\"], g[\"mean\"], marker=\"o\", lw=1.6, label=f\"layer {layer}\")\n",
    "        plt.fill_between(g[\"M\"], g[\"ci_low\"], g[\"ci_high\"], alpha=0.15)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Tokens per POS (M, log scale)\")\n",
    "    plt.ylabel(metric_label)\n",
    "    plt.title(title or f\"{metric_label} stability vs M\")\n",
    "    plt.legend(frameon=False, fontsize=\"small\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "def sweep_B(reps_pos: np.ndarray, M: int, compute_once, B_max: int, B_grid: List[int], replace: bool, layers_idx: List[int]) -> pd.DataFrame:\n",
    "    mu, lo, hi, A = bs_layer_loop(reps_pos, M, B_max, compute_once, replace=replace, layers_idx=layers_idx, seed=RAND_SEED + 999, return_reps=True)\n",
    "    rows = []\n",
    "    for B in B_grid:\n",
    "        b = min(int(B), int(B_max))\n",
    "        Ab = A[:b]\n",
    "        mub = np.nanmean(Ab, axis=0)\n",
    "        lob = np.nanpercentile(Ab, 2.5, axis=0)\n",
    "        hib = np.nanpercentile(Ab, 97.5, axis=0)\n",
    "        for j, l in enumerate(layers_idx):\n",
    "            rows.append({\n",
    "                \"B\": int(b),\n",
    "                \"layer\": int(l),\n",
    "                \"mean\": float(mub[j]),\n",
    "                \"ci_low\": float(lob[j]),\n",
    "                \"ci_high\": float(hib[j]),\n",
    "                \"ci_width\": float(hib[j] - lob[j]),\n",
    "                \"replace\": bool(replace),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def plot_B_curve(dfB: pd.DataFrame, metric_label: str, out_path: Path, title: str = \"\"):\n",
    "    plt.figure(figsize=(7.6, 4.4))\n",
    "    for layer, g in dfB.groupby(\"layer\"):\n",
    "        g = g.sort_values(\"B\")\n",
    "        plt.plot(g[\"B\"], g[\"mean\"], marker=\"o\", lw=1.6, label=f\"layer {layer}\")\n",
    "        plt.fill_between(g[\"B\"], g[\"ci_low\"], g[\"ci_high\"], alpha=0.15)\n",
    "    plt.xlabel(\"Bootstrap replicates (B)\")\n",
    "    plt.ylabel(metric_label)\n",
    "    plt.title(title or f\"{metric_label} stability vs B\")\n",
    "    plt.legend(frameon=False, fontsize=\"small\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# METADATA DUMP\n",
    "# ===========================\n",
    "def dump_run_metadata(out_path: Path):\n",
    "    meta = {\n",
    "        \"time\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"csv_path\": str(CSV_PATH),\n",
    "        \"baseline\": BASELINE,\n",
    "        \"word_rep_mode\": WORD_REP_MODE,\n",
    "        \"exclude_pos\": sorted(list(EXCLUDE_POS)),\n",
    "        \"seed\": int(RAND_SEED),\n",
    "        \"device\": device,\n",
    "        \"batch_size\": int(BATCH_SIZE),\n",
    "        \"safe_max_tokens_per_pos\": int(SAFE_MAX_TOKENS_PER_POS),\n",
    "        \"bootstrap\": {\n",
    "            \"N_BOOTSTRAP_FAST\": int(N_BOOTSTRAP_FAST),\n",
    "            \"N_BOOTSTRAP_HEAVY\": int(N_BOOTSTRAP_HEAVY),\n",
    "            \"heavy_cap\": int(HEAVY_BS_MAX_SAMP_PER_POS),\n",
    "            \"ci_percentiles\": [2.5, 97.5],\n",
    "        },\n",
    "        \"gride\": {\n",
    "            \"range_max\": int(DADAPY_GRID_RANGE_MAX),\n",
    "            \"summary\": \"ids[-1] (largest scale)\",\n",
    "            \"dadapy_available\": bool(HAS_DADAPY),\n",
    "        },\n",
    "        \"stability\": {\n",
    "            \"RUN_STABILITY\": bool(RUN_STABILITY),\n",
    "            \"STAB_TOPK_POS\": int(STAB_TOPK_POS),\n",
    "            \"M_grid\": [int(STAB_M_MIN), int(STAB_M_MAX), int(STAB_M_GRID_POINTS)],\n",
    "            \"M_reps\": int(STAB_M_REPS),\n",
    "            \"M_replace\": bool(STAB_M_REPLACE),\n",
    "            \"tol_ci_halfwidth\": float(STAB_TOL_CI_HALFWIDTH),\n",
    "            \"stable_for\": int(STAB_STABLE_FOR),\n",
    "            \"RUN_B_CONVERGENCE\": bool(RUN_B_CONVERGENCE),\n",
    "            \"B_max\": int(STAB_B_MAX),\n",
    "            \"B_grid\": STAB_B_GRID,\n",
    "        },\n",
    "        \"packages\": {},\n",
    "    }\n",
    "\n",
    "    # best-effort versions\n",
    "    try:\n",
    "        import numpy, pandas, transformers\n",
    "        meta[\"packages\"][\"numpy\"] = numpy.__version__\n",
    "        meta[\"packages\"][\"pandas\"] = pandas.__version__\n",
    "        meta[\"packages\"][\"torch\"] = torch.__version__\n",
    "        meta[\"packages\"][\"transformers\"] = transformers.__version__\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if HAS_DADAPY:\n",
    "            import dadapy\n",
    "            meta[\"packages\"][\"dadapy\"] = getattr(dadapy, \"__version__\", \"unknown\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    out_path.write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# MAIN (SAFE POS-by-POS)\n",
    "# ===========================\n",
    "def run_safe_pos_pipeline():\n",
    "    dump_run_metadata(PLOT_DIR / \"run_metadata.json\")\n",
    "\n",
    "    # 1) load data\n",
    "    df_all, word_df = load_word_df(CSV_PATH, EXCLUDE_POS)\n",
    "    POS_TAGS = sorted(word_df.pos.unique().tolist())\n",
    "    palette = make_class_palette(POS_TAGS)\n",
    "\n",
    "    print(f\"✓ corpus ready — {len(word_df):,} tokens across {len(POS_TAGS)} POS\")\n",
    "    print(f\"• Device={device}  • DADApy={'yes' if HAS_DADAPY else 'no'}  • IsoScore={'pkg' if _HAS_ISOSCORE else 'fallback'}\")\n",
    "    print(f\"• SAFE_MAX_TOKENS_PER_POS={SAFE_MAX_TOKENS_PER_POS}  • BATCH_SIZE={BATCH_SIZE}\")\n",
    "\n",
    "    # 2) build model once\n",
    "    tokzr, model = build_tokenizer_and_model(BASELINE)\n",
    "    L = model.config.num_hidden_layers + 1\n",
    "    layers = np.arange(L)\n",
    "\n",
    "    # 3) compute metrics POS-by-POS (safe)\n",
    "    metric_to_posstats: Dict[str, Dict[str, Dict[str, np.ndarray]]] = {m: {} for m in ALL_METRICS}\n",
    "\n",
    "    # for stability: choose top-K POS by frequency\n",
    "    pos_counts = word_df.pos.value_counts()\n",
    "    stab_pos_list = pos_counts.head(STAB_TOPK_POS).index.tolist()\n",
    "\n",
    "    for pos in POS_TAGS:\n",
    "        # sample for this POS only (safe cap)\n",
    "        pos_df = sample_pos(word_df, pos, cap=SAFE_MAX_TOKENS_PER_POS)\n",
    "        if len(pos_df) < 3:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n→ POS={pos}  (sampled {len(pos_df)} tokens)\")\n",
    "\n",
    "        reps_pos, filled = embed_subset(df_all, pos_df, tokzr, model, WORD_REP_MODE, BATCH_SIZE)\n",
    "        pos_df = pos_df.loc[filled].reset_index(drop=True)\n",
    "        reps_pos = reps_pos[:, filled]  # keep only filled vectors\n",
    "        Np = reps_pos.shape[1]\n",
    "        if Np < 3:\n",
    "            print(\"  (skipping: too few embedded tokens)\")\n",
    "            del reps_pos\n",
    "            gc.collect()\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "        # compute each metric for this POS\n",
    "        for metric in ALL_METRICS:\n",
    "            if metric in FAST_ONCE:\n",
    "                compute_once = FAST_ONCE[metric]\n",
    "                n_bs = N_BOOTSTRAP_FAST\n",
    "                M = Np  # fast metrics can use all\n",
    "            else:\n",
    "                compute_once = HEAVY_ONCE.get(metric)\n",
    "                n_bs = N_BOOTSTRAP_HEAVY\n",
    "                M = min(Np, HEAVY_BS_MAX_SAMP_PER_POS)\n",
    "\n",
    "            if compute_once is None:\n",
    "                print(f\"  (skipping {metric}: estimator unavailable)\")\n",
    "                continue\n",
    "\n",
    "            mu, lo, hi = bs_layer_loop(reps_pos, M=M, n_reps=n_bs, compute_once=compute_once, replace=True, seed=RAND_SEED)\n",
    "            metric_to_posstats[metric][pos] = {\"mean\": mu, \"lo\": lo, \"hi\": hi, \"n\": np.array([Np])}\n",
    "\n",
    "        # optional stability curves (run on top-K POS only)\n",
    "        if RUN_STABILITY and pos in stab_pos_list:\n",
    "            layers_idx = [0, L // 2, L - 1]\n",
    "            for metric in [m for m in ALL_METRICS if m in HEAVY_ONCE or m in FAST_ONCE]:\n",
    "                if metric in FAST_ONCE:\n",
    "                    compute_once = FAST_ONCE[metric]\n",
    "                    tol = 0.01  # typical for [0,1] scores\n",
    "                else:\n",
    "                    compute_once = HEAVY_ONCE.get(metric)\n",
    "                    tol = STAB_TOL_CI_HALFWIDTH\n",
    "                if compute_once is None:\n",
    "                    continue\n",
    "\n",
    "                Ms = make_M_grid(Np, STAB_M_MIN, STAB_M_MAX, STAB_M_GRID_POINTS)\n",
    "                dfM = sweep_M(reps_pos, Ms, compute_once, STAB_M_REPS, STAB_M_REPLACE, layers_idx)\n",
    "                dfM[\"pos\"] = pos\n",
    "                dfM[\"metric\"] = metric\n",
    "                dfM[\"model\"] = BASELINE\n",
    "                dfM.to_csv(STAB_DIR / f\"stability_M_{metric}_{pos}_{BASELINE}.csv\", index=False)\n",
    "                plot_M_curve(dfM, LABELS.get(metric, metric), STAB_DIR / f\"stability_M_{metric}_{pos}_{BASELINE}.png\",\n",
    "                             title=f\"{LABELS.get(metric, metric)} • POS={pos} • stability vs M\")\n",
    "\n",
    "                # report M* for last layer\n",
    "                m_star = min_M_for_tolerance(dfM, layer=L - 1, tol_ci_halfwidth=tol, stable_for=STAB_STABLE_FOR)\n",
    "                print(f\"  stability({metric}): suggested M* (layer {L-1}) = {m_star}  (tol halfwidth={tol})\")\n",
    "\n",
    "                if RUN_B_CONVERGENCE:\n",
    "                    M_fixed = min(Np, HEAVY_BS_MAX_SAMP_PER_POS) if metric not in FAST_ONCE else Np\n",
    "                    dfB = sweep_B(reps_pos, M_fixed, compute_once, STAB_B_MAX, STAB_B_GRID, STAB_B_REPLACE, layers_idx)\n",
    "                    dfB[\"pos\"] = pos\n",
    "                    dfB[\"metric\"] = metric\n",
    "                    dfB[\"model\"] = BASELINE\n",
    "                    dfB[\"M_fixed\"] = int(M_fixed)\n",
    "                    dfB.to_csv(STAB_DIR / f\"stability_B_{metric}_{pos}_{BASELINE}.csv\", index=False)\n",
    "                    plot_B_curve(dfB, LABELS.get(metric, metric), STAB_DIR / f\"stability_B_{metric}_{pos}_{BASELINE}.png\",\n",
    "                                 title=f\"{LABELS.get(metric, metric)} • POS={pos} • stability vs B (M={M_fixed})\")\n",
    "\n",
    "        # cleanup per POS (critical for kernel safety)\n",
    "        del reps_pos\n",
    "        gc.collect()\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # 4) Save + plot per metric (small objects only)\n",
    "    for metric, pos_to_stats in metric_to_posstats.items():\n",
    "        if not pos_to_stats:\n",
    "            continue\n",
    "        # normalize shape for n\n",
    "        for p in list(pos_to_stats.keys()):\n",
    "            n_arr = pos_to_stats[p].get(\"n\")\n",
    "            if isinstance(n_arr, np.ndarray) and n_arr.size == 1:\n",
    "                pos_to_stats[p][\"n\"] = int(n_arr[0])  # type: ignore\n",
    "\n",
    "        save_metric_csv_all_pos(metric, pos_to_stats, layers, BASELINE, subset_name=\"raw\")\n",
    "        plot_metric_with_ci(pos_to_stats, layers, metric,\n",
    "                            title=f\"{LABELS.get(metric, metric)} • {BASELINE}\",\n",
    "                            out_path=PLOT_DIR / f\"raw_{metric}_{BASELINE}.png\",\n",
    "                            palette=palette)\n",
    "        print(f\"\\n✓ saved metric {metric}:\")\n",
    "        print(f\"  CSV:  {CSV_DIR / f'pos_raw_{metric}_{BASELINE}.csv'}\")\n",
    "        print(f\"  Plot: {PLOT_DIR / f'raw_{metric}_{BASELINE}.png'}\")\n",
    "\n",
    "    # final cleanup\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\n✓ done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_safe_pos_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515caff6-0cb0-40d3-878d-8bcbfaafce32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
