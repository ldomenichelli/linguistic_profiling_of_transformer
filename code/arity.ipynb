{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fccdd2f-009b-403c-a55e-4cd573675633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "import numpy as np, pandas as pd, torch\n",
    "import torch.utils.data as torchdata\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel, BertModel, AutoConfig\n",
    "from IsoScore import IsoScore\n",
    "from dadapy import Data\n",
    "from skdim.id import MLE, MOM, TLE, CorrInt, FisherS, lPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a0f2d-c11a-4284-8e66-375fc9910fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ corpus ready — 194,916 tokens across arity classes ['0', '1', '2', '3', '4']\n",
      "Sample sizes per arity (raw cap):\n",
      "{'0': 50000, '1': 20239, '2': 18037, '3': 13546, '4': 14030}\n",
      "Unique word types per arity (with class 0 capped at 30k tokens):\n",
      "{'0': 4936, '1': 7099, '2': 6408, '3': 4932, '4': 4798}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "bert-base-uncased (embed subset):   0%|               | 0/10067 [00:00<?, ?it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   0%|     | 11/10067 [00:00<01:35, 105.22it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   0%|     | 22/10067 [00:00<01:35, 105.19it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   0%|     | 33/10067 [00:00<01:34, 106.72it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   0%|     | 44/10067 [00:00<01:35, 104.60it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   1%|     | 55/10067 [00:00<01:36, 103.98it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   1%|     | 66/10067 [00:00<01:35, 104.54it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   1%|     | 78/10067 [00:00<01:33, 106.91it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   1%|     | 89/10067 [00:00<01:33, 106.49it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   1%|    | 100/10067 [00:00<01:36, 102.80it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   1%|    | 111/10067 [00:01<01:35, 104.48it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   1%|    | 122/10067 [00:01<01:35, 103.75it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   1%|    | 133/10067 [00:01<01:35, 104.06it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   1%|    | 144/10067 [00:01<01:34, 104.65it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   2%|    | 155/10067 [00:01<01:35, 104.29it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   2%|    | 166/10067 [00:01<01:34, 104.27it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   2%|    | 177/10067 [00:01<01:34, 104.82it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   2%|    | 188/10067 [00:01<01:33, 105.72it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   2%|    | 199/10067 [00:01<01:32, 106.17it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   2%|    | 210/10067 [00:01<01:32, 106.22it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   2%|    | 221/10067 [00:02<01:32, 106.69it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   2%|    | 232/10067 [00:02<01:32, 106.02it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   2%|    | 243/10067 [00:02<01:32, 105.81it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   3%|    | 255/10067 [00:02<01:31, 107.17it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   3%|    | 266/10067 [00:02<01:33, 104.88it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   3%|    | 277/10067 [00:02<01:32, 106.09it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   3%|    | 288/10067 [00:02<01:32, 106.28it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   3%|    | 299/10067 [00:02<01:31, 106.74it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   3%|    | 310/10067 [00:02<01:31, 106.48it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   3%|▏   | 321/10067 [00:03<01:30, 107.44it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   3%|▏   | 332/10067 [00:03<01:30, 106.99it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   3%|▏   | 343/10067 [00:03<01:30, 107.13it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   4%|▏   | 354/10067 [00:03<01:31, 105.80it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   4%|▏   | 365/10067 [00:03<01:30, 106.79it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   4%|▏   | 377/10067 [00:03<01:29, 108.67it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   4%|▏   | 388/10067 [00:03<01:29, 108.22it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   4%|▏   | 399/10067 [00:03<01:29, 108.08it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   4%|▏   | 410/10067 [00:03<01:29, 107.66it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   4%|▏   | 421/10067 [00:03<01:31, 105.30it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   4%|▏   | 432/10067 [00:04<01:31, 105.21it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   4%|▏   | 443/10067 [00:04<01:31, 105.33it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   5%|▏   | 454/10067 [00:04<01:30, 106.67it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   5%|▏   | 466/10067 [00:04<01:29, 107.67it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   5%|▏   | 477/10067 [00:04<01:28, 108.03it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   5%|▏   | 488/10067 [00:04<01:29, 107.61it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   5%|▏   | 499/10067 [00:04<01:29, 107.21it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   5%|▏   | 510/10067 [00:04<01:29, 106.61it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   5%|▏   | 521/10067 [00:04<01:31, 104.71it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   5%|▏   | 533/10067 [00:05<01:28, 107.29it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   5%|▏   | 544/10067 [00:05<01:28, 107.22it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   6%|▏   | 555/10067 [00:05<01:29, 106.61it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   6%|▏   | 566/10067 [00:05<01:31, 104.23it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   6%|▏   | 577/10067 [00:05<01:31, 103.88it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   6%|▏   | 588/10067 [00:05<01:30, 104.88it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   6%|▏   | 599/10067 [00:05<01:29, 105.38it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   6%|▏   | 610/10067 [00:05<01:29, 106.00it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   6%|▏   | 621/10067 [00:05<01:29, 105.30it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   6%|▎   | 632/10067 [00:05<01:28, 106.49it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   6%|▎   | 643/10067 [00:06<01:28, 106.90it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   7%|▎   | 655/10067 [00:06<01:27, 108.13it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   7%|▎   | 666/10067 [00:06<01:28, 106.76it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   7%|▎   | 677/10067 [00:06<01:27, 107.01it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   7%|▎   | 688/10067 [00:06<01:27, 107.64it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   7%|▎   | 699/10067 [00:06<01:26, 108.01it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   7%|▎   | 710/10067 [00:06<01:26, 108.21it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   7%|▎   | 721/10067 [00:06<01:27, 107.36it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   7%|▎   | 732/10067 [00:06<01:27, 106.81it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   7%|▎   | 743/10067 [00:06<01:27, 106.99it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   7%|▎   | 754/10067 [00:07<01:27, 106.03it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   8%|▎   | 765/10067 [00:07<01:26, 106.98it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   8%|▎   | 776/10067 [00:07<01:26, 107.86it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   8%|▎   | 788/10067 [00:07<01:25, 109.12it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   8%|▎   | 799/10067 [00:07<01:25, 108.80it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   8%|▎   | 811/10067 [00:07<01:24, 109.35it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   8%|▎   | 822/10067 [00:07<01:24, 109.50it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   8%|▎   | 834/10067 [00:07<01:24, 109.89it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   8%|▎   | 845/10067 [00:07<01:25, 107.98it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   9%|▎   | 856/10067 [00:08<01:25, 107.14it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   9%|▎   | 867/10067 [00:08<01:25, 107.44it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   9%|▎   | 878/10067 [00:08<01:25, 107.67it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   9%|▎   | 889/10067 [00:08<01:25, 107.51it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   9%|▎   | 900/10067 [00:08<01:25, 107.71it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   9%|▎   | 911/10067 [00:08<01:25, 107.11it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   9%|▎   | 922/10067 [00:08<01:25, 107.44it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   9%|▎   | 934/10067 [00:08<01:24, 108.61it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):   9%|▍   | 946/10067 [00:08<01:23, 109.76it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  10%|▍   | 957/10067 [00:08<01:23, 108.88it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  10%|▍   | 968/10067 [00:09<01:24, 108.25it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  10%|▍   | 979/10067 [00:09<01:23, 108.75it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  10%|▍   | 990/10067 [00:09<01:23, 108.36it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  10%|▎  | 1001/10067 [00:09<01:23, 108.46it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  10%|▎  | 1013/10067 [00:09<01:22, 109.35it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  10%|▎  | 1025/10067 [00:09<01:22, 110.16it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  10%|▎  | 1037/10067 [00:09<01:22, 109.15it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  10%|▎  | 1048/10067 [00:09<01:23, 107.88it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  11%|▎  | 1060/10067 [00:09<01:22, 109.05it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  11%|▎  | 1071/10067 [00:10<01:22, 108.92it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  11%|▎  | 1082/10067 [00:10<01:23, 108.21it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  11%|▎  | 1093/10067 [00:10<01:22, 108.18it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  11%|▎  | 1104/10067 [00:10<01:22, 108.29it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  11%|▎  | 1115/10067 [00:10<01:23, 107.54it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  11%|▎  | 1127/10067 [00:10<01:22, 108.88it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  11%|▎  | 1138/10067 [00:10<01:22, 107.73it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  11%|▎  | 1149/10067 [00:10<01:22, 107.93it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  12%|▎  | 1160/10067 [00:10<01:22, 107.74it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  12%|▎  | 1172/10067 [00:10<01:21, 108.93it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  12%|▎  | 1183/10067 [00:11<01:22, 107.18it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  12%|▎  | 1194/10067 [00:11<01:22, 107.28it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  12%|▎  | 1205/10067 [00:11<01:22, 107.73it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  12%|▎  | 1217/10067 [00:11<01:21, 108.74it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  12%|▎  | 1228/10067 [00:11<01:21, 108.34it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  12%|▎  | 1239/10067 [00:11<01:21, 108.49it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  12%|▎  | 1250/10067 [00:11<01:21, 108.65it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  13%|▍  | 1261/10067 [00:11<01:22, 107.33it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  13%|▍  | 1272/10067 [00:11<01:21, 107.28it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  13%|▍  | 1283/10067 [00:11<01:22, 106.81it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  13%|▍  | 1294/10067 [00:12<01:21, 107.71it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  13%|▍  | 1305/10067 [00:12<01:20, 108.37it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  13%|▍  | 1317/10067 [00:12<01:20, 108.86it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  13%|▍  | 1328/10067 [00:12<01:21, 107.86it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  13%|▍  | 1339/10067 [00:12<01:20, 108.27it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  13%|▍  | 1350/10067 [00:12<01:21, 107.27it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  14%|▍  | 1361/10067 [00:12<01:20, 107.50it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  14%|▍  | 1373/10067 [00:12<01:20, 108.54it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  14%|▍  | 1385/10067 [00:12<01:19, 109.22it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  14%|▍  | 1396/10067 [00:13<01:20, 107.52it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  14%|▍  | 1408/10067 [00:13<01:19, 108.35it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  14%|▍  | 1419/10067 [00:13<01:21, 106.14it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  14%|▍  | 1431/10067 [00:13<01:19, 108.12it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  14%|▍  | 1442/10067 [00:13<01:19, 108.12it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  14%|▍  | 1454/10067 [00:13<01:18, 109.56it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  15%|▍  | 1465/10067 [00:13<01:18, 109.18it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  15%|▍  | 1476/10067 [00:13<01:18, 108.76it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  15%|▍  | 1487/10067 [00:13<01:19, 108.06it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  15%|▍  | 1498/10067 [00:13<01:19, 108.37it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  15%|▍  | 1509/10067 [00:14<01:19, 107.99it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  15%|▍  | 1520/10067 [00:14<01:19, 107.77it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  15%|▍  | 1532/10067 [00:14<01:18, 109.10it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  15%|▍  | 1543/10067 [00:14<01:18, 108.89it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  15%|▍  | 1554/10067 [00:14<01:17, 109.19it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  16%|▍  | 1565/10067 [00:14<01:19, 107.21it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  16%|▍  | 1576/10067 [00:14<01:18, 107.90it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  16%|▍  | 1588/10067 [00:14<01:17, 109.15it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  16%|▍  | 1599/10067 [00:14<01:17, 109.11it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  16%|▍  | 1610/10067 [00:15<01:18, 107.67it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  16%|▍  | 1621/10067 [00:15<01:18, 108.16it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  16%|▍  | 1633/10067 [00:15<01:17, 109.15it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  16%|▍  | 1645/10067 [00:15<01:16, 109.98it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  16%|▍  | 1656/10067 [00:15<01:16, 109.92it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  17%|▍  | 1667/10067 [00:15<01:16, 109.94it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  17%|▌  | 1678/10067 [00:15<01:16, 109.50it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  17%|▌  | 1690/10067 [00:15<01:16, 109.82it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  17%|▌  | 1701/10067 [00:15<01:17, 108.48it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  17%|▌  | 1712/10067 [00:15<01:17, 108.14it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  17%|▌  | 1723/10067 [00:16<01:17, 107.80it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  17%|▌  | 1735/10067 [00:16<01:16, 108.49it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  17%|▌  | 1746/10067 [00:16<01:17, 107.80it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  17%|▌  | 1757/10067 [00:16<01:16, 108.26it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  18%|▌  | 1768/10067 [00:16<01:17, 107.11it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  18%|▌  | 1780/10067 [00:16<01:16, 108.39it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  18%|▌  | 1791/10067 [00:16<01:16, 107.83it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  18%|▌  | 1803/10067 [00:16<01:15, 108.80it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  18%|▌  | 1814/10067 [00:16<01:15, 108.86it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  18%|▌  | 1825/10067 [00:16<01:15, 108.90it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  18%|▌  | 1836/10067 [00:17<01:16, 107.67it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  18%|▌  | 1848/10067 [00:17<01:15, 109.34it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  18%|▌  | 1860/10067 [00:17<01:14, 109.49it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  19%|▌  | 1871/10067 [00:17<01:14, 109.31it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  19%|▌  | 1882/10067 [00:17<01:15, 108.62it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  19%|▌  | 1893/10067 [00:17<01:15, 108.96it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  19%|▌  | 1904/10067 [00:17<01:15, 107.74it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  19%|▌  | 1916/10067 [00:17<01:14, 108.96it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  19%|▌  | 1927/10067 [00:17<01:14, 109.03it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  19%|▌  | 1939/10067 [00:18<01:13, 110.37it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  19%|▌  | 1951/10067 [00:18<01:14, 109.05it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  19%|▌  | 1963/10067 [00:18<01:13, 109.70it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  20%|▌  | 1974/10067 [00:18<01:14, 109.06it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  20%|▌  | 1986/10067 [00:18<01:14, 109.10it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  20%|▌  | 1997/10067 [00:18<01:14, 107.71it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  20%|▌  | 2008/10067 [00:18<01:14, 108.34it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  20%|▌  | 2020/10067 [00:18<01:13, 109.48it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  20%|▌  | 2031/10067 [00:18<01:13, 109.25it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  20%|▌  | 2042/10067 [00:18<01:13, 109.02it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  20%|▌  | 2054/10067 [00:19<01:13, 109.63it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  21%|▌  | 2065/10067 [00:19<01:13, 109.27it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  21%|▌  | 2077/10067 [00:19<01:12, 110.24it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  21%|▌  | 2089/10067 [00:19<01:12, 110.69it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  21%|▋  | 2101/10067 [00:19<01:12, 110.23it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  21%|▋  | 2113/10067 [00:19<01:13, 108.51it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  21%|▋  | 2124/10067 [00:19<01:13, 108.41it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  21%|▋  | 2135/10067 [00:19<01:12, 108.79it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  21%|▋  | 2146/10067 [00:19<01:12, 108.76it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  21%|▋  | 2157/10067 [00:20<01:14, 106.83it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  22%|▋  | 2168/10067 [00:20<01:13, 107.47it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  22%|▋  | 2179/10067 [00:20<01:14, 106.53it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  22%|▋  | 2190/10067 [00:20<01:13, 106.65it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  22%|▋  | 2202/10067 [00:20<01:12, 108.60it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  22%|▋  | 2214/10067 [00:20<01:11, 109.71it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  22%|▋  | 2225/10067 [00:20<01:11, 109.16it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  22%|▋  | 2237/10067 [00:20<01:11, 109.80it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  22%|▋  | 2248/10067 [00:20<01:11, 109.22it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  22%|▋  | 2259/10067 [00:20<01:12, 107.54it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  23%|▋  | 2271/10067 [00:21<01:11, 108.63it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  23%|▋  | 2282/10067 [00:21<01:11, 108.97it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  23%|▋  | 2293/10067 [00:21<01:11, 108.58it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  23%|▋  | 2304/10067 [00:21<01:11, 108.72it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  23%|▋  | 2315/10067 [00:21<01:11, 107.77it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  23%|▋  | 2326/10067 [00:21<01:11, 108.13it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  23%|▋  | 2337/10067 [00:21<01:11, 107.87it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  23%|▋  | 2348/10067 [00:21<01:11, 107.90it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  23%|▋  | 2359/10067 [00:21<01:11, 108.08it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  24%|▋  | 2370/10067 [00:22<01:12, 106.08it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  24%|▋  | 2382/10067 [00:22<01:11, 107.08it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  24%|▋  | 2394/10067 [00:22<01:11, 107.96it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  24%|▋  | 2405/10067 [00:22<01:10, 108.04it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  24%|▋  | 2416/10067 [00:22<01:11, 107.47it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  24%|▋  | 2427/10067 [00:22<01:10, 108.11it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  24%|▋  | 2438/10067 [00:22<01:10, 108.38it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  24%|▋  | 2450/10067 [00:22<01:09, 109.13it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  24%|▋  | 2462/10067 [00:22<01:09, 110.14it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  25%|▋  | 2474/10067 [00:22<01:08, 110.12it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  25%|▋  | 2486/10067 [00:23<01:10, 108.28it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  25%|▋  | 2498/10067 [00:23<01:09, 109.56it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  25%|▋  | 2509/10067 [00:23<01:08, 109.56it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  25%|▊  | 2520/10067 [00:23<01:09, 108.64it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  25%|▊  | 2531/10067 [00:23<01:09, 109.01it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  25%|▊  | 2543/10067 [00:23<01:08, 109.82it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  25%|▊  | 2554/10067 [00:23<01:08, 109.33it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  25%|▊  | 2566/10067 [00:23<01:08, 109.88it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  26%|▊  | 2578/10067 [00:23<01:07, 110.30it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  26%|▊  | 2590/10067 [00:24<01:08, 109.50it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  26%|▊  | 2602/10067 [00:24<01:07, 109.80it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  26%|▊  | 2614/10067 [00:24<01:07, 109.79it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  26%|▊  | 2625/10067 [00:24<01:08, 109.05it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  26%|▊  | 2637/10067 [00:24<01:07, 109.49it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  26%|▊  | 2649/10067 [00:24<01:07, 110.31it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  26%|▊  | 2661/10067 [00:24<01:07, 109.79it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  27%|▊  | 2672/10067 [00:24<01:07, 109.52it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  27%|▊  | 2684/10067 [00:24<01:07, 110.13it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  27%|▊  | 2696/10067 [00:24<01:07, 109.18it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  27%|▊  | 2707/10067 [00:25<01:07, 108.65it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  27%|▊  | 2718/10067 [00:25<01:07, 108.48it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  27%|▊  | 2729/10067 [00:25<01:07, 108.46it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  27%|▊  | 2740/10067 [00:25<01:07, 108.67it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  27%|▊  | 2751/10067 [00:25<01:07, 108.47it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  27%|▊  | 2763/10067 [00:25<01:06, 109.50it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  28%|▊  | 2774/10067 [00:25<01:06, 109.36it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  28%|▊  | 2786/10067 [00:25<01:06, 109.42it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  28%|▊  | 2798/10067 [00:25<01:06, 109.50it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  28%|▊  | 2809/10067 [00:26<01:06, 109.64it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  28%|▊  | 2820/10067 [00:26<01:06, 109.48it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  28%|▊  | 2831/10067 [00:26<01:06, 108.36it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  28%|▊  | 2842/10067 [00:26<01:07, 107.50it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  28%|▊  | 2854/10067 [00:26<01:05, 109.88it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  28%|▊  | 2865/10067 [00:26<01:06, 108.02it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  29%|▊  | 2877/10067 [00:26<01:06, 108.69it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  29%|▊  | 2889/10067 [00:26<01:05, 109.33it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  29%|▊  | 2901/10067 [00:26<01:05, 109.92it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  29%|▊  | 2913/10067 [00:26<01:04, 110.14it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  29%|▊  | 2925/10067 [00:27<01:05, 109.72it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  29%|▉  | 2937/10067 [00:27<01:04, 110.73it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  29%|▉  | 2949/10067 [00:27<01:04, 110.69it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  29%|▉  | 2961/10067 [00:27<01:04, 110.10it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  30%|▉  | 2973/10067 [00:27<01:04, 110.10it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  30%|▉  | 2985/10067 [00:27<01:04, 109.98it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  30%|▉  | 2997/10067 [00:27<01:03, 110.72it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  30%|▉  | 3009/10067 [00:27<01:04, 110.09it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  30%|▉  | 3021/10067 [00:27<01:03, 111.09it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  30%|▉  | 3033/10067 [00:28<01:03, 110.75it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  30%|▉  | 3045/10067 [00:28<01:03, 110.77it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  30%|▉  | 3057/10067 [00:28<01:03, 110.54it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  30%|▉  | 3069/10067 [00:28<01:03, 110.42it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  31%|▉  | 3081/10067 [00:28<01:04, 109.05it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  31%|▉  | 3093/10067 [00:28<01:03, 109.71it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  31%|▉  | 3104/10067 [00:28<01:03, 109.58it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  31%|▉  | 3115/10067 [00:28<01:03, 109.30it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  31%|▉  | 3127/10067 [00:28<01:02, 110.30it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  31%|▉  | 3139/10067 [00:29<01:02, 110.78it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  31%|▉  | 3151/10067 [00:29<01:02, 111.40it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  31%|▉  | 3163/10067 [00:29<01:02, 110.66it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  32%|▉  | 3175/10067 [00:29<01:02, 110.80it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  32%|▉  | 3187/10067 [00:29<01:01, 111.51it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  32%|▉  | 3199/10067 [00:29<01:01, 111.36it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  32%|▉  | 3211/10067 [00:29<01:02, 110.11it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  32%|▉  | 3223/10067 [00:29<01:02, 110.15it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  32%|▉  | 3235/10067 [00:29<01:01, 110.38it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  32%|▉  | 3247/10067 [00:29<01:01, 110.59it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  32%|▉  | 3259/10067 [00:30<01:02, 109.59it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  32%|▉  | 3270/10067 [00:30<01:02, 109.55it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  33%|▉  | 3281/10067 [00:30<01:02, 109.08it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  33%|▉  | 3293/10067 [00:30<01:01, 109.86it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  33%|▉  | 3305/10067 [00:30<01:01, 109.67it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  33%|▉  | 3316/10067 [00:30<01:02, 108.61it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  33%|▉  | 3327/10067 [00:30<01:01, 109.00it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  33%|▉  | 3338/10067 [00:30<01:01, 108.79it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  33%|▉  | 3350/10067 [00:30<01:00, 110.46it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  33%|█  | 3362/10067 [00:31<01:00, 110.54it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  34%|█  | 3374/10067 [00:31<01:00, 111.48it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  34%|█  | 3386/10067 [00:31<01:00, 110.87it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  34%|█  | 3398/10067 [00:31<01:00, 110.59it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  34%|█  | 3410/10067 [00:31<00:59, 110.95it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  34%|█  | 3422/10067 [00:31<01:00, 110.19it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  34%|█  | 3434/10067 [00:31<01:00, 110.24it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  34%|█  | 3446/10067 [00:31<00:59, 110.87it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  34%|█  | 3458/10067 [00:31<00:59, 111.28it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  34%|█  | 3470/10067 [00:32<01:00, 109.92it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  35%|█  | 3481/10067 [00:32<00:59, 109.89it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  35%|█  | 3492/10067 [00:32<01:00, 109.36it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  35%|█  | 3503/10067 [00:32<01:01, 106.94it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  35%|█  | 3514/10067 [00:32<01:00, 107.68it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  35%|█  | 3526/10067 [00:32<01:00, 108.73it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  35%|█  | 3537/10067 [00:32<00:59, 108.98it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  35%|█  | 3549/10067 [00:32<00:59, 109.49it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  35%|█  | 3561/10067 [00:32<00:59, 110.19it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  35%|█  | 3573/10067 [00:32<00:58, 110.14it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  36%|█  | 3585/10067 [00:33<00:58, 111.16it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  36%|█  | 3597/10067 [00:33<00:57, 111.61it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  36%|█  | 3609/10067 [00:33<00:58, 110.50it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  36%|█  | 3621/10067 [00:33<00:57, 111.59it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  36%|█  | 3633/10067 [00:33<00:57, 110.97it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  36%|█  | 3645/10067 [00:33<00:57, 111.51it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  36%|█  | 3657/10067 [00:33<00:58, 110.02it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  36%|█  | 3669/10067 [00:33<00:58, 109.85it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  37%|█  | 3681/10067 [00:33<00:57, 110.17it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  37%|█  | 3693/10067 [00:34<00:57, 110.68it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  37%|█  | 3705/10067 [00:34<00:57, 110.73it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  37%|█  | 3717/10067 [00:34<00:57, 109.61it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  37%|█  | 3728/10067 [00:34<00:57, 109.50it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  37%|█  | 3740/10067 [00:34<00:57, 110.22it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  37%|█  | 3752/10067 [00:34<00:57, 109.97it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  37%|█  | 3764/10067 [00:34<00:57, 110.46it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  38%|█▏ | 3776/10067 [00:34<00:56, 111.27it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  38%|█▏ | 3788/10067 [00:34<00:56, 111.00it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  38%|█▏ | 3800/10067 [00:35<00:56, 110.08it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  38%|█▏ | 3812/10067 [00:35<00:56, 110.09it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  38%|█▏ | 3824/10067 [00:35<00:56, 110.60it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  38%|█▏ | 3836/10067 [00:35<00:55, 111.55it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  38%|█▏ | 3848/10067 [00:35<00:55, 112.18it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  38%|█▏ | 3860/10067 [00:35<00:55, 112.76it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  38%|█▏ | 3872/10067 [00:35<00:55, 111.59it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  39%|█▏ | 3884/10067 [00:35<00:55, 111.12it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  39%|█▏ | 3896/10067 [00:35<00:55, 110.74it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  39%|█▏ | 3908/10067 [00:35<00:55, 111.09it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  39%|█▏ | 3920/10067 [00:36<00:55, 111.01it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  39%|█▏ | 3932/10067 [00:36<00:55, 110.40it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  39%|█▏ | 3944/10067 [00:36<00:55, 110.25it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  39%|█▏ | 3956/10067 [00:36<00:54, 111.25it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  39%|█▏ | 3968/10067 [00:36<00:54, 111.35it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  40%|█▏ | 3980/10067 [00:36<00:55, 110.47it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  40%|█▏ | 3992/10067 [00:36<00:54, 110.54it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  40%|█▏ | 4004/10067 [00:36<00:54, 110.75it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  40%|█▏ | 4016/10067 [00:36<00:54, 111.71it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  40%|█▏ | 4028/10067 [00:37<00:53, 112.09it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  40%|█▏ | 4040/10067 [00:37<00:53, 111.73it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  40%|█▏ | 4052/10067 [00:37<00:53, 111.93it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  40%|█▏ | 4064/10067 [00:37<00:53, 111.54it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  40%|█▏ | 4076/10067 [00:37<00:54, 110.75it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  41%|█▏ | 4088/10067 [00:37<00:54, 110.26it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  41%|█▏ | 4100/10067 [00:37<00:53, 111.11it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  41%|█▏ | 4112/10067 [00:37<00:53, 110.63it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  41%|█▏ | 4124/10067 [00:37<00:53, 110.85it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  41%|█▏ | 4136/10067 [00:38<00:53, 110.96it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  41%|█▏ | 4148/10067 [00:38<00:53, 111.19it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  41%|█▏ | 4160/10067 [00:38<00:52, 111.90it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  41%|█▏ | 4172/10067 [00:38<00:52, 111.72it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  42%|█▏ | 4184/10067 [00:38<00:52, 112.19it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  42%|█▎ | 4196/10067 [00:38<00:52, 112.14it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  42%|█▋  | 4208/10067 [00:38<01:01, 95.23it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  42%|█▋  | 4218/10067 [00:38<01:03, 91.68it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  42%|█▋  | 4230/10067 [00:38<00:59, 98.65it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  42%|█▎ | 4243/10067 [00:39<00:55, 105.81it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  42%|█▎ | 4256/10067 [00:39<00:52, 111.10it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  42%|█▎ | 4268/10067 [00:39<00:54, 105.92it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  43%|█▎ | 4280/10067 [00:39<00:52, 109.70it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  43%|█▎ | 4293/10067 [00:39<00:50, 114.12it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  43%|█▎ | 4306/10067 [00:39<00:49, 115.92it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  43%|█▎ | 4319/10067 [00:39<00:48, 118.02it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  43%|█▎ | 4332/10067 [00:39<00:47, 120.57it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  43%|█▎ | 4345/10067 [00:39<00:46, 121.76it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  43%|█▎ | 4358/10067 [00:40<00:46, 121.93it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  43%|█▎ | 4371/10067 [00:40<00:46, 122.86it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  44%|█▎ | 4384/10067 [00:40<00:47, 120.81it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  44%|█▋  | 4397/10067 [00:40<01:00, 94.41it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  44%|█▊  | 4408/10067 [00:40<01:08, 82.59it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  44%|█▊  | 4418/10067 [00:40<01:15, 75.29it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  44%|█▊  | 4427/10067 [00:40<01:18, 72.27it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  44%|█▊  | 4435/10067 [00:41<01:19, 70.65it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  44%|█▊  | 4443/10067 [00:41<01:22, 68.09it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  44%|█▊  | 4450/10067 [00:41<01:23, 67.18it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  44%|█▊  | 4457/10067 [00:41<01:27, 64.32it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  44%|█▊  | 4464/10067 [00:41<01:27, 64.16it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  44%|█▊  | 4471/10067 [00:41<01:28, 63.15it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  44%|█▊  | 4478/10067 [00:41<01:26, 64.26it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  45%|█▊  | 4485/10067 [00:41<01:28, 63.09it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  45%|█▊  | 4492/10067 [00:41<01:30, 61.40it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  45%|█▊  | 4499/10067 [00:42<01:30, 61.49it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  45%|█▊  | 4506/10067 [00:42<01:30, 61.48it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  45%|█▊  | 4513/10067 [00:42<01:28, 62.55it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  45%|█▊  | 4520/10067 [00:42<01:29, 61.91it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  45%|█▊  | 4527/10067 [00:42<01:31, 60.56it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  45%|█▊  | 4534/10067 [00:42<01:30, 61.00it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  45%|█▊  | 4541/10067 [00:42<01:28, 62.29it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  45%|█▊  | 4548/10067 [00:42<01:29, 61.34it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  45%|█▊  | 4555/10067 [00:43<01:31, 60.30it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  45%|█▊  | 4562/10067 [00:43<01:30, 60.53it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  45%|█▊  | 4569/10067 [00:43<01:30, 60.83it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  45%|█▊  | 4576/10067 [00:43<01:28, 62.10it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  46%|█▊  | 4583/10067 [00:43<01:28, 61.78it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  46%|█▊  | 4590/10067 [00:43<01:27, 62.35it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  46%|█▊  | 4597/10067 [00:43<01:28, 61.70it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  46%|█▊  | 4604/10067 [00:43<01:29, 61.03it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  46%|█▊  | 4611/10067 [00:43<01:28, 61.70it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  46%|█▊  | 4618/10067 [00:44<01:29, 60.72it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  46%|█▊  | 4625/10067 [00:44<01:29, 61.00it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  46%|█▊  | 4632/10067 [00:44<01:30, 60.37it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  46%|█▊  | 4639/10067 [00:44<01:30, 59.90it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  46%|█▊  | 4646/10067 [00:44<01:30, 60.20it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  46%|█▊  | 4653/10067 [00:44<01:30, 59.94it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  46%|█▊  | 4659/10067 [00:44<01:31, 59.42it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  46%|█▊  | 4666/10067 [00:44<01:30, 59.79it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  46%|█▊  | 4672/10067 [00:44<01:30, 59.30it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  46%|█▊  | 4678/10067 [00:45<01:31, 58.91it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  47%|█▊  | 4684/10067 [00:45<01:31, 58.89it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  47%|█▊  | 4690/10067 [00:45<01:31, 59.07it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  47%|█▊  | 4696/10067 [00:45<01:31, 58.71it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  47%|█▊  | 4702/10067 [00:45<01:31, 58.46it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  47%|█▊  | 4708/10067 [00:45<01:32, 58.18it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  47%|█▊  | 4714/10067 [00:45<01:32, 58.12it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  47%|█▉  | 4720/10067 [00:45<01:32, 57.78it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  47%|█▉  | 4727/10067 [00:45<01:31, 58.43it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  47%|█▉  | 4733/10067 [00:45<01:32, 57.68it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  47%|█▉  | 4739/10067 [00:46<01:32, 57.45it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  47%|█▉  | 4745/10067 [00:46<01:32, 57.59it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  47%|█▉  | 4751/10067 [00:46<01:32, 57.20it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  47%|█▉  | 4757/10067 [00:46<01:32, 57.38it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  47%|█▉  | 4763/10067 [00:46<01:32, 57.63it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  47%|█▉  | 4769/10067 [00:46<01:32, 57.42it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  47%|█▉  | 4775/10067 [00:46<01:32, 57.09it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  48%|█▉  | 4782/10067 [00:46<01:30, 58.50it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  48%|█▉  | 4788/10067 [00:46<01:31, 57.88it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  48%|█▉  | 4794/10067 [00:47<01:30, 57.97it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  48%|█▉  | 4800/10067 [00:47<01:31, 57.54it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  48%|█▉  | 4806/10067 [00:47<01:31, 57.22it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  48%|█▉  | 4813/10067 [00:47<01:29, 58.68it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  48%|█▉  | 4819/10067 [00:47<01:30, 58.10it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  48%|█▉  | 4825/10067 [00:47<01:29, 58.38it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  48%|█▉  | 4832/10067 [00:47<01:28, 59.09it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  48%|█▉  | 4838/10067 [00:47<01:29, 58.57it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  48%|█▉  | 4845/10067 [00:47<01:28, 58.82it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  48%|█▉  | 4852/10067 [00:48<01:27, 59.49it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  48%|█▉  | 4858/10067 [00:48<01:28, 58.78it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  48%|█▉  | 4864/10067 [00:48<01:30, 57.61it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  48%|█▉  | 4870/10067 [00:48<01:30, 57.67it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  48%|█▉  | 4877/10067 [00:48<01:29, 58.17it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  49%|█▉  | 4883/10067 [00:48<01:29, 57.87it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  49%|█▉  | 4889/10067 [00:48<01:30, 57.34it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  49%|█▉  | 4896/10067 [00:48<01:29, 57.93it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  49%|█▉  | 4902/10067 [00:48<01:29, 57.43it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  49%|█▉  | 4909/10067 [00:49<01:28, 58.41it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  49%|█▉  | 4915/10067 [00:49<01:27, 58.70it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  49%|█▉  | 4921/10067 [00:49<01:29, 57.69it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  49%|█▉  | 4927/10067 [00:49<01:29, 57.65it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  49%|█▉  | 4934/10067 [00:49<01:27, 58.63it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  49%|█▉  | 4940/10067 [00:49<01:28, 58.19it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  49%|█▉  | 4947/10067 [00:49<01:26, 58.96it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  49%|█▉  | 4953/10067 [00:49<01:28, 58.06it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  49%|█▉  | 4959/10067 [00:49<01:27, 58.59it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  49%|█▉  | 4965/10067 [00:49<01:27, 57.99it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  49%|█▉  | 4972/10067 [00:50<01:26, 58.65it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  49%|█▉  | 4978/10067 [00:50<01:27, 58.42it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  50%|█▉  | 4984/10067 [00:50<01:26, 58.72it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  50%|█▉  | 4990/10067 [00:50<01:25, 59.07it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  50%|█▉  | 4996/10067 [00:50<01:28, 57.12it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  50%|█▉  | 5002/10067 [00:50<01:29, 56.49it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  50%|█▉  | 5008/10067 [00:50<01:29, 56.45it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  50%|█▉  | 5015/10067 [00:50<01:27, 57.92it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  50%|█▉  | 5022/10067 [00:50<01:26, 58.59it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  50%|█▉  | 5029/10067 [00:51<01:23, 60.28it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  50%|██  | 5036/10067 [00:51<01:24, 59.75it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  50%|██  | 5043/10067 [00:51<01:23, 60.03it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  50%|██  | 5050/10067 [00:51<01:23, 59.78it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  50%|██  | 5057/10067 [00:51<01:22, 60.42it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  50%|██  | 5064/10067 [00:51<01:21, 61.64it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  50%|██  | 5071/10067 [00:51<01:22, 60.38it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  50%|██  | 5078/10067 [00:51<01:23, 59.61it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  51%|██  | 5084/10067 [00:52<01:24, 58.76it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  51%|██  | 5091/10067 [00:52<01:24, 59.07it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  51%|██  | 5098/10067 [00:52<01:22, 60.57it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  51%|██  | 5105/10067 [00:52<01:21, 60.87it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  51%|██  | 5112/10067 [00:52<01:23, 59.55it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  51%|██  | 5119/10067 [00:52<01:22, 59.65it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  51%|██  | 5125/10067 [00:52<01:22, 59.54it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  51%|██  | 5131/10067 [00:52<01:23, 58.91it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  51%|██  | 5137/10067 [00:52<01:24, 58.38it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  51%|██  | 5144/10067 [00:53<01:23, 59.30it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  51%|██  | 5150/10067 [00:53<01:24, 58.00it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  51%|██  | 5156/10067 [00:53<01:24, 58.34it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  51%|██  | 5163/10067 [00:53<01:22, 59.09it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  51%|██  | 5169/10067 [00:53<01:24, 58.17it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  51%|██  | 5176/10067 [00:53<01:23, 58.82it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  51%|██  | 5183/10067 [00:53<01:22, 59.54it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  52%|██  | 5190/10067 [00:53<01:19, 61.20it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  52%|██  | 5197/10067 [00:53<01:21, 59.40it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  52%|██  | 5204/10067 [00:54<01:21, 59.85it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  52%|██  | 5210/10067 [00:54<01:21, 59.81it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  52%|██  | 5217/10067 [00:54<01:20, 59.92it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  52%|██  | 5223/10067 [00:54<01:22, 58.63it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  52%|██  | 5229/10067 [00:54<01:22, 58.84it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  52%|██  | 5236/10067 [00:54<01:19, 60.44it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  52%|██  | 5243/10067 [00:54<01:19, 60.86it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  52%|██  | 5250/10067 [00:54<01:18, 61.17it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  52%|██  | 5257/10067 [00:54<01:19, 60.78it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  52%|██  | 5264/10067 [00:55<01:19, 60.52it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  52%|██  | 5271/10067 [00:55<01:19, 60.35it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  52%|██  | 5278/10067 [00:55<01:21, 59.05it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  52%|██  | 5284/10067 [00:55<01:21, 58.98it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  53%|██  | 5291/10067 [00:55<01:18, 60.94it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  53%|██  | 5298/10067 [00:55<01:19, 60.13it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  53%|██  | 5305/10067 [00:55<01:18, 60.76it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  53%|██  | 5312/10067 [00:55<01:19, 59.87it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  53%|██  | 5319/10067 [00:55<01:17, 61.15it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  53%|██  | 5326/10067 [00:56<01:18, 60.49it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  53%|██  | 5333/10067 [00:56<01:18, 60.27it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  53%|██  | 5340/10067 [00:56<01:18, 60.40it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  53%|██  | 5347/10067 [00:56<01:18, 60.02it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  53%|██▏ | 5354/10067 [00:56<01:18, 60.12it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  53%|██▏ | 5361/10067 [00:56<01:16, 61.52it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  53%|██▏ | 5368/10067 [00:56<01:17, 60.98it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  53%|██▏ | 5375/10067 [00:56<01:16, 61.34it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  53%|██▏ | 5382/10067 [00:56<01:17, 60.67it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  54%|██▏ | 5389/10067 [00:57<01:16, 60.82it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  54%|██▏ | 5396/10067 [00:57<01:18, 59.79it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  54%|██▏ | 5402/10067 [00:57<01:19, 58.53it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  54%|██▏ | 5409/10067 [00:57<01:18, 59.10it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  54%|██▏ | 5416/10067 [00:57<01:17, 59.80it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  54%|██▏ | 5423/10067 [00:57<01:17, 59.89it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  54%|██▏ | 5430/10067 [00:57<01:17, 60.17it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  54%|██▏ | 5437/10067 [00:57<01:16, 60.21it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  54%|██▏ | 5444/10067 [00:58<01:17, 60.03it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  54%|██▏ | 5451/10067 [00:58<01:17, 59.22it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  54%|██▏ | 5458/10067 [00:58<01:17, 59.76it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  54%|██▏ | 5464/10067 [00:58<01:17, 59.70it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  54%|██▏ | 5471/10067 [00:58<01:16, 59.99it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  54%|██▏ | 5478/10067 [00:58<01:15, 60.48it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  54%|██▏ | 5485/10067 [00:58<01:16, 60.19it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  55%|██▏ | 5492/10067 [00:58<01:16, 59.97it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  55%|██▏ | 5499/10067 [00:58<01:16, 59.83it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  55%|██▏ | 5505/10067 [00:59<01:17, 59.19it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  55%|██▏ | 5511/10067 [00:59<01:16, 59.19it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  55%|██▏ | 5517/10067 [00:59<01:18, 58.03it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  55%|██▏ | 5523/10067 [00:59<01:18, 58.04it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  55%|██▏ | 5530/10067 [00:59<01:17, 58.79it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  55%|██▏ | 5536/10067 [00:59<01:17, 58.61it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  55%|██▏ | 5543/10067 [00:59<01:16, 59.47it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  55%|██▏ | 5550/10067 [00:59<01:14, 60.70it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  55%|██▏ | 5557/10067 [00:59<01:14, 60.69it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  55%|██▏ | 5564/10067 [01:00<01:14, 60.22it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  55%|██▏ | 5571/10067 [01:00<01:14, 59.97it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  55%|██▏ | 5578/10067 [01:00<01:14, 60.16it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  55%|██▏ | 5585/10067 [01:00<01:14, 60.27it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  56%|██▏ | 5592/10067 [01:00<01:13, 60.72it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  56%|██▏ | 5599/10067 [01:00<01:14, 60.23it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  56%|██▏ | 5606/10067 [01:00<01:13, 60.81it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  56%|██▏ | 5613/10067 [01:00<01:13, 60.46it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  56%|██▏ | 5620/10067 [01:00<01:14, 59.74it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  56%|██▏ | 5626/10067 [01:01<01:14, 59.22it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  56%|██▏ | 5633/10067 [01:01<01:13, 60.14it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  56%|██▏ | 5640/10067 [01:01<01:13, 60.42it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  56%|██▏ | 5647/10067 [01:01<01:13, 60.27it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  56%|██▏ | 5654/10067 [01:01<01:13, 60.16it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  56%|██▏ | 5661/10067 [01:01<01:13, 59.78it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  56%|██▎ | 5668/10067 [01:01<01:12, 60.29it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  56%|██▎ | 5675/10067 [01:01<01:12, 60.37it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  56%|██▎ | 5682/10067 [01:01<01:12, 60.17it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  57%|██▎ | 5689/10067 [01:02<01:12, 60.21it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  57%|██▎ | 5696/10067 [01:02<01:12, 59.89it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  57%|██▎ | 5703/10067 [01:02<01:12, 60.23it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  57%|██▎ | 5710/10067 [01:02<01:13, 59.05it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  57%|██▎ | 5717/10067 [01:02<01:12, 59.70it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  57%|██▎ | 5723/10067 [01:02<01:13, 59.33it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  57%|██▎ | 5730/10067 [01:02<01:12, 60.03it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  57%|██▎ | 5737/10067 [01:02<01:11, 60.64it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  57%|██▎ | 5744/10067 [01:03<01:12, 59.96it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  57%|██▎ | 5751/10067 [01:03<01:12, 59.93it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  57%|██▎ | 5757/10067 [01:03<01:12, 59.11it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  57%|██▎ | 5764/10067 [01:03<01:11, 59.99it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  57%|██▎ | 5770/10067 [01:03<01:12, 59.52it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  57%|██▎ | 5777/10067 [01:03<01:11, 59.65it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  57%|██▎ | 5783/10067 [01:03<01:11, 59.63it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  58%|██▎ | 5789/10067 [01:03<01:12, 58.91it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  58%|██▎ | 5796/10067 [01:03<01:10, 60.47it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  58%|██▎ | 5803/10067 [01:04<01:10, 60.07it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  58%|██▎ | 5810/10067 [01:04<01:10, 60.16it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  58%|██▎ | 5817/10067 [01:04<01:10, 60.53it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  58%|██▎ | 5824/10067 [01:04<01:11, 59.35it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  58%|██▎ | 5830/10067 [01:04<01:11, 59.31it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  58%|██▎ | 5837/10067 [01:04<01:11, 59.54it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  58%|██▎ | 5844/10067 [01:04<01:10, 60.25it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  58%|██▎ | 5851/10067 [01:04<01:10, 59.76it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  58%|██▎ | 5857/10067 [01:04<01:10, 59.43it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  58%|██▎ | 5863/10067 [01:05<01:11, 58.99it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  58%|██▎ | 5870/10067 [01:05<01:10, 59.47it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  58%|██▎ | 5876/10067 [01:05<01:10, 59.08it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  58%|██▎ | 5883/10067 [01:05<01:10, 59.74it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  58%|██▎ | 5889/10067 [01:05<01:09, 59.77it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  59%|██▎ | 5895/10067 [01:05<01:09, 59.80it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  59%|██▎ | 5901/10067 [01:05<01:10, 59.26it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  59%|██▎ | 5908/10067 [01:05<01:09, 59.73it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  59%|██▎ | 5915/10067 [01:05<01:09, 60.10it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  59%|██▎ | 5922/10067 [01:05<01:08, 60.74it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  59%|██▎ | 5929/10067 [01:06<01:08, 60.21it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  59%|██▎ | 5936/10067 [01:06<01:08, 60.05it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  59%|██▎ | 5943/10067 [01:06<01:08, 60.19it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  59%|██▎ | 5950/10067 [01:06<01:09, 59.65it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  59%|██▎ | 5956/10067 [01:06<01:09, 59.38it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  59%|██▎ | 5963/10067 [01:06<01:08, 59.56it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  59%|██▎ | 5969/10067 [01:06<01:09, 58.98it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  59%|██▎ | 5975/10067 [01:06<01:09, 58.46it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  59%|██▍ | 5981/10067 [01:06<01:09, 58.62it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  59%|██▍ | 5988/10067 [01:07<01:07, 60.10it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  60%|██▍ | 5995/10067 [01:07<01:08, 59.26it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  60%|██▍ | 6001/10067 [01:07<01:08, 59.16it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  60%|██▍ | 6008/10067 [01:07<01:08, 59.53it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  60%|██▍ | 6014/10067 [01:07<01:09, 58.49it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  60%|██▍ | 6020/10067 [01:07<01:08, 58.82it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  60%|██▍ | 6026/10067 [01:07<01:08, 59.03it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  60%|██▍ | 6032/10067 [01:07<01:08, 59.15it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  60%|██▍ | 6038/10067 [01:07<01:08, 59.14it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  60%|██▍ | 6044/10067 [01:08<01:08, 58.68it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  60%|██▍ | 6050/10067 [01:08<01:08, 58.22it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  60%|██▍ | 6057/10067 [01:08<01:07, 59.31it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  60%|██▍ | 6063/10067 [01:08<01:07, 59.10it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  60%|██▍ | 6069/10067 [01:08<01:07, 58.97it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  60%|██▍ | 6076/10067 [01:08<01:06, 60.02it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  60%|██▍ | 6082/10067 [01:08<01:07, 59.08it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  60%|██▍ | 6089/10067 [01:08<01:07, 59.13it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  61%|██▍ | 6096/10067 [01:08<01:06, 59.91it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  61%|██▍ | 6102/10067 [01:09<01:06, 59.26it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  61%|██▍ | 6108/10067 [01:09<01:07, 59.04it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  61%|██▍ | 6114/10067 [01:09<01:07, 58.54it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  61%|██▍ | 6120/10067 [01:09<01:07, 58.84it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  61%|██▍ | 6127/10067 [01:09<01:06, 59.12it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  61%|██▍ | 6133/10067 [01:09<01:06, 59.15it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  61%|██▍ | 6139/10067 [01:09<01:06, 59.02it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  61%|██▍ | 6145/10067 [01:09<01:06, 59.03it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  61%|██▍ | 6151/10067 [01:09<01:06, 58.86it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  61%|██▍ | 6157/10067 [01:09<01:06, 58.64it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  61%|██▍ | 6164/10067 [01:10<01:05, 59.18it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  61%|██▍ | 6170/10067 [01:10<01:06, 58.62it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  61%|██▍ | 6177/10067 [01:10<01:05, 59.03it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  61%|██▍ | 6183/10067 [01:10<01:05, 59.20it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  61%|██▍ | 6190/10067 [01:10<01:05, 59.61it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  62%|██▍ | 6196/10067 [01:10<01:05, 59.04it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  62%|██▍ | 6202/10067 [01:10<01:06, 58.48it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  62%|██▍ | 6208/10067 [01:10<01:06, 58.47it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  62%|██▍ | 6214/10067 [01:10<01:05, 58.55it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  62%|██▍ | 6220/10067 [01:11<01:05, 58.47it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  62%|██▍ | 6226/10067 [01:11<01:05, 58.51it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  62%|██▍ | 6232/10067 [01:11<01:05, 58.73it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  62%|██▍ | 6238/10067 [01:11<01:05, 58.07it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  62%|██▍ | 6244/10067 [01:11<01:05, 58.52it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  62%|██▍ | 6251/10067 [01:11<01:04, 58.83it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  62%|██▍ | 6257/10067 [01:11<01:04, 58.65it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  62%|██▍ | 6263/10067 [01:11<01:05, 58.32it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  62%|██▍ | 6269/10067 [01:11<01:05, 58.39it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  62%|██▍ | 6276/10067 [01:12<01:04, 59.15it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  62%|██▍ | 6283/10067 [01:12<01:02, 60.78it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  62%|██▍ | 6290/10067 [01:12<01:03, 59.81it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  63%|██▌ | 6296/10067 [01:12<01:03, 59.31it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  63%|██▌ | 6302/10067 [01:12<01:04, 58.53it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  63%|██▌ | 6308/10067 [01:12<01:04, 58.50it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  63%|██▌ | 6314/10067 [01:12<01:04, 58.57it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  63%|██▌ | 6320/10067 [01:12<01:03, 58.77it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  63%|██▌ | 6326/10067 [01:12<01:04, 57.97it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  63%|██▌ | 6332/10067 [01:12<01:03, 58.47it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  63%|██▌ | 6338/10067 [01:13<01:04, 57.92it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  63%|██▌ | 6344/10067 [01:13<01:04, 57.93it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  63%|██▌ | 6351/10067 [01:13<01:03, 58.58it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  63%|██▌ | 6358/10067 [01:13<01:02, 59.24it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  63%|██▌ | 6364/10067 [01:13<01:02, 59.27it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  63%|██▌ | 6370/10067 [01:13<01:02, 58.78it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  63%|██▌ | 6376/10067 [01:13<01:02, 59.12it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  63%|██▌ | 6382/10067 [01:13<01:02, 59.34it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  63%|██▌ | 6389/10067 [01:13<01:01, 59.80it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  64%|██▌ | 6395/10067 [01:14<01:02, 58.70it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  64%|██▌ | 6401/10067 [01:14<01:02, 58.87it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  64%|██▌ | 6408/10067 [01:14<01:01, 59.51it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  64%|██▌ | 6414/10067 [01:14<01:01, 59.23it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  64%|██▌ | 6420/10067 [01:14<01:01, 59.44it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  64%|██▌ | 6426/10067 [01:14<01:02, 58.52it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  64%|██▌ | 6432/10067 [01:14<01:01, 58.77it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  64%|██▌ | 6438/10067 [01:14<01:01, 58.97it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  64%|██▌ | 6444/10067 [01:14<01:01, 58.76it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  64%|██▌ | 6450/10067 [01:14<01:02, 58.10it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  64%|██▌ | 6457/10067 [01:15<01:00, 59.95it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  64%|██▌ | 6463/10067 [01:15<01:00, 59.26it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  64%|██▌ | 6469/10067 [01:15<01:00, 59.05it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  64%|██▌ | 6475/10067 [01:15<01:01, 58.68it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  64%|██▌ | 6481/10067 [01:15<01:01, 58.66it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  64%|██▌ | 6487/10067 [01:15<01:01, 58.60it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  64%|██▌ | 6493/10067 [01:15<01:00, 58.72it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  65%|██▌ | 6499/10067 [01:15<01:01, 58.46it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  65%|██▌ | 6505/10067 [01:15<01:00, 58.86it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  65%|██▌ | 6511/10067 [01:15<01:00, 58.73it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  65%|██▌ | 6517/10067 [01:16<01:00, 58.37it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  65%|██▌ | 6523/10067 [01:16<01:00, 58.59it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  65%|██▌ | 6529/10067 [01:16<01:00, 58.45it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  65%|██▌ | 6535/10067 [01:16<01:00, 58.51it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  65%|██▌ | 6541/10067 [01:16<01:00, 58.69it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  65%|██▌ | 6547/10067 [01:16<00:59, 58.86it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  65%|██▌ | 6553/10067 [01:16<00:59, 58.84it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  65%|██▌ | 6559/10067 [01:16<01:00, 58.01it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  65%|██▌ | 6565/10067 [01:16<01:00, 58.30it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  65%|██▌ | 6571/10067 [01:17<01:00, 58.26it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  65%|██▌ | 6577/10067 [01:17<00:59, 58.18it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  65%|██▌ | 6583/10067 [01:17<00:59, 58.46it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  65%|██▌ | 6589/10067 [01:17<00:59, 58.81it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  66%|██▌ | 6595/10067 [01:17<00:58, 58.87it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  66%|██▌ | 6601/10067 [01:17<00:58, 59.02it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  66%|██▋ | 6607/10067 [01:17<00:58, 58.78it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  66%|██▋ | 6613/10067 [01:17<00:58, 58.66it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  66%|██▋ | 6619/10067 [01:17<00:58, 58.62it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  66%|██▋ | 6626/10067 [01:17<00:58, 59.22it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  66%|██▋ | 6632/10067 [01:18<00:57, 59.29it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  66%|██▋ | 6638/10067 [01:18<00:58, 58.23it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  66%|██▋ | 6644/10067 [01:18<00:58, 58.17it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  66%|██▋ | 6650/10067 [01:18<00:58, 58.15it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  66%|██▋ | 6656/10067 [01:18<00:59, 57.44it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  66%|██▋ | 6663/10067 [01:18<00:58, 58.43it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  66%|██▋ | 6669/10067 [01:18<00:59, 57.51it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  66%|██▋ | 6676/10067 [01:18<00:58, 58.39it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  66%|██▋ | 6682/10067 [01:18<00:58, 57.51it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  66%|██▋ | 6688/10067 [01:19<00:58, 57.38it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  67%|██▋ | 6695/10067 [01:19<00:57, 58.20it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  67%|██▋ | 6702/10067 [01:19<00:57, 58.93it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  67%|██▋ | 6708/10067 [01:19<00:57, 58.68it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  67%|██▋ | 6714/10067 [01:19<00:57, 58.18it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  67%|██▋ | 6720/10067 [01:19<00:57, 58.29it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  67%|██▋ | 6726/10067 [01:19<00:57, 58.25it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  67%|██▋ | 6732/10067 [01:19<00:57, 58.02it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  67%|██▋ | 6738/10067 [01:19<00:57, 58.35it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  67%|██▋ | 6744/10067 [01:19<00:57, 58.25it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  67%|██▋ | 6750/10067 [01:20<00:57, 57.79it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  67%|██▋ | 6757/10067 [01:20<00:56, 58.62it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  67%|██▋ | 6763/10067 [01:20<00:56, 58.01it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  67%|██▋ | 6770/10067 [01:20<00:56, 58.53it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  67%|██▋ | 6776/10067 [01:20<00:56, 58.62it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  67%|██▋ | 6782/10067 [01:20<00:56, 58.00it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  67%|██▋ | 6788/10067 [01:20<00:56, 58.00it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  67%|██▋ | 6794/10067 [01:20<00:56, 58.18it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  68%|██▋ | 6800/10067 [01:20<00:56, 57.60it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  68%|██▋ | 6806/10067 [01:21<00:56, 57.56it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  68%|██▋ | 6812/10067 [01:21<00:55, 58.25it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  68%|██▋ | 6818/10067 [01:21<00:55, 58.43it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  68%|██▋ | 6824/10067 [01:21<00:55, 58.55it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  68%|██▋ | 6830/10067 [01:21<00:55, 57.87it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  68%|██▋ | 6836/10067 [01:21<00:55, 58.16it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  68%|██▋ | 6842/10067 [01:21<00:55, 58.27it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  68%|██▋ | 6848/10067 [01:21<00:54, 58.54it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  68%|██▋ | 6854/10067 [01:21<00:55, 58.27it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  68%|██▋ | 6860/10067 [01:21<00:54, 58.77it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  68%|██▋ | 6866/10067 [01:22<00:54, 58.95it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  68%|██▋ | 6872/10067 [01:22<00:54, 58.93it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  68%|██▋ | 6878/10067 [01:22<00:54, 58.70it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  68%|██▋ | 6884/10067 [01:22<00:54, 58.07it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  68%|██▋ | 6890/10067 [01:22<00:54, 58.31it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  69%|██▋ | 6896/10067 [01:22<00:54, 58.57it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  69%|██▋ | 6903/10067 [01:22<00:53, 58.81it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  69%|██▋ | 6909/10067 [01:22<00:54, 58.32it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  69%|██▋ | 6915/10067 [01:22<00:54, 58.20it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  69%|██▋ | 6921/10067 [01:23<00:53, 58.32it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  69%|██▊ | 6927/10067 [01:23<00:53, 58.46it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  69%|██▊ | 6933/10067 [01:23<00:54, 57.74it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  69%|██▊ | 6939/10067 [01:23<00:53, 57.93it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  69%|██▊ | 6945/10067 [01:23<00:53, 58.05it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  69%|██▊ | 6951/10067 [01:23<00:54, 57.58it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  69%|██▊ | 6957/10067 [01:23<00:53, 57.80it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  69%|██▊ | 6963/10067 [01:23<00:53, 57.80it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  69%|██▊ | 6969/10067 [01:23<00:53, 57.80it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  69%|██▊ | 6975/10067 [01:23<00:53, 57.76it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  69%|██▊ | 6981/10067 [01:24<00:53, 57.69it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  69%|██▊ | 6988/10067 [01:24<00:52, 58.84it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  69%|██▊ | 6994/10067 [01:24<00:52, 58.68it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  70%|██▊ | 7000/10067 [01:24<00:52, 58.69it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  70%|██▊ | 7006/10067 [01:24<00:51, 58.98it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  70%|██▊ | 7012/10067 [01:24<00:51, 59.21it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  70%|██▊ | 7019/10067 [01:24<00:51, 59.72it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  70%|██▊ | 7025/10067 [01:24<00:51, 59.32it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  70%|██▊ | 7031/10067 [01:24<00:51, 58.62it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  70%|██▊ | 7037/10067 [01:25<00:51, 58.73it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  70%|██▊ | 7044/10067 [01:25<00:51, 59.23it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  70%|██▊ | 7051/10067 [01:25<00:49, 60.73it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  70%|██▊ | 7058/10067 [01:25<00:49, 60.69it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  70%|██▊ | 7065/10067 [01:25<00:48, 61.69it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  70%|██▊ | 7072/10067 [01:25<00:49, 60.46it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  70%|██▊ | 7079/10067 [01:25<00:48, 61.30it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  70%|██▊ | 7086/10067 [01:25<00:48, 60.87it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  70%|██▊ | 7093/10067 [01:25<00:49, 60.34it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  71%|██▊ | 7100/10067 [01:26<00:49, 60.16it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  71%|██▊ | 7107/10067 [01:26<00:48, 61.06it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  71%|██▊ | 7114/10067 [01:26<00:47, 62.40it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  71%|██▊ | 7121/10067 [01:26<00:47, 62.02it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  71%|██▊ | 7128/10067 [01:26<00:47, 61.24it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  71%|██▊ | 7135/10067 [01:26<00:47, 62.31it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  71%|██▊ | 7142/10067 [01:26<00:47, 61.28it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  71%|██▊ | 7149/10067 [01:26<00:47, 61.90it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  71%|██▊ | 7156/10067 [01:26<00:47, 60.93it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  71%|██▊ | 7163/10067 [01:27<00:46, 61.98it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  71%|██▊ | 7170/10067 [01:27<00:47, 60.89it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  71%|██▊ | 7177/10067 [01:27<00:48, 59.45it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  71%|██▊ | 7184/10067 [01:27<00:48, 59.66it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  71%|██▊ | 7191/10067 [01:27<00:47, 60.48it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  72%|██▊ | 7198/10067 [01:27<00:46, 61.14it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  72%|██▊ | 7205/10067 [01:27<00:47, 60.83it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  72%|██▊ | 7212/10067 [01:27<00:47, 59.94it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  72%|██▊ | 7219/10067 [01:27<00:46, 60.89it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  72%|██▊ | 7226/10067 [01:28<00:47, 60.12it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  72%|██▊ | 7233/10067 [01:28<00:47, 59.75it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  72%|██▉ | 7240/10067 [01:28<00:47, 60.07it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  72%|██▉ | 7247/10067 [01:28<00:46, 61.27it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  72%|██▉ | 7254/10067 [01:28<00:46, 61.02it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  72%|██▉ | 7261/10067 [01:28<00:46, 60.63it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  72%|██▉ | 7268/10067 [01:28<00:45, 61.59it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  72%|██▉ | 7275/10067 [01:28<00:44, 62.22it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  72%|██▉ | 7282/10067 [01:29<00:45, 61.78it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  72%|██▉ | 7289/10067 [01:29<00:46, 60.04it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  72%|██▉ | 7296/10067 [01:29<00:46, 60.02it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  73%|██▉ | 7303/10067 [01:29<00:46, 60.05it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  73%|██▉ | 7310/10067 [01:29<00:46, 59.63it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  73%|██▉ | 7316/10067 [01:29<00:46, 59.64it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  73%|██▉ | 7323/10067 [01:29<00:45, 60.85it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  73%|██▉ | 7330/10067 [01:29<00:45, 60.31it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  73%|██▉ | 7337/10067 [01:29<00:44, 60.68it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  73%|██▉ | 7344/10067 [01:30<00:45, 60.27it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  73%|██▉ | 7351/10067 [01:30<00:44, 60.62it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  73%|██▉ | 7358/10067 [01:30<00:43, 62.26it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  73%|██▉ | 7365/10067 [01:30<00:43, 61.50it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  73%|██▉ | 7372/10067 [01:30<00:43, 62.28it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  73%|██▉ | 7379/10067 [01:30<00:43, 61.55it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  73%|██▉ | 7386/10067 [01:30<00:44, 60.85it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  73%|██▉ | 7393/10067 [01:30<00:43, 61.54it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  74%|██▉ | 7400/10067 [01:30<00:44, 60.01it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  74%|██▉ | 7407/10067 [01:31<00:43, 60.75it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  74%|██▉ | 7414/10067 [01:31<00:43, 60.54it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  74%|██▉ | 7421/10067 [01:31<00:44, 59.79it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  74%|██▉ | 7427/10067 [01:31<00:44, 59.45it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  74%|██▉ | 7433/10067 [01:31<00:44, 58.84it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  74%|██▉ | 7439/10067 [01:31<00:44, 58.79it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  74%|██▉ | 7445/10067 [01:31<00:44, 58.37it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  74%|██▉ | 7451/10067 [01:31<00:44, 58.38it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  74%|██▉ | 7457/10067 [01:31<00:45, 57.41it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  74%|██▉ | 7463/10067 [01:32<00:45, 57.51it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  74%|██▉ | 7469/10067 [01:32<00:44, 58.09it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  74%|██▉ | 7475/10067 [01:32<00:44, 57.95it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  74%|██▉ | 7482/10067 [01:32<00:44, 58.44it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  74%|██▉ | 7488/10067 [01:32<00:44, 58.43it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  74%|██▉ | 7494/10067 [01:32<00:44, 58.06it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  75%|██▉ | 7500/10067 [01:32<00:43, 58.36it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  75%|██▉ | 7506/10067 [01:32<00:44, 58.12it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  75%|██▉ | 7513/10067 [01:32<00:43, 58.93it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  75%|██▉ | 7519/10067 [01:33<00:43, 58.22it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  75%|██▉ | 7525/10067 [01:33<00:43, 58.17it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  75%|██▉ | 7531/10067 [01:33<00:43, 57.79it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  75%|██▉ | 7537/10067 [01:33<00:43, 58.31it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  75%|██▉ | 7543/10067 [01:33<00:43, 57.87it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  75%|██▉ | 7550/10067 [01:33<00:42, 58.54it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  75%|███ | 7557/10067 [01:33<00:42, 59.50it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  75%|███ | 7563/10067 [01:33<00:42, 58.82it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  75%|███ | 7569/10067 [01:33<00:42, 58.81it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  75%|███ | 7575/10067 [01:33<00:42, 58.44it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  75%|███ | 7581/10067 [01:34<00:42, 58.36it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  75%|███ | 7587/10067 [01:34<00:42, 58.08it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  75%|███ | 7593/10067 [01:34<00:42, 57.88it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  75%|███ | 7599/10067 [01:34<00:42, 57.51it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  76%|███ | 7605/10067 [01:34<00:42, 57.78it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  76%|███ | 7611/10067 [01:34<00:42, 57.82it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  76%|███ | 7617/10067 [01:34<00:42, 58.23it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  76%|███ | 7623/10067 [01:34<00:41, 58.67it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  76%|███ | 7629/10067 [01:34<00:41, 58.67it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  76%|███ | 7635/10067 [01:34<00:41, 58.71it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  76%|███ | 7641/10067 [01:35<00:41, 58.45it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  76%|███ | 7647/10067 [01:35<00:41, 58.38it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  76%|███ | 7653/10067 [01:35<00:41, 58.45it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  76%|███ | 7659/10067 [01:35<00:41, 57.71it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  76%|███ | 7665/10067 [01:35<00:41, 58.07it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  76%|███ | 7671/10067 [01:35<00:41, 57.70it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  76%|███ | 7677/10067 [01:35<00:41, 57.34it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  76%|███ | 7683/10067 [01:35<00:41, 57.29it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  76%|███ | 7689/10067 [01:35<00:41, 57.33it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  76%|███ | 7695/10067 [01:36<00:41, 57.62it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  76%|███ | 7701/10067 [01:36<00:41, 57.09it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  77%|███ | 7707/10067 [01:36<00:40, 57.89it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  77%|███ | 7714/10067 [01:36<00:40, 58.75it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  77%|███ | 7720/10067 [01:36<00:40, 58.61it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  77%|███ | 7726/10067 [01:36<00:40, 58.16it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  77%|███ | 7732/10067 [01:36<00:40, 58.37it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  77%|███ | 7738/10067 [01:36<00:39, 58.30it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  77%|███ | 7744/10067 [01:36<00:39, 58.63it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  77%|███ | 7750/10067 [01:36<00:39, 58.52it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  77%|███ | 7756/10067 [01:37<00:39, 58.18it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  77%|███ | 7762/10067 [01:37<00:39, 58.69it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  77%|███ | 7768/10067 [01:37<00:39, 58.45it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  77%|███ | 7774/10067 [01:37<00:39, 58.32it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  77%|███ | 7780/10067 [01:37<00:39, 57.76it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  77%|███ | 7786/10067 [01:37<00:39, 58.22it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  77%|███ | 7792/10067 [01:37<00:39, 58.22it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  77%|███ | 7798/10067 [01:37<00:38, 58.62it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  78%|███ | 7804/10067 [01:37<00:38, 58.04it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  78%|███ | 7810/10067 [01:38<00:38, 58.24it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  78%|███ | 7816/10067 [01:38<00:38, 58.14it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  78%|███ | 7822/10067 [01:38<00:38, 58.21it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  78%|███ | 7829/10067 [01:38<00:38, 58.62it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  78%|███ | 7835/10067 [01:38<00:38, 58.73it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  78%|███ | 7841/10067 [01:38<00:37, 59.06it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  78%|███ | 7847/10067 [01:38<00:37, 58.49it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  78%|███ | 7854/10067 [01:38<00:37, 59.35it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  78%|███ | 7860/10067 [01:38<00:37, 59.52it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  78%|███▏| 7866/10067 [01:38<00:37, 59.29it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  78%|███▏| 7872/10067 [01:39<00:37, 58.83it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  78%|███▏| 7879/10067 [01:39<00:37, 58.99it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  78%|███▏| 7886/10067 [01:39<00:36, 59.32it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  78%|███▏| 7893/10067 [01:39<00:36, 59.55it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  78%|███▏| 7899/10067 [01:39<00:36, 58.84it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  79%|███▏| 7905/10067 [01:39<00:36, 58.48it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  79%|███▏| 7911/10067 [01:39<00:36, 58.56it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  79%|███▏| 7917/10067 [01:39<00:37, 57.99it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  79%|███▏| 7923/10067 [01:39<00:37, 57.35it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  79%|███▏| 7930/10067 [01:40<00:36, 58.84it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  79%|███▏| 7936/10067 [01:40<00:36, 57.88it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  79%|███▏| 7942/10067 [01:40<00:36, 57.95it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  79%|███▏| 7948/10067 [01:40<00:36, 58.14it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  79%|███▏| 7954/10067 [01:40<00:36, 58.26it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  79%|███▏| 7960/10067 [01:40<00:36, 57.96it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  79%|███▏| 7966/10067 [01:40<00:36, 57.39it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  79%|███▏| 7972/10067 [01:40<00:36, 58.10it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  79%|███▏| 7978/10067 [01:40<00:36, 57.62it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  79%|███▏| 7985/10067 [01:40<00:35, 58.85it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  79%|███▏| 7991/10067 [01:41<00:35, 59.13it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  79%|███▏| 7997/10067 [01:41<00:35, 58.35it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  79%|███▏| 8003/10067 [01:41<00:35, 57.77it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  80%|███▏| 8009/10067 [01:41<00:35, 57.31it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  80%|███▏| 8015/10067 [01:41<00:35, 57.23it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  80%|███▏| 8021/10067 [01:41<00:35, 57.21it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  80%|███▏| 8027/10067 [01:41<00:35, 57.03it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  80%|███▏| 8033/10067 [01:41<00:35, 57.44it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  80%|███▏| 8039/10067 [01:41<00:35, 57.48it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  80%|███▏| 8045/10067 [01:42<00:35, 57.57it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  80%|███▏| 8052/10067 [01:42<00:33, 59.89it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  80%|███▏| 8058/10067 [01:42<00:33, 59.36it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  80%|███▏| 8064/10067 [01:42<00:34, 58.90it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  80%|███▏| 8070/10067 [01:42<00:34, 58.57it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  80%|███▏| 8076/10067 [01:42<00:34, 58.06it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  80%|███▏| 8082/10067 [01:42<00:34, 57.72it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  80%|███▏| 8088/10067 [01:42<00:34, 57.96it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  80%|███▏| 8094/10067 [01:42<00:33, 58.20it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  80%|███▏| 8100/10067 [01:42<00:33, 58.01it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  81%|███▏| 8107/10067 [01:43<00:33, 58.49it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  81%|███▏| 8113/10067 [01:43<00:33, 58.26it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  81%|███▏| 8120/10067 [01:43<00:32, 59.19it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  81%|███▏| 8127/10067 [01:43<00:32, 59.94it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  81%|███▏| 8134/10067 [01:43<00:32, 59.74it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  81%|███▏| 8140/10067 [01:43<00:32, 59.51it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  81%|███▏| 8146/10067 [01:43<00:32, 58.39it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  81%|███▏| 8152/10067 [01:43<00:33, 57.71it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  81%|███▏| 8159/10067 [01:43<00:32, 57.88it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  81%|███▏| 8165/10067 [01:44<00:32, 58.29it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  81%|███▏| 8175/10067 [01:44<00:27, 68.71it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  81%|███▎| 8184/10067 [01:44<00:26, 72.22it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  81%|███▎| 8192/10067 [01:44<00:25, 73.90it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  81%|███▎| 8200/10067 [01:44<00:25, 73.53it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  82%|███▎| 8209/10067 [01:44<00:23, 77.84it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  82%|███▎| 8218/10067 [01:44<00:23, 80.01it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  82%|███▎| 8228/10067 [01:44<00:22, 83.21it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  82%|███▎| 8237/10067 [01:44<00:21, 83.96it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  82%|███▎| 8246/10067 [01:45<00:21, 83.94it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  82%|███▎| 8255/10067 [01:45<00:21, 84.50it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  82%|███▎| 8264/10067 [01:45<00:21, 84.59it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  82%|███▎| 8273/10067 [01:45<00:24, 74.65it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  82%|███▎| 8281/10067 [01:45<00:23, 75.62it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  82%|███▎| 8289/10067 [01:45<00:24, 72.24it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  82%|███▎| 8298/10067 [01:45<00:23, 75.84it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  83%|███▎| 8307/10067 [01:45<00:22, 78.88it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  83%|███▎| 8316/10067 [01:45<00:21, 81.08it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  83%|███▎| 8325/10067 [01:46<00:21, 82.60it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  83%|███▎| 8334/10067 [01:46<00:20, 83.68it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  83%|███▎| 8343/10067 [01:46<00:20, 84.35it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  83%|███▎| 8352/10067 [01:46<00:20, 84.73it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  83%|███▎| 8361/10067 [01:46<00:20, 84.75it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  83%|███▎| 8370/10067 [01:46<00:19, 85.00it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  83%|███▎| 8379/10067 [01:46<00:19, 85.63it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  83%|███▎| 8388/10067 [01:46<00:19, 85.73it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  83%|███▎| 8397/10067 [01:46<00:19, 85.69it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  84%|███▎| 8406/10067 [01:46<00:19, 85.83it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  84%|███▎| 8415/10067 [01:47<00:19, 86.09it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  84%|███▎| 8424/10067 [01:47<00:18, 86.56it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  84%|███▎| 8433/10067 [01:47<00:19, 85.79it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  84%|███▎| 8442/10067 [01:47<00:19, 85.09it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  84%|███▎| 8451/10067 [01:47<00:18, 85.31it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  84%|███▎| 8460/10067 [01:47<00:19, 84.55it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  84%|███▎| 8469/10067 [01:47<00:18, 85.14it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  84%|███▎| 8478/10067 [01:47<00:18, 85.59it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  84%|███▎| 8487/10067 [01:48<00:21, 72.70it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  84%|███▍| 8496/10067 [01:48<00:20, 75.71it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  84%|███▍| 8504/10067 [01:48<00:20, 75.66it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  85%|███▍| 8513/10067 [01:48<00:19, 78.37it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  85%|███▍| 8522/10067 [01:48<00:19, 79.92it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  85%|███▍| 8531/10067 [01:48<00:18, 81.31it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  85%|███▍| 8540/10067 [01:48<00:18, 82.46it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  85%|███▍| 8549/10067 [01:48<00:18, 83.16it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  85%|███▍| 8558/10067 [01:48<00:17, 84.02it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  85%|███▍| 8567/10067 [01:48<00:17, 83.65it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  85%|███▍| 8576/10067 [01:49<00:17, 84.54it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  85%|███▍| 8585/10067 [01:49<00:17, 84.95it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  85%|███▍| 8594/10067 [01:49<00:19, 76.32it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  85%|███▍| 8602/10067 [01:49<00:19, 76.62it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  86%|███▍| 8611/10067 [01:49<00:18, 79.05it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  86%|███▍| 8620/10067 [01:49<00:17, 81.35it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  86%|███▍| 8629/10067 [01:49<00:17, 82.68it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  86%|███▍| 8638/10067 [01:49<00:17, 83.71it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  86%|███▍| 8647/10067 [01:49<00:18, 76.42it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  86%|███▍| 8656/10067 [01:50<00:17, 79.63it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  86%|███▍| 8665/10067 [01:50<00:17, 82.12it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  86%|███▍| 8674/10067 [01:50<00:16, 83.59it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  86%|███▍| 8683/10067 [01:50<00:16, 85.04it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  86%|███▍| 8692/10067 [01:50<00:16, 85.66it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  86%|███▍| 8701/10067 [01:50<00:15, 85.84it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  87%|███▍| 8710/10067 [01:50<00:16, 82.28it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  87%|███▍| 8719/10067 [01:50<00:16, 83.15it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  87%|███▍| 8728/10067 [01:50<00:15, 84.91it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  87%|███▍| 8737/10067 [01:51<00:15, 85.71it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  87%|███▍| 8746/10067 [01:51<00:15, 86.02it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  87%|███▍| 8755/10067 [01:51<00:15, 85.86it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  87%|███▍| 8764/10067 [01:51<00:15, 85.98it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  87%|███▍| 8773/10067 [01:51<00:17, 75.50it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  87%|███▍| 8782/10067 [01:51<00:16, 78.42it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  87%|███▍| 8791/10067 [01:51<00:15, 81.19it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  87%|███▍| 8800/10067 [01:51<00:15, 83.52it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  88%|███▌| 8809/10067 [01:51<00:14, 85.25it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  88%|███▌| 8818/10067 [01:51<00:14, 86.18it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  88%|███▌| 8827/10067 [01:52<00:14, 86.29it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  88%|███▌| 8836/10067 [01:52<00:14, 86.99it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  88%|███▌| 8845/10067 [01:52<00:15, 79.73it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  88%|███▌| 8854/10067 [01:52<00:15, 76.08it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  88%|███▌| 8863/10067 [01:52<00:15, 79.58it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  88%|███▌| 8873/10067 [01:52<00:14, 82.76it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  88%|███▌| 8882/10067 [01:52<00:14, 84.38it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  88%|███▌| 8891/10067 [01:52<00:13, 85.88it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  88%|███▌| 8901/10067 [01:52<00:13, 87.92it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  89%|███▌| 8911/10067 [01:53<00:12, 89.28it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  89%|███▌| 8921/10067 [01:53<00:12, 89.97it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  89%|███▌| 8931/10067 [01:53<00:12, 90.24it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  89%|███▌| 8941/10067 [01:53<00:12, 90.95it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  89%|███▌| 8951/10067 [01:53<00:12, 91.61it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  89%|███▌| 8961/10067 [01:53<00:12, 91.46it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  89%|███▌| 8971/10067 [01:53<00:11, 91.66it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  89%|███▌| 8981/10067 [01:53<00:11, 90.94it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  89%|███▌| 8991/10067 [01:53<00:11, 91.28it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  89%|███▌| 9001/10067 [01:54<00:11, 91.93it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  90%|███▌| 9011/10067 [01:54<00:11, 91.78it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  90%|███▌| 9021/10067 [01:54<00:11, 91.89it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  90%|███▌| 9031/10067 [01:54<00:11, 92.41it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  90%|███▌| 9041/10067 [01:54<00:11, 91.85it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  90%|███▌| 9051/10067 [01:54<00:11, 91.70it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  90%|███▌| 9061/10067 [01:54<00:11, 91.05it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  90%|███▌| 9071/10067 [01:54<00:11, 88.77it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  90%|███▌| 9081/10067 [01:54<00:10, 89.81it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  90%|███▌| 9091/10067 [01:55<00:10, 90.35it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  90%|███▌| 9101/10067 [01:55<00:10, 91.48it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  91%|███▌| 9111/10067 [01:55<00:10, 92.21it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  91%|███▌| 9121/10067 [01:55<00:10, 91.92it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  91%|███▋| 9131/10067 [01:55<00:10, 92.15it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  91%|███▋| 9141/10067 [01:55<00:10, 92.00it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  91%|███▋| 9151/10067 [01:55<00:09, 91.98it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  91%|███▋| 9161/10067 [01:55<00:09, 91.99it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  91%|███▋| 9171/10067 [01:55<00:09, 91.58it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  91%|███▋| 9181/10067 [01:56<00:10, 85.13it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  91%|███▋| 9190/10067 [01:56<00:11, 78.55it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  91%|███▋| 9198/10067 [01:56<00:12, 69.65it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  91%|███▋| 9211/10067 [01:56<00:10, 83.28it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  92%|███▋| 9224/10067 [01:56<00:08, 94.31it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  92%|██▊| 9237/10067 [01:56<00:08, 102.51it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  92%|██▊| 9249/10067 [01:56<00:07, 106.34it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  92%|██▊| 9262/10067 [01:56<00:07, 110.59it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  92%|██▊| 9274/10067 [01:56<00:07, 113.13it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  92%|██▊| 9287/10067 [01:57<00:06, 115.57it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  92%|██▊| 9300/10067 [01:57<00:06, 117.33it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  93%|██▊| 9313/10067 [01:57<00:06, 118.43it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  93%|██▊| 9326/10067 [01:57<00:06, 119.55it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  93%|██▊| 9339/10067 [01:57<00:06, 120.81it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  93%|██▊| 9352/10067 [01:57<00:05, 120.86it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  93%|██▊| 9365/10067 [01:57<00:05, 121.13it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  93%|██▊| 9378/10067 [01:57<00:05, 120.85it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  93%|██▊| 9391/10067 [01:57<00:05, 121.09it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  93%|██▊| 9404/10067 [01:58<00:05, 121.42it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  94%|██▊| 9417/10067 [01:58<00:05, 120.96it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  94%|██▊| 9430/10067 [01:58<00:05, 120.96it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  94%|██▊| 9443/10067 [01:58<00:05, 121.38it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  94%|██▊| 9456/10067 [01:58<00:05, 121.73it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  94%|██▊| 9469/10067 [01:58<00:04, 122.13it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  94%|██▊| 9482/10067 [01:58<00:04, 122.28it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  94%|██▊| 9495/10067 [01:58<00:04, 121.70it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  94%|██▊| 9508/10067 [01:58<00:04, 122.04it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  95%|██▊| 9521/10067 [01:59<00:04, 121.68it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  95%|██▊| 9534/10067 [01:59<00:04, 121.28it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  95%|██▊| 9547/10067 [01:59<00:04, 121.88it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  95%|██▊| 9560/10067 [01:59<00:04, 121.46it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  95%|██▊| 9573/10067 [01:59<00:04, 121.18it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  95%|██▊| 9586/10067 [01:59<00:03, 121.89it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  95%|██▊| 9599/10067 [01:59<00:03, 122.23it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  95%|██▊| 9612/10067 [01:59<00:03, 122.18it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  96%|██▊| 9625/10067 [01:59<00:03, 122.65it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  96%|██▊| 9638/10067 [01:59<00:03, 122.43it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  96%|██▉| 9651/10067 [02:00<00:03, 122.47it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  96%|██▉| 9664/10067 [02:00<00:03, 122.09it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  96%|██▉| 9677/10067 [02:00<00:03, 121.59it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  96%|██▉| 9690/10067 [02:00<00:03, 122.24it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  96%|██▉| 9703/10067 [02:00<00:02, 122.59it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  97%|██▉| 9716/10067 [02:00<00:02, 123.03it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  97%|██▉| 9729/10067 [02:00<00:02, 123.12it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  97%|██▉| 9742/10067 [02:00<00:02, 123.12it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  97%|██▉| 9755/10067 [02:00<00:02, 122.95it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  97%|██▉| 9768/10067 [02:01<00:02, 122.17it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  97%|██▉| 9781/10067 [02:01<00:02, 122.08it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  97%|██▉| 9794/10067 [02:01<00:02, 122.49it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  97%|██▉| 9807/10067 [02:01<00:02, 122.57it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  98%|██▉| 9820/10067 [02:01<00:02, 122.76it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  98%|██▉| 9833/10067 [02:01<00:01, 122.92it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  98%|██▉| 9846/10067 [02:01<00:01, 123.32it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  98%|██▉| 9859/10067 [02:01<00:01, 123.20it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  98%|██▉| 9872/10067 [02:01<00:01, 123.04it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  98%|██▉| 9885/10067 [02:01<00:01, 123.21it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  98%|██▉| 9898/10067 [02:02<00:01, 123.26it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  98%|██▉| 9911/10067 [02:02<00:01, 123.52it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  99%|██▉| 9924/10067 [02:02<00:01, 123.83it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  99%|██▉| 9937/10067 [02:02<00:01, 123.89it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  99%|██▉| 9950/10067 [02:02<00:00, 124.15it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  99%|██▉| 9963/10067 [02:02<00:00, 124.48it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  99%|██▉| 9976/10067 [02:02<00:00, 124.52it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  99%|██▉| 9989/10067 [02:02<00:00, 124.61it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  99%|█▉| 10002/10067 [02:02<00:00, 124.65it/s]\u001b[A\n",
      "bert-base-uncased (embed subset):  99%|█▉| 10015/10067 [02:03<00:00, 124.95it/s]\u001b[A\n",
      "bert-base-uncased (embed subset): 100%|█▉| 10028/10067 [02:03<00:00, 125.03it/s]\u001b[A\n",
      "bert-base-uncased (embed subset): 100%|█▉| 10041/10067 [02:03<00:00, 125.04it/s]\u001b[A\n",
      "bert-base-uncased (embed subset): 100%|█▉| 10054/10067 [02:03<00:00, 125.04it/s]\u001b[A\n",
      "bert-base-uncased (embed subset): 100%|███| 10067/10067 [02:03<00:00, 81.55it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ embedded 115,852 tokens  • layers=13  • rep_mode=first\n",
      "\n",
      "→ Computing metric: gride …\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel, GPT2TokenizerFast\n",
    "\n",
    "# =========================== Optional deps ===========================\n",
    "HAS_DADAPY = False\n",
    "try:\n",
    "    from dadapy import Data  # DADApy ID estimators (TwoNN, GRIDE)\n",
    "    HAS_DADAPY = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# IsoScore: library if available, else a monotone fallback\n",
    "try:\n",
    "    from isoscore import IsoScore\n",
    "    _HAS_ISOSCORE = True\n",
    "except Exception:\n",
    "    _HAS_ISOSCORE = False\n",
    "    class _IsoScoreFallback:\n",
    "        @staticmethod\n",
    "        def IsoScore(X: np.ndarray) -> float:\n",
    "            C = np.cov(X.T, ddof=0)\n",
    "            ev = np.linalg.eigvalsh(C)\n",
    "            if ev.mean() <= 0 or ev[-1] <= 0:\n",
    "                return 0.0\n",
    "            # mean / max eigenvalue in [0,1] (↑ ~ more isotropic)\n",
    "            return float(np.clip(ev.mean() / ev[-1], 0.0, 1.0))\n",
    "    IsoScore = _IsoScoreFallback()\n",
    "\n",
    "# =============================== CONFIG ===============================\n",
    "CSV_PATH   = \"en_ewt-ud-train_sentences.csv\"\n",
    "\n",
    "# Set to \"gpt2\" (decoder) or \"bert-base-uncased\" (encoder)\n",
    "BASELINE   = \"bert-base-uncased\"          # <-- you said you use GPT-2\n",
    "WORD_REP_MODE = \"first\"       # <-- and \"last\"\n",
    "\n",
    "# No per-class cap for the fast metrics\n",
    "RAW_MAX_PER_CLASS = int(1e12)\n",
    "\n",
    "# Cap overrides: cap only arity \"0\" to 30_000; others use RAW_MAX_PER_CLASS\n",
    "PER_CLASS_CAPS: Dict[str, int] = {\"0\": 50_000}\n",
    "\n",
    "# Bootstrap replicates (tune down if slow)\n",
    "N_BOOTSTRAP_FAST   = 50\n",
    "N_BOOTSTRAP_HEAVY  = 200\n",
    "\n",
    "# Per-replicate sample size (M = min(cap, N_class))\n",
    "FAST_BS_MAX_SAMP_PER_CLASS  = int(1e12)\n",
    "HEAVY_BS_MAX_SAMP_PER_CLASS = 5000\n",
    "\n",
    "RAND_SEED=42\n",
    "PLOT_DIR     = Path(\"results_ARITY\"); PLOT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CSV_DIR      = Path(\"tables_ARITY\") / \"arity_bootstrap\"; CSV_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Throughput (raise if you have more GPU memory)\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Reproducibility & device\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Seaborn style\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "EPS = 1e-12\n",
    "\n",
    "# =============================== HELPERS ===============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _center(X: np.ndarray) -> np.ndarray:\n",
    "    return X - X.mean(0, keepdims=True)\n",
    "\n",
    "def _eigvals_from_X(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Eigenvalues of covariance up to a constant via SVD of centered X (descending).\"\"\"\n",
    "    Xc = _center(X.astype(np.float32, copy=False))\n",
    "    try:\n",
    "        _, S, _ = np.linalg.svd(Xc, full_matrices=False)\n",
    "        lam = (S**2).astype(np.float64)\n",
    "        lam.sort()\n",
    "        return lam[::-1]\n",
    "    except Exception:\n",
    "        return np.array([], dtype=np.float64)\n",
    "\n",
    "def _jitter_unique(X: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"Add tiny noise if there are duplicate rows (helps NN-based estimators).\"\"\"\n",
    "    try:\n",
    "        if np.unique(X, axis=0).shape[0] < X.shape[0]:\n",
    "            X = X + np.random.normal(scale=eps, size=X.shape).astype(X.dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return X\n",
    "\n",
    "def _num_hidden_layers(model) -> int:\n",
    "    n = getattr(model.config, \"num_hidden_layers\", None)\n",
    "    if n is None: n = getattr(model.config, \"n_layer\", None)\n",
    "    if n is None: raise ValueError(\"Cannot determine num_hidden_layers from model.config\")\n",
    "    return int(n)\n",
    "\n",
    "def _hidden_size(model) -> int:\n",
    "    d = getattr(model.config, \"hidden_size\", None)\n",
    "    if d is None: d = getattr(model.config, \"n_embd\", None)\n",
    "    if d is None: raise ValueError(\"Cannot determine hidden_size from model.config\")\n",
    "    return int(d)\n",
    "\n",
    "def _is_gpt_like(model) -> bool:\n",
    "    mt = str(getattr(model.config, \"model_type\", \"\")).lower()\n",
    "    name = str(getattr(getattr(model, \"name_or_path\", \"\"), \"lower\", lambda: \"\")())\n",
    "    return (\"gpt2\" in mt) or (\"gpt2\" in name)\n",
    "\n",
    "def _pick_arity_col(df: pd.DataFrame) -> str:\n",
    "    for cand in [\"arity\", \"ariety\", \"ARITY\", \"Arity\"]:\n",
    "        if cand in df.columns: return cand\n",
    "    raise ValueError(\"No arity column found. Expected one of: arity, ariety, ARITY, Arity\")\n",
    "\n",
    "# ========= Per-subsample single-value compute functions (used inside bootstrap) =========\n",
    "def _iso_once(X: np.ndarray) -> float:\n",
    "    return float(IsoScore.IsoScore(X))\n",
    "\n",
    "def _pcaXX_once(X: np.ndarray, var_ratio: float) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    c = np.cumsum(lam); thr = c[-1] * var_ratio\n",
    "    return float(np.searchsorted(c, thr) + 1)\n",
    "\n",
    "def _pca99_once(X: np.ndarray) -> float:\n",
    "    return _pcaXX_once(X, 0.99)\n",
    "\n",
    "def _erank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    p = lam / (lam.sum() + EPS)\n",
    "    H = -(p * np.log(p + EPS)).sum()\n",
    "    return float(np.exp(H))\n",
    "\n",
    "def _pr_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    s1 = lam.sum(); s2 = (lam**2).sum()\n",
    "    return float((s1**2) / (s2 + EPS))\n",
    "\n",
    "def _stable_rank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    return float(lam.sum() / (lam.max() + EPS))\n",
    "\n",
    "def _dadapy_twonn_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    id_est, _, _ = d.compute_id_2NN()\n",
    "    return float(id_est)\n",
    "\n",
    "def _dadapy_gride_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    d.compute_distances(maxk=64)\n",
    "    ids, _, _ = d.return_id_scaling_gride(range_max=64)\n",
    "    return float(ids[-1])\n",
    "\n",
    "# =============================== DATA (arity already in CSV) ===============================\n",
    "def load_arity_df(csv_path: str):\n",
    "    \"\"\"\n",
    "    Reads sentence-level rows with list columns: tokens, <arity>.\n",
    "    Expands to one row per token with an arity class in {0,1,2,3,4} (4 = 4+).\n",
    "    Returns:\n",
    "      df_sent (sent-level with tokens),\n",
    "      df_tok  (token-level: sentence_id, word_id, arity_class, word)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, usecols=[\"sentence_id\",\"tokens\"], dtype={\"sentence_id\": str})\n",
    "    df_all = pd.read_csv(csv_path)\n",
    "    ar_col = _pick_arity_col(df_all)\n",
    "    df[ar_col] = df_all[ar_col]\n",
    "\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df.tokens = df.tokens.apply(_to_list)\n",
    "    df[ar_col] = df[ar_col].apply(_to_list)\n",
    "\n",
    "    rows = []\n",
    "    for sid, toks, A in df[[\"sentence_id\",\"tokens\", ar_col]].itertuples(index=False):\n",
    "        L = min(len(toks), len(A))\n",
    "        for wid, (tok, a) in enumerate(zip(toks[:L], A[:L])):\n",
    "            try:\n",
    "                ai = int(a)\n",
    "            except Exception:\n",
    "                ai = 0\n",
    "            cl = str(min(max(ai, 0), 4))   # cap at 4 (means 4+)\n",
    "            rows.append((sid, wid, cl, tok))\n",
    "    df_tok = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\",\"arity_class\",\"word\"])\n",
    "    return df[[\"sentence_id\",\"tokens\"]].drop_duplicates(\"sentence_id\"), df_tok\n",
    "\n",
    "def sample_raw(df_tok: pd.DataFrame,\n",
    "               per_class_cap: int = RAW_MAX_PER_CLASS,\n",
    "               per_class_caps: Dict[str, int] | None = None) -> pd.DataFrame:\n",
    "    \"\"\"Per-class cap without frequency matching (with optional overrides per class).\"\"\"\n",
    "    picks = []\n",
    "    caps = per_class_caps or {}\n",
    "    for c, sub in df_tok.groupby(\"arity_class\", sort=False):\n",
    "        cap = caps.get(str(c), per_class_cap)\n",
    "        n = min(len(sub), cap)\n",
    "        picks.append(sub.sample(n, random_state=RAND_SEED, replace=False))\n",
    "    return pd.concat(picks, ignore_index=True)\n",
    "\n",
    "def make_class_palette(classes: List[str]) -> Dict[str, Tuple[float, float, float]]:\n",
    "    base_colors: List[Tuple[float, float, float]] = []\n",
    "    for name in (\"tab20\", \"tab20b\", \"tab20c\"):\n",
    "        try: base_colors.extend(sns.color_palette(name, 20))\n",
    "        except Exception: pass\n",
    "    if len(base_colors) < len(classes):\n",
    "        base_colors = list(sns.color_palette(\"husl\", len(classes)))\n",
    "    ordered = list(sorted(classes, key=lambda s: int(s) if s.isdigit() else 99))\n",
    "    return {cls: base_colors[i % len(base_colors)] for i, cls in enumerate(ordered)}\n",
    "\n",
    "# =============================== TOKENIZER/MODEL LOADING ===============================\n",
    "def _load_tokenizer_and_model(baseline: str):\n",
    "    \"\"\"\n",
    "    Robustly load tokenizer+model. For GPT-2, prefer GPT2TokenizerFast and\n",
    "    try both 'gpt2' and 'openai-community/gpt2' repo IDs.\n",
    "    \"\"\"\n",
    "    candidates = [baseline]\n",
    "    b = baseline.lower()\n",
    "    if \"gpt2\" in b:\n",
    "        # try both IDs (some environments only have one cached / accessible)\n",
    "        if baseline != \"openai-community/gpt2\":\n",
    "            candidates.append(\"openai-community/gpt2\")\n",
    "        if baseline != \"gpt2\":\n",
    "            candidates.append(\"gpt2\")\n",
    "\n",
    "    last_err = None\n",
    "    for mid in candidates:\n",
    "        try:\n",
    "            if \"gpt2\" in mid.lower():\n",
    "                tokzr = GPT2TokenizerFast.from_pretrained(mid, add_prefix_space=True)\n",
    "            else:\n",
    "                tokzr = AutoTokenizer.from_pretrained(mid, use_fast=True, add_prefix_space=True)\n",
    "            model = AutoModel.from_pretrained(mid, output_hidden_states=True)\n",
    "            return tokzr, model, mid\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    # If we got here, surface the most informative error\n",
    "    raise RuntimeError(f\"Failed to load tokenizer/model for any of {candidates}: {last_err}\")\n",
    "\n",
    "# =============================== EMBEDDING (BERT & GPT‑2) ===============================\n",
    "def embed_subset(df_sent: pd.DataFrame,\n",
    "                 subset_df: pd.DataFrame,\n",
    "                 baseline: str = BASELINE,\n",
    "                 word_rep_mode: str = WORD_REP_MODE,\n",
    "                 batch_size: int = BATCH_SIZE) -> Tuple[np.ndarray, np.ndarray, str]:\n",
    "    \"\"\"\n",
    "    Return (reps, filled, rep_mode_used).\n",
    "    reps shape: (L, N, D) where L includes the embedding layer (layer 0).\n",
    "    \"\"\"\n",
    "    df_sent[\"sentence_id\"]  = df_sent[\"sentence_id\"].astype(str)\n",
    "    subset_df[\"sentence_id\"] = subset_df[\"sentence_id\"].astype(str)\n",
    "\n",
    "    # sid -> list[(global_idx, word_id)]\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(subset_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(str(sid), []).append((gidx, int(wid)))\n",
    "\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_sent[df_sent.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    # Robust tokenizer/model load (fixes GPT-2 \"vocab_file NoneType\" crashes)\n",
    "    tokzr, model, model_id_used = _load_tokenizer_and_model(baseline)\n",
    "\n",
    "    # Pre-tokenized input with word mapping\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # GPT‑2 specifics: add_prefix_space + pad token\n",
    "    if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "        enc_kwargs[\"add_prefix_space\"] = True\n",
    "    if tokzr.pad_token is None and getattr(tokzr, \"eos_token\", None) is not None:\n",
    "        tokzr.pad_token = tokzr.eos_token  # safe default for GPT-2\n",
    "\n",
    "    if getattr(model.config, \"pad_token_id\", None) is None and tokzr.pad_token_id is not None:\n",
    "        model.config.pad_token_id = tokzr.pad_token_id\n",
    "    if device == \"cuda\": model.half()\n",
    "    model = model.eval().to(device)\n",
    "\n",
    "    L = _num_hidden_layers(model) + 1   # include embeddings\n",
    "    D = _hidden_size(model)\n",
    "    N = len(subset_df)\n",
    "    reps   = np.zeros((L, N, D), np.float16)\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    # Choose/validate rep mode depending on model family\n",
    "    gpt_like = _is_gpt_like(model)\n",
    "    if gpt_like:\n",
    "        rep_mode = word_rep_mode if word_rep_mode in {\"last\",\"mean\"} else \"last\"\n",
    "    else:\n",
    "        rep_mode = word_rep_mode if word_rep_mode in {\"first\",\"mean\",\"last\"} else \"first\"\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{model_id_used} (embed subset)\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L,B,T,D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                # Map word_id -> token positions for this item\n",
    "                word_map = {}\n",
    "                # NOTE: word_ids() requires a *fast* tokenizer; ensured by GPT2TokenizerFast above.\n",
    "                for tidx, wid in enumerate(enc_be.word_ids(b)):\n",
    "                    if wid is not None:\n",
    "                        word_map.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = word_map.get(wid)\n",
    "                    if not toks: continue\n",
    "                    if rep_mode == \"first\":\n",
    "                        vec = h[:, b, toks[0], :]\n",
    "                    elif rep_mode == \"last\":\n",
    "                        vec = h[:, b, toks[-1], :]\n",
    "                    else:  # \"mean\"\n",
    "                        vec = h[:, b, toks, :].mean(axis=1)\n",
    "                    reps[:, gidx, :] = vec.astype(np.float16, copy=False)\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    missing = int((~filled).sum())\n",
    "    if missing: print(f\"⚠ Missing vectors for {missing} of {N} sampled words\")\n",
    "    del model; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    return reps, filled, rep_mode\n",
    "\n",
    "# =============================== BOOTSTRAP CORE ===============================\n",
    "def _bs_layer_loop(rep_sub: np.ndarray, M: int, n_reps: int, compute_once: Callable[[np.ndarray], float]):\n",
    "    \"\"\"Bootstrap: sample M with replacement and apply compute_once(X_layer) -> scalar for each layer.\"\"\"\n",
    "    L, N, D = rep_sub.shape\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    A = np.full((n_reps, L), np.nan, np.float32)\n",
    "    for r in range(n_reps):\n",
    "        idx = rng.integers(0, N, size=M)\n",
    "        for l in range(L):\n",
    "            X = rep_sub[l, idx].astype(np.float32, copy=False)\n",
    "            try:    A[r, l] = float(compute_once(X))\n",
    "            except Exception: A[r, l] = np.nan\n",
    "    mu = np.nanmean(A, axis=0).astype(np.float32)\n",
    "    lo = np.nanpercentile(A, 2.5, axis=0).astype(np.float32)\n",
    "    hi = np.nanpercentile(A, 97.5, axis=0).astype(np.float32)\n",
    "    return mu, lo, hi\n",
    "\n",
    "# ---- Metric registries ----\n",
    "FAST_ONCE: Dict[str, Callable[[np.ndarray], float]] = {\n",
    "    \"iso\": _iso_once,     # IsoScore\n",
    "    \"pca99\": _pca99_once  # lPCA @ 0.99 explained variance\n",
    "}\n",
    "\n",
    "HEAVY_ONCE: Dict[str, Callable[[np.ndarray], float] | None] = {\n",
    "    \"gride\": _dadapy_gride_once\n",
    "}\n",
    "\n",
    "LABELS = {\n",
    "    \"pca99\":\"lPCA 0.99\",\n",
    "    \"gride\":\"GRIDE\"\n",
    "}\n",
    "\n",
    "# Keep runtime reasonable (add more if you want)\n",
    "ALL_METRICS = [\"gride\", \"pca99\", \"iso\"]\n",
    "\n",
    "# =============================== SAVE / PLOT ===============================\n",
    "def save_metric_csv_all_classes(metric: str,\n",
    "                                class_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                                layers: np.ndarray,\n",
    "                                baseline: str,\n",
    "                                subset_name: str = \"raw\",\n",
    "                                rep_mode_used: str = \"\"):\n",
    "    rows = []\n",
    "    for c, stats in class_to_stats.items():\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        for l, val in enumerate(mu):\n",
    "            rows.append({\n",
    "                \"subset\": subset_name, \"model\": baseline, \"feature\": \"arity\",\n",
    "                \"class\": c, \"metric\": metric, \"layer\": int(layers[l]),\n",
    "                \"mean\": float(val) if np.isfinite(val) else np.nan,\n",
    "                \"ci_low\": float(lo[l]) if isinstance(lo, np.ndarray) and np.isfinite(lo[l]) else np.nan,\n",
    "                \"ci_high\": float(hi[l]) if isinstance(hi, np.ndarray) and np.isfinite(hi[l]) else np.nan,\n",
    "                \"n_tokens\": int(stats.get(\"n\", 0)),\n",
    "                \"word_rep_mode\": rep_mode_used or WORD_REP_MODE,\n",
    "                \"source_csv\": Path(CSV_PATH).name,\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    out = CSV_DIR / f\"arity_{subset_name}_{metric}_{baseline}.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "\n",
    "def plot_metric_with_ci(class_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                        layers: np.ndarray, metric: str, title: str, out_path: Path,\n",
    "                        palette: Dict[str, Tuple[float, float, float]] | None = None):\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    for c in sorted(class_to_stats.keys(), key=lambda s: int(s)):\n",
    "        stats = class_to_stats[c]\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        if mu is None or np.all(np.isnan(mu)): continue\n",
    "        color = palette.get(c) if isinstance(palette, dict) else None\n",
    "        plt.plot(layers, mu, label=c, lw=1.8, color=color)\n",
    "        if isinstance(lo, np.ndarray) and isinstance(hi, np.ndarray) and not np.all(np.isnan(lo)):\n",
    "            plt.fill_between(layers, lo, hi, alpha=0.15, color=color)\n",
    "    plt.xlabel(\"Layer\"); plt.ylabel(LABELS.get(metric, metric.upper())); plt.title(title)\n",
    "    plt.legend(ncol=3, fontsize=\"small\", title=\"Arity (4 = 4+)\", frameon=False)\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=220); plt.close()\n",
    "\n",
    "# =============================== DRIVER ===============================\n",
    "def run_arity_pipeline():\n",
    "    # 1) Load tokens + arity classes\n",
    "    df_sent, ar_df = load_arity_df(CSV_PATH)\n",
    "    classes = sorted(ar_df.arity_class.unique(), key=lambda s: int(s))\n",
    "    palette = make_class_palette(classes)\n",
    "    print(f\"✓ corpus ready — {len(ar_df):,} tokens across arity classes {classes}\")\n",
    "\n",
    "    # 2) Optional per-class cap (cap arity \"0\" to 30k via PER_CLASS_CAPS)\n",
    "    raw_df = sample_raw(ar_df, RAW_MAX_PER_CLASS, PER_CLASS_CAPS)\n",
    "    print(\"Sample sizes per arity (raw cap):\")\n",
    "    print(raw_df.arity_class.value_counts().sort_index().to_dict())\n",
    "\n",
    "\n",
    "    type_counts = (\n",
    "        raw_df.groupby(\"arity_class\")[\"word\"]\n",
    "              .nunique()\n",
    "              .sort_index()\n",
    "              .to_dict()\n",
    "    )\n",
    "    print(\"Unique word types per arity (with class 0 capped at 30k tokens):\")\n",
    "    print(type_counts)\n",
    "\n",
    "\n",
    "    # 3) Embed once (encoder or decoder; rep mode auto-validated)\n",
    "    reps, filled, rep_mode_used = embed_subset(df_sent, raw_df, BASELINE, WORD_REP_MODE, BATCH_SIZE)\n",
    "    raw_df = raw_df.reset_index(drop=True).loc[filled].reset_index(drop=True)\n",
    "    cls_arr = raw_df.arity_class.values\n",
    "    L = reps.shape[0]; layers = np.arange(L)\n",
    "    print(f\"✓ embedded {len(raw_df):,} tokens  • layers={L}  • rep_mode={rep_mode_used}\")\n",
    "\n",
    "    # 4) Metric loop\n",
    "    for metric in ALL_METRICS:\n",
    "        print(f\"\\n→ Computing metric: {metric} …\")\n",
    "        compute_once = FAST_ONCE.get(metric) or HEAVY_ONCE.get(metric)\n",
    "        if compute_once is None:\n",
    "            print(f\"  (skipping {metric}: estimator unavailable)\")\n",
    "            continue\n",
    "\n",
    "        # choose bootstrap budget\n",
    "        n_bs = N_BOOTSTRAP_FAST if metric in FAST_ONCE else N_BOOTSTRAP_HEAVY\n",
    "        Mcap = FAST_BS_MAX_SAMP_PER_CLASS if metric in FAST_ONCE else HEAVY_BS_MAX_SAMP_PER_CLASS\n",
    "\n",
    "        class_results: Dict[str, Dict[str, np.ndarray]] = {}\n",
    "        for c in classes:\n",
    "            idx = np.where(cls_arr == c)[0]\n",
    "            if idx.size < 3:\n",
    "                continue\n",
    "            sub = reps[:, idx]  # (L, n_c, D)\n",
    "            Nc = sub.shape[1]\n",
    "            M = min(Mcap, Nc)\n",
    "            mu, lo, hi = _bs_layer_loop(sub, M, n_bs, compute_once)\n",
    "            class_results[c] = {\"mean\": mu, \"lo\": lo, \"hi\": hi, \"n\": int(Nc)}\n",
    "\n",
    "        save_metric_csv_all_classes(metric, class_results, layers, BASELINE,\n",
    "                                    subset_name=\"raw\", rep_mode_used=rep_mode_used)\n",
    "        plot_metric_with_ci(class_results, layers, metric,\n",
    "                            title=f\"{LABELS.get(metric, metric.upper())} • {BASELINE} • rep={rep_mode_used}\",\n",
    "                            out_path=PLOT_DIR / f\"arity_raw_{metric}_{BASELINE}_rep-{rep_mode_used}.png\",\n",
    "                            palette=palette)\n",
    "        print(f\"  ✓ saved: CSV= {CSV_DIR}/arity_raw_{metric}_{BASELINE}.csv  \"\n",
    "              f\"plot= {PLOT_DIR}/arity_raw_{metric}_{BASELINE}_rep-{rep_mode_used}.png\")\n",
    "\n",
    "        del class_results; gc.collect()\n",
    "        if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    del reps; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    print(\"\\n✓ done (incremental outputs produced per metric).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_arity_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b2380-e891-49bd-9cbc-1eba57f4c619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ plotting subset — 194,916 tokens across arity classes ['0', '1', '2', '3', '4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_155358/258873810.py:187: FutureWarning:\n",
      "\n",
      "`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "\n",
      "bert-base-uncased (embed subset):  57%|██▊  | 2850/5034 [00:28<00:22, 96.80it/s]"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, GPT2TokenizerFast\n",
    "\n",
    "# Plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.colors as pc\n",
    "\n",
    "# =============================== CONFIG ===============================\n",
    "CSV_PATH       = \"en_ewt-ud-train_sentences.csv\"   # needs: sentence_id, tokens (list[str]), arity (list[int])\n",
    "BASELINE       = \"bert-base-uncased\"                            # or \"openai-community/gpt2\" or \"bert-base-uncased\"\n",
    "WORD_REP_MODE  = \"first\"                            # BERT: {\"first\",\"last\",\"mean\"}; GPT-2: {\"last\",\"mean\"}\n",
    "\n",
    "# Optional: subsample for plotting smoothness (per class)\n",
    "PLOT_MAX_PER_CLASS = None                          # e.g., 4000; None = use all tokens\n",
    "\n",
    "# Output\n",
    "OUT_DIR  = Path(\"pca3d_arity\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "HTML_OUT = OUT_DIR / f\"{BASELINE.replace('/','_')}_arity_pca3d_layers.html\"\n",
    "\n",
    "# Throughput / device\n",
    "BATCH_SIZE = 2\n",
    "RAND_SEED  = 42\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# =============================== HELPERS ===============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _num_hidden_layers(model) -> int:\n",
    "    n = getattr(model.config, \"num_hidden_layers\", None)\n",
    "    if n is None: n = getattr(model.config, \"n_layer\", None)   # GPT-2\n",
    "    if n is None: raise ValueError(\"Cannot determine num_hidden_layers\")\n",
    "    return int(n)\n",
    "\n",
    "def _hidden_size(model) -> int:\n",
    "    d = getattr(model.config, \"hidden_size\", None)\n",
    "    if d is None: d = getattr(model.config, \"n_embd\", None)    # GPT-2\n",
    "    if d is None: raise ValueError(\"Cannot determine hidden size\")\n",
    "    return int(d)\n",
    "\n",
    "def _is_gpt_like(model) -> bool:\n",
    "    mt = str(getattr(model.config, \"model_type\", \"\")).lower()\n",
    "    name = str(getattr(getattr(model, \"name_or_path\", \"\"), \"lower\", lambda: \"\")())\n",
    "    return (\"gpt2\" in mt) or (\"gpt2\" in name)\n",
    "\n",
    "def _load_tok_and_model(model_id: str):\n",
    "    \"\"\"\n",
    "    Robust loader:\n",
    "    - GPT‑2: force *fast* tokenizer, try both 'gpt2' and 'openai-community/gpt2'\n",
    "      (fixes NoneType vocab path issues).\n",
    "    - Right padding; set PAD=EOS if missing (safe for batched inference).\n",
    "    \"\"\"\n",
    "    cands = [model_id]\n",
    "    if \"gpt2\" in model_id.lower():\n",
    "        if model_id != \"openai-community/gpt2\": cands.append(\"openai-community/gpt2\")\n",
    "        if model_id != \"gpt2\": cands.append(\"gpt2\")\n",
    "\n",
    "    last_err = None\n",
    "    for mid in cands:\n",
    "        try:\n",
    "            if \"gpt2\" in mid.lower():\n",
    "                tok = GPT2TokenizerFast.from_pretrained(mid, add_prefix_space=True)\n",
    "            else:\n",
    "                tok = AutoTokenizer.from_pretrained(mid, use_fast=True, add_prefix_space=True)\n",
    "\n",
    "            # Right padding + PAD token (GPT‑2 has no pad by default)\n",
    "            if getattr(tok, \"padding_side\", None) != \"right\":\n",
    "                tok.padding_side = \"right\"\n",
    "            if tok.pad_token is None and getattr(tok, \"eos_token\", None) is not None:\n",
    "                tok.pad_token = tok.eos_token\n",
    "\n",
    "            mdl = AutoModel.from_pretrained(mid, output_hidden_states=True)\n",
    "            if getattr(mdl.config, \"pad_token_id\", None) is None and tok.pad_token_id is not None:\n",
    "                mdl.config.pad_token_id = tok.pad_token_id\n",
    "            mdl = mdl.eval().to(device)\n",
    "            if device == \"cuda\":\n",
    "                mdl.half()\n",
    "            return tok, mdl, mid\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    raise RuntimeError(f\"Failed to load tokenizer/model for {cands}: {last_err}\")\n",
    "\n",
    "# =============================== DATA (arity already in CSV) ===============================\n",
    "def load_arity_df(csv_path: str):\n",
    "    \"\"\"\n",
    "    Reads sentence-level rows with list columns: tokens, arity (list[int]).\n",
    "    Expands to one row per token with arity_class in {0,1,2,3,4} (4 = 4+).\n",
    "    Returns:\n",
    "      df_sent: sentence_id + tokens\n",
    "      df_tok : sentence_id, word_id, arity_class, word\n",
    "    \"\"\"\n",
    "    df_all = pd.read_csv(csv_path)\n",
    "    if \"arity\" not in df_all.columns:\n",
    "        # try a few common misspellings/variants\n",
    "        for cand in [\"ariety\", \"ARITY\", \"Arity\"]:\n",
    "            if cand in df_all.columns:\n",
    "                df_all = df_all.rename(columns={cand: \"arity\"})\n",
    "                break\n",
    "    if \"arity\" not in df_all.columns:\n",
    "        raise ValueError(\"CSV must contain a list[int] column named 'arity' (or ariety/ARITY/Arity).\")\n",
    "\n",
    "    df = df_all[[\"sentence_id\",\"tokens\",\"arity\"]].copy()\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df.tokens = df.tokens.apply(_to_list)\n",
    "    df.arity  = df.arity.apply(_to_list)\n",
    "\n",
    "    rows = []\n",
    "    for sid, toks, A in df[[\"sentence_id\",\"tokens\",\"arity\"]].itertuples(index=False):\n",
    "        L = min(len(toks), len(A))\n",
    "        for wid in range(L):\n",
    "            try:\n",
    "                ai = int(A[wid])\n",
    "            except Exception:\n",
    "                ai = 0\n",
    "            cl = str(min(max(ai, 0), 4))   # cap at 4 (means 4+)\n",
    "            rows.append((sid, wid, cl, toks[wid]))\n",
    "    df_tok = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\",\"arity_class\",\"word\"])\n",
    "    df_sent = df[[\"sentence_id\",\"tokens\"]].drop_duplicates(\"sentence_id\")\n",
    "    if df_tok.empty:\n",
    "        raise ValueError(\"No token rows constructed—check your 'arity' column contents.\")\n",
    "    return df_sent, df_tok\n",
    "\n",
    "def sample_per_class(df_tok: pd.DataFrame, per_class_cap: int | None) -> pd.DataFrame:\n",
    "    \"\"\"Optional per-class subsample for plotting.\"\"\"\n",
    "    if per_class_cap is None:\n",
    "        return df_tok.reset_index(drop=True)\n",
    "    picks = []\n",
    "    for c, sub in df_tok.groupby(\"arity_class\", sort=False):\n",
    "        n = min(len(sub), per_class_cap)\n",
    "        picks.append(sub.sample(n, random_state=RAND_SEED, replace=False))\n",
    "    return pd.concat(picks, ignore_index=True)\n",
    "\n",
    "# =============================== EMBEDDING ===============================\n",
    "def embed_subset(df_sent: pd.DataFrame,\n",
    "                 subset_df: pd.DataFrame,\n",
    "                 baseline: str = BASELINE,\n",
    "                 word_rep_mode: str = WORD_REP_MODE,\n",
    "                 batch_size: int = BATCH_SIZE) -> Tuple[np.ndarray, np.ndarray, str, str]:\n",
    "    \"\"\"\n",
    "    Return (reps (L,N,D), filled mask (N,), rep_mode_used, model_tag).\n",
    "    \"\"\"\n",
    "    df_sent[\"sentence_id\"]   = df_sent[\"sentence_id\"].astype(str)\n",
    "    subset_df[\"sentence_id\"] = subset_df[\"sentence_id\"].astype(str)\n",
    "\n",
    "    # sid -> list[(global_idx, word_id)]\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(subset_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(str(sid), []).append((gidx, int(wid)))\n",
    "\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_sent[df_sent.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    tokzr, model, model_tag = _load_tok_and_model(baseline)\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "        enc_kwargs[\"add_prefix_space\"] = True  # needed for byte-level BPE when pre-tokenized\n",
    "\n",
    "    # Choose/validate rep mode depending on model family\n",
    "    rep_mode = word_rep_mode\n",
    "    if _is_gpt_like(model) and rep_mode not in {\"last\",\"mean\"}:\n",
    "        rep_mode = \"last\"\n",
    "\n",
    "    L = _num_hidden_layers(model) + 1   # include embeddings\n",
    "    D = _hidden_size(model)\n",
    "    N = len(subset_df)\n",
    "\n",
    "    reps   = np.zeros((L, N, D), np.float16)\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{model_tag} (embed subset)\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L,B,T,D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                mp = {}\n",
    "                wids = enc_be.word_ids(b)  # fast tokenizer is required for word_ids()\n",
    "                if wids is None:\n",
    "                    raise RuntimeError(\"Fast tokenizer required: word_ids() unavailable.\")\n",
    "                for tidx, wid in enumerate(wids):\n",
    "                    if wid is not None:\n",
    "                        mp.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = mp.get(wid)\n",
    "                    if not toks: continue\n",
    "                    if rep_mode == \"first\":\n",
    "                        vec = h[:, b, toks[0], :]\n",
    "                    elif rep_mode == \"last\":\n",
    "                        vec = h[:, b, toks[-1], :]\n",
    "                    else:  # \"mean\"\n",
    "                        vec = h[:, b, toks, :].mean(axis=1)\n",
    "                    reps[:, gidx, :] = vec.astype(np.float16, copy=False)\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    missing = int((~filled).sum())\n",
    "    if missing:\n",
    "        print(f\"⚠ Missing vectors for {missing} tokens (skipped in PCA).\")\n",
    "        reps = reps[:, filled]\n",
    "    del model; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    return reps, filled, rep_mode, model_tag\n",
    "\n",
    "# =============================== PCA 3D PER LAYER ===============================\n",
    "def _pca3d_layer(X: np.ndarray, n_components: int = 3) -> np.ndarray:\n",
    "    \"\"\"Lightweight PCA to 3D via SVD (no sklearn dependency).\"\"\"\n",
    "    X = X.astype(np.float32, copy=False)\n",
    "    Xc = X - X.mean(0, keepdims=True)\n",
    "    U, S, _ = np.linalg.svd(Xc, full_matrices=False)\n",
    "    return (U[:, :n_components] * S[:n_components]).astype(np.float32, copy=False)  # (n,3)\n",
    "\n",
    "def _qual_palette_for_classes(classes: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Build a discrete palette: stack multiple qualitative sets; if still short,\n",
    "    sample a continuous scale (Turbo) with evenly spaced samples.\n",
    "    \"\"\"\n",
    "    seqs = [\n",
    "        px.colors.qualitative.Bold,      # 10\n",
    "        px.colors.qualitative.D3,        # 10\n",
    "        px.colors.qualitative.Vivid,     # 11\n",
    "        px.colors.qualitative.Safe,      # 11\n",
    "        px.colors.qualitative.Alphabet,  # 26\n",
    "        px.colors.qualitative.Dark24,    # 24\n",
    "        px.colors.qualitative.Light24,   # 24\n",
    "        px.colors.qualitative.Set3,      # 12\n",
    "        px.colors.qualitative.Set2,      # 8\n",
    "        px.colors.qualitative.Set1,      # 9\n",
    "        px.colors.qualitative.Pastel2,   # 8\n",
    "        px.colors.qualitative.Pastel1,   # 9,\n",
    "    ]\n",
    "    pool = []\n",
    "    for s in seqs:\n",
    "        pool.extend(s)\n",
    "    k = len(classes)\n",
    "    if len(pool) < k:\n",
    "        t = np.linspace(0.0, 1.0, k, endpoint=True)\n",
    "        colors = [pc.sample_colorscale(\"Turbo\", [ti])[0] for ti in t]\n",
    "    else:\n",
    "        colors = pool[:k]\n",
    "    return {cls: colors[i] for i, cls in enumerate(classes)}\n",
    "\n",
    "def pca3d_by_arity_and_plot(reps: np.ndarray,\n",
    "                            words: List[str],\n",
    "                            classes_arr: np.ndarray,\n",
    "                            all_classes: List[str],\n",
    "                            model_tag: str,\n",
    "                            html_out: Path):\n",
    "    \"\"\"\n",
    "    Build one 3D scatter trace per class per layer; a slider toggles layers.\n",
    "    \"\"\"\n",
    "    L, N, D = reps.shape\n",
    "    print(f\"PCA plotting on {N:,} tokens across {L} layers…\")\n",
    "\n",
    "    # PCA per layer\n",
    "    Y_layers: List[np.ndarray] = []\n",
    "    for l in range(L):\n",
    "        Y_layers.append(_pca3d_layer(reps[l]))  # (N,3)\n",
    "\n",
    "    cmap = _qual_palette_for_classes(all_classes)\n",
    "\n",
    "    traces = []\n",
    "    n_per_layer = len(all_classes)\n",
    "\n",
    "    for l in range(L):\n",
    "        Y = Y_layers[l]\n",
    "        show_legend = (l == 0)  # keep legend only for layer 0\n",
    "        for j, c in enumerate(all_classes):\n",
    "            mask = (classes_arr == c)\n",
    "            if not np.any(mask):\n",
    "                x = y = z = []; hov = []\n",
    "            else:\n",
    "                x, y, z = Y[mask, 0], Y[mask, 1], Y[mask, 2]\n",
    "                hov = [f\"{w} | arity={c}\" for w in np.asarray(words)[mask]]\n",
    "\n",
    "            traces.append(\n",
    "                go.Scatter3d(\n",
    "                    x=x, y=y, z=z,\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(size=2, opacity=0.75, color=cmap[c]),\n",
    "                    name=c,\n",
    "                    legendgroup=c,\n",
    "                    showlegend=show_legend,\n",
    "                    hovertext=hov,\n",
    "                    # NOTE: do NOT format this string (contains %{...} placeholders)\n",
    "                    hovertemplate=(\n",
    "                        \"<b>%{hovertext}</b><br>\"\n",
    "                        \"x=%{x:.3f}<br>y=%{y:.3f}<br>z=%{z:.3f}\"\n",
    "                        \"<extra></extra>\"\n",
    "                    ),\n",
    "                    visible=(l == 0),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Slider: toggle visibility per selected layer\n",
    "    n_total = n_per_layer * L\n",
    "    steps = []\n",
    "    for l in range(L):\n",
    "        vis = [False] * n_total\n",
    "        start = l * n_per_layer\n",
    "        vis[start : start + n_per_layer] = [True] * n_per_layer\n",
    "        steps.append(dict(\n",
    "            method=\"update\",\n",
    "            args=[{\"visible\": vis},\n",
    "                  {\"title\": f\"{model_tag} • PCA 3D by arity • Layer {l} (drag to rotate)\"}],\n",
    "            label=str(l),\n",
    "        ))\n",
    "\n",
    "    sliders = [dict(\n",
    "        active=0,\n",
    "        steps=steps,\n",
    "        currentvalue={\"prefix\": \"Layer: \"},\n",
    "        pad={\"t\": 10}\n",
    "    )]\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title=f\"{model_tag} • PCA 3D by arity • Layer 0 (drag to rotate)\",\n",
    "        scene=dict(xaxis_title=\"PC1\", yaxis_title=\"PC2\", zaxis_title=\"PC3\", aspectmode=\"data\"),\n",
    "        margin=dict(l=0, r=0, b=0, t=40),\n",
    "        sliders=sliders,\n",
    "        showlegend=True,\n",
    "        legend=dict(title=\"arity (4 = 4+)\", itemsizing=\"trace\")\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "    fig.show()\n",
    "    fig.write_html(str(html_out), include_plotlyjs=\"cdn\")\n",
    "    print(\"✓ Saved interactive HTML to:\", html_out.resolve())\n",
    "\n",
    "# =============================== DRIVER ===============================\n",
    "def run_pca3d_arity():\n",
    "    # 1) Load arity classes\n",
    "    df_sent, ar_df = load_arity_df(CSV_PATH)\n",
    "    classes = sorted(ar_df.arity_class.unique(), key=lambda s: int(s))\n",
    "    print(f\"✓ plotting subset — {len(ar_df):,} tokens across arity classes {classes}\")\n",
    "\n",
    "    # 2) Optional per-class cap\n",
    "    raw_df = sample_per_class(ar_df, PLOT_MAX_PER_CLASS)\n",
    "\n",
    "    # 3) Embed once\n",
    "    reps, filled, rep_mode, model_tag = embed_subset(df_sent, raw_df, BASELINE, WORD_REP_MODE, BATCH_SIZE)\n",
    "    raw_df = raw_df.reset_index(drop=True).loc[filled].reset_index(drop=True)\n",
    "\n",
    "    # 4) Labels & hover text\n",
    "    cls_arr = raw_df.arity_class.values.astype(str)\n",
    "    words   = raw_df.word.astype(str).tolist()\n",
    "\n",
    "    # 5) PCA→3D per layer + Plotly\n",
    "    pca3d_by_arity_and_plot(\n",
    "        reps.astype(np.float32, copy=False), words, cls_arr, classes,\n",
    "        model_tag=f\"{model_tag} (rep={rep_mode})\", html_out=HTML_OUT\n",
    "    )\n",
    "\n",
    "    # Cleanup\n",
    "    del reps; gc.collect()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pca3d_arity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a4a5d4-4f00-46af-b3f1-55218eedf756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ corpus ready — 184,849 tokens across arity classes ['0', '1', '2', '3', '4']\n",
      "Sample sizes per arity (raw cap):\n",
      "{'0': 30000, '1': 19369, '2': 17546, '3': 13383, '4': 13911}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 11:48:07.472787: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/tmp/ipykernel_301437/4197700323.py:359: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
      "openai-community/gpt2 (embed subset): 100%|█| 10067/10067 [01:30<00:00, 110.65it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ embedded 94,209 tokens  • layers=13  • rep_mode=last\n",
      "\n",
      "→ Computing metric: pca99 …\n",
      "  ✓ saved: CSV= tables_ARITY_no_index/arity_bootstrap/arity_raw_pca99_gpt2.csv  plot= results_ARITY_no_index/arity_raw_pca99_gpt2_rep-last.png\n",
      "\n",
      "✓ done (incremental outputs produced per metric).\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# =========================== Optional deps ===========================\n",
    "HAS_DADAPY = False\n",
    "try:\n",
    "    from dadapy import Data  # DADApy ID estimators (TwoNN, GRIDE)\n",
    "    HAS_DADAPY = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# IsoScore: library if available, else a monotone fallback\n",
    "try:\n",
    "    from isoscore import IsoScore\n",
    "    _HAS_ISOSCORE = True\n",
    "except Exception:\n",
    "    _HAS_ISOSCORE = False\n",
    "    class _IsoScoreFallback:\n",
    "        @staticmethod\n",
    "        def IsoScore(X: np.ndarray) -> float:\n",
    "            C = np.cov(X.T, ddof=0)\n",
    "            ev = np.linalg.eigvalsh(C)\n",
    "            if ev.mean() <= 0 or ev[-1] <= 0:\n",
    "                return 0.0\n",
    "            # mean / max eigenvalue in [0,1] (↑ ~ more isotropic)\n",
    "            return float(np.clip(ev.mean() / ev[-1], 0.0, 1.0))\n",
    "    IsoScore = _IsoScoreFallback()\n",
    "\n",
    "# =============================== CONFIG ===============================\n",
    "# Update this path if your file has a different name (e.g., \"en_ewt-ud-train_sentences (2).csv\")\n",
    "CSV_PATH   = \"en_ewt-ud-train_sentences.csv\"\n",
    "\n",
    "# Set to \"gpt2\" (decoder) or \"bert-base-uncased\" (encoder)\n",
    "BASELINE   = \"gpt2\"\n",
    "WORD_REP_MODE = \"last\"  # GPT-2: {\"last\",\"mean\"}\n",
    "\n",
    "# Exclude FIRST word (index 0) of each sentence from analysis\n",
    "EXCLUDE_FIRST_WORD = True\n",
    "\n",
    "# No per-class cap for the fast metrics\n",
    "RAW_MAX_PER_CLASS = int(1e12)\n",
    "\n",
    "# Cap overrides: cap only arity \"0\" to 30_000; others use RAW_MAX_PER_CLASS\n",
    "PER_CLASS_CAPS: Dict[str, int] = {\"0\": 30_000}\n",
    "\n",
    "# Bootstrap replicates (tune down if slow)\n",
    "N_BOOTSTRAP_FAST   = 50\n",
    "N_BOOTSTRAP_HEAVY  = 20\n",
    "\n",
    "# Per-replicate sample size (M = min(cap, N_class))\n",
    "FAST_BS_MAX_SAMP_PER_CLASS  = int(1e12)\n",
    "HEAVY_BS_MAX_SAMP_PER_CLASS = 5000\n",
    "\n",
    "RAND_SEED=42\n",
    "PLOT_DIR     = Path(\"results_ARITY_no_index\"); PLOT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CSV_DIR      = Path(\"tables_ARITY_no_index\") / \"arity_bootstrap\"; CSV_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Throughput (raise if you have more GPU memory)\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Reproducibility & device\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Seaborn style\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "EPS = 1e-12\n",
    "\n",
    "# =============================== HELPERS ===============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _center(X: np.ndarray) -> np.ndarray:\n",
    "    return X - X.mean(0, keepdims=True)\n",
    "\n",
    "def _eigvals_from_X(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Eigenvalues of covariance up to a constant via SVD of centered X (descending).\"\"\"\n",
    "    Xc = _center(X.astype(np.float32, copy=False))\n",
    "    try:\n",
    "        _, S, _ = np.linalg.svd(Xc, full_matrices=False)\n",
    "        lam = (S**2).astype(np.float64)\n",
    "        lam.sort()\n",
    "        return lam[::-1]\n",
    "    except Exception:\n",
    "        return np.array([], dtype=np.float64)\n",
    "\n",
    "def _jitter_unique(X: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"Add tiny noise if there are duplicate rows (helps NN-based estimators).\"\"\"\n",
    "    try:\n",
    "        if np.unique(X, axis=0).shape[0] < X.shape[0]:\n",
    "            X = X + np.random.normal(scale=eps, size=X.shape).astype(X.dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return X\n",
    "\n",
    "def _num_hidden_layers(model) -> int:\n",
    "    n = getattr(model.config, \"num_hidden_layers\", None)\n",
    "    if n is None: n = getattr(model.config, \"n_layer\", None)\n",
    "    if n is None: raise ValueError(\"Cannot determine num_hidden_layers from model.config\")\n",
    "    return int(n)\n",
    "\n",
    "def _hidden_size(model) -> int:\n",
    "    d = getattr(model.config, \"hidden_size\", None)\n",
    "    if d is None: d = getattr(model.config, \"n_embd\", None)\n",
    "    if d is None: raise ValueError(\"Cannot determine hidden_size from model.config\")\n",
    "    return int(d)\n",
    "\n",
    "def _is_gpt_like(model) -> bool:\n",
    "    mt = str(getattr(model.config, \"model_type\", \"\")).lower()\n",
    "    name = str(getattr(getattr(model, \"name_or_path\", \"\"), \"lower\", lambda: \"\")())\n",
    "    return (\"gpt2\" in mt) or (\"gpt2\" in name)\n",
    "\n",
    "def _pick_arity_col(df: pd.DataFrame) -> str:\n",
    "    for cand in [\"arity\", \"ariety\", \"ARITY\", \"Arity\"]:\n",
    "        if cand in df.columns: return cand\n",
    "    raise ValueError(\"No arity column found. Expected one of: arity, ariety, ARITY, Arity\")\n",
    "\n",
    "# -------- Safe tokenizer+model loader (fixes GPT-2 NoneType path error) --------\n",
    "def _safe_load_tok_and_model(baseline: str):\n",
    "    \"\"\"\n",
    "    Robust tokenizer+model loader for GPT-2 and others.\n",
    "    - Patches os.path.isfile during tokenizer load to avoid NoneType path errors.\n",
    "    - Falls back between 'gpt2' and 'openai-community/gpt2'.\n",
    "    Returns: (tokzr, model, model_id_used)\n",
    "    \"\"\"\n",
    "    import os as _os\n",
    "\n",
    "    bl = baseline.lower()\n",
    "    if \"gpt2\" in bl:\n",
    "        candidates = []\n",
    "        for mid in (baseline, \"openai-community/gpt2\", \"gpt2\"):\n",
    "            if mid not in candidates:\n",
    "                candidates.append(mid)\n",
    "    else:\n",
    "        candidates = [baseline]\n",
    "\n",
    "    last_err = None\n",
    "    for mid in candidates:\n",
    "        try:\n",
    "            # Patch during tokenizer load only\n",
    "            _orig_isfile = _os.path.isfile\n",
    "            _os.path.isfile = (lambda p: False if p is None else _orig_isfile(p))\n",
    "            try:\n",
    "                tokzr = AutoTokenizer.from_pretrained(mid, use_fast=True, add_prefix_space=True)\n",
    "            finally:\n",
    "                _os.path.isfile = _orig_isfile\n",
    "\n",
    "            # Right padding + pad token (for GPT-2)\n",
    "            if getattr(tokzr, \"padding_side\", None) != \"right\":\n",
    "                tokzr.padding_side = \"right\"\n",
    "            if tokzr.pad_token is None and getattr(tokzr, \"eos_token\", None) is not None:\n",
    "                tokzr.pad_token = tokzr.eos_token\n",
    "\n",
    "            # Pass add_prefix_space=True at call time if supported\n",
    "            tok_call_kwargs = {}\n",
    "            try:\n",
    "                if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "                    tok_call_kwargs[\"add_prefix_space\"] = True\n",
    "            except Exception:\n",
    "                pass\n",
    "            tokzr._safe_call_kwargs = tok_call_kwargs\n",
    "\n",
    "            model = AutoModel.from_pretrained(mid, output_hidden_states=True)\n",
    "            if getattr(model.config, \"pad_token_id\", None) is None and tokzr.pad_token_id is not None:\n",
    "                model.config.pad_token_id = tokzr.pad_token_id\n",
    "\n",
    "            return tokzr, model, mid\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "\n",
    "    raise RuntimeError(f\"Failed to load tokenizer/model for {baseline} \"\n",
    "                       f\"(tried {candidates}). Last error: {last_err}\")\n",
    "\n",
    "# ========= Per-subsample single-value compute functions (used inside bootstrap) =========\n",
    "def _iso_once(X: np.ndarray) -> float:\n",
    "    return float(IsoScore.IsoScore(X))\n",
    "\n",
    "def _pcaXX_once(X: np.ndarray, var_ratio: float) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    c = np.cumsum(lam); thr = c[-1] * var_ratio\n",
    "    return float(np.searchsorted(c, thr) + 1)\n",
    "\n",
    "def _pca99_once(X: np.ndarray) -> float:\n",
    "    return _pcaXX_once(X, 0.99)\n",
    "\n",
    "def _erank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    p = lam / (lam.sum() + EPS)\n",
    "    H = -(p * np.log(p + EPS)).sum()\n",
    "    return float(np.exp(H))\n",
    "\n",
    "def _pr_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    s1 = lam.sum(); s2 = (lam**2).sum()\n",
    "    return float((s1**2) / (s2 + EPS))\n",
    "\n",
    "def _stable_rank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    return float(lam.sum() / (lam.max() + EPS))\n",
    "\n",
    "def _dadapy_twonn_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    id_est, _, _ = d.compute_id_2NN()\n",
    "    return float(id_est)\n",
    "\n",
    "def _dadapy_gride_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    d.compute_distances(maxk=64)\n",
    "    ids, _, _ = d.return_id_scaling_gride(range_max=64)\n",
    "    return float(ids[-1])\n",
    "\n",
    "# =============================== DATA (arity already in CSV) ===============================\n",
    "def load_arity_df(csv_path: str):\n",
    "    \"\"\"\n",
    "    Reads sentence-level rows with list columns: tokens, <arity>.\n",
    "    Expands to one row per token with an arity class in {0,1,2,3,4} (4 = 4+).\n",
    "    EXCLUDES the first token (word_id == 0) of each sentence if EXCLUDE_FIRST_WORD=True.\n",
    "    Returns:\n",
    "      df_sent (sent-level with tokens),\n",
    "      df_tok  (token-level: sentence_id, word_id, arity_class, word)\n",
    "    \"\"\"\n",
    "    if csv_path is None:\n",
    "        raise TypeError(\"CSV_PATH is None — set CSV_PATH to your dataset filename.\")\n",
    "    df = pd.read_csv(csv_path, usecols=[\"sentence_id\",\"tokens\"], dtype={\"sentence_id\": str})\n",
    "    df_all = pd.read_csv(csv_path)\n",
    "    ar_col = _pick_arity_col(df_all)\n",
    "    df[ar_col] = df_all[ar_col]\n",
    "\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df.tokens = df.tokens.apply(_to_list)\n",
    "    df[ar_col] = df[ar_col].apply(_to_list)\n",
    "\n",
    "    rows = []\n",
    "    for sid, toks, A in df[[\"sentence_id\",\"tokens\", ar_col]].itertuples(index=False):\n",
    "        L = min(len(toks), len(A))\n",
    "        for wid, (tok, a) in enumerate(zip(toks[:L], A[:L])):\n",
    "            # ---- EXCLUDE FIRST WORD ----\n",
    "            if EXCLUDE_FIRST_WORD and wid == 0:\n",
    "                continue\n",
    "            try:\n",
    "                ai = int(a)\n",
    "            except Exception:\n",
    "                ai = 0\n",
    "            cl = str(min(max(ai, 0), 4))   # cap at 4 (means 4+)\n",
    "            rows.append((sid, wid, cl, tok))\n",
    "\n",
    "    df_tok = pd.DataFrame(rows, columns=[\"sentence_id\",\"word_id\",\"arity_class\",\"word\"])\n",
    "\n",
    "    # Safety: enforce again\n",
    "    if EXCLUDE_FIRST_WORD and not df_tok.empty:\n",
    "        df_tok = df_tok[df_tok.word_id != 0].reset_index(drop=True)\n",
    "\n",
    "    df_sent = df[[\"sentence_id\",\"tokens\"]].drop_duplicates(\"sentence_id\")\n",
    "    return df_sent, df_tok\n",
    "\n",
    "def sample_raw(df_tok: pd.DataFrame,\n",
    "               per_class_cap: int = RAW_MAX_PER_CLASS,\n",
    "               per_class_caps: Dict[str, int] | None = None) -> pd.DataFrame:\n",
    "    \"\"\"Per-class cap without frequency matching (with optional overrides per class).\"\"\"\n",
    "    picks = []\n",
    "    caps = per_class_caps or {}\n",
    "    for c, sub in df_tok.groupby(\"arity_class\", sort=False):\n",
    "        cap = caps.get(str(c), per_class_cap)\n",
    "        n = min(len(sub), cap)\n",
    "        picks.append(sub.sample(n, random_state=RAND_SEED, replace=False))\n",
    "    return pd.concat(picks, ignore_index=True)\n",
    "\n",
    "def make_class_palette(classes: List[str]) -> Dict[str, Tuple[float, float, float]]:\n",
    "    base_colors: List[Tuple[float, float, float]] = []\n",
    "    for name in (\"tab20\", \"tab20b\", \"tab20c\"):\n",
    "        try: base_colors.extend(sns.color_palette(name, 20))\n",
    "        except Exception: pass\n",
    "    if len(base_colors) < len(classes):\n",
    "        base_colors = list(sns.color_palette(\"husl\", len(classes)))\n",
    "    ordered = list(sorted(classes, key=lambda s: int(s) if s.isdigit() else 99))\n",
    "    return {cls: base_colors[i % len(base_colors)] for i, cls in enumerate(ordered)}\n",
    "\n",
    "# =============================== TOKENIZER/MODEL (robust) ===============================\n",
    "def _load_tokenizer_and_model(baseline: str):\n",
    "    \"\"\"Backward-compatible alias using the safe loader above.\"\"\"\n",
    "    return _safe_load_tok_and_model(baseline)\n",
    "\n",
    "# =============================== EMBEDDING (BERT & GPT‑2) ===============================\n",
    "def embed_subset(df_sent: pd.DataFrame,\n",
    "                 subset_df: pd.DataFrame,\n",
    "                 baseline: str = BASELINE,\n",
    "                 word_rep_mode: str = WORD_REP_MODE,\n",
    "                 batch_size: int = BATCH_SIZE) -> Tuple[np.ndarray, np.ndarray, str]:\n",
    "    \"\"\"\n",
    "    Return (reps, filled, rep_mode_used).\n",
    "    reps shape: (L, N, D) where L includes the embedding layer (layer 0).\n",
    "    \"\"\"\n",
    "    df_sent[\"sentence_id\"]  = df_sent[\"sentence_id\"].astype(str)\n",
    "    subset_df[\"sentence_id\"] = subset_df[\"sentence_id\"].astype(str)\n",
    "\n",
    "    # sid -> list[(global_idx, word_id)]\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(subset_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(str(sid), []).append((gidx, int(wid)))\n",
    "\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_sent[df_sent.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    # Robust tokenizer/model load (fixes GPT-2 \"NoneType path\" crashes)\n",
    "    tokzr, model, model_id_used = _load_tokenizer_and_model(baseline)\n",
    "\n",
    "    # Pre-tokenized input with word mapping\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
    "    # If tokenizer supports add_prefix_space, supply it\n",
    "    enc_kwargs.update(getattr(tokzr, \"_safe_call_kwargs\", {}))\n",
    "\n",
    "    # GPT‑2 specifics: ensure pad token is set\n",
    "    if tokzr.pad_token is None and getattr(tokzr, \"eos_token\", None) is not None:\n",
    "        tokzr.pad_token = tokzr.eos_token\n",
    "    if getattr(model.config, \"pad_token_id\", None) is None and tokzr.pad_token_id is not None:\n",
    "        model.config.pad_token_id = tokzr.pad_token_id\n",
    "\n",
    "    if device == \"cuda\": model.half()\n",
    "    model = model.eval().to(device)\n",
    "\n",
    "    L = _num_hidden_layers(model) + 1   # include embeddings\n",
    "    D = _hidden_size(model)\n",
    "    N = len(subset_df)\n",
    "\n",
    "    reps   = np.zeros((L, N, D), np.float32)  # float32 for stable SVD/stats\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    # Choose/validate rep mode depending on model family\n",
    "    gpt_like = _is_gpt_like(model)\n",
    "    if gpt_like:\n",
    "        rep_mode = word_rep_mode if word_rep_mode in {\"last\",\"mean\"} else \"last\"\n",
    "    else:\n",
    "        rep_mode = word_rep_mode if word_rep_mode in {\"first\",\"mean\",\"last\"} else \"first\"\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{model_id_used} (embed subset)\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L,B,T,D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                # Map word_id -> token positions for this item\n",
    "                word_map = {}\n",
    "                wids = enc_be.word_ids(b)  # fast tokenizer needed\n",
    "                if wids is None:\n",
    "                    raise RuntimeError(\"Fast tokenizer required (word_ids unavailable).\")\n",
    "                for tidx, wid in enumerate(wids):\n",
    "                    if wid is not None:\n",
    "                        word_map.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = word_map.get(wid)\n",
    "                    if not toks: continue\n",
    "                    if rep_mode == \"first\":\n",
    "                        vec = h[:, b, toks[0], :]\n",
    "                    elif rep_mode == \"last\":\n",
    "                        vec = h[:, b, toks[-1], :]\n",
    "                    else:  # \"mean\"\n",
    "                        vec = h[:, b, toks, :].mean(axis=1)\n",
    "                    reps[:, gidx, :] = vec.astype(np.float32, copy=False)\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    missing = int((~filled).sum())\n",
    "    if missing: print(f\"⚠ Missing vectors for {missing} of {N} sampled words\")\n",
    "    del model; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    return reps, filled, rep_mode\n",
    "\n",
    "# =============================== BOOTSTRAP CORE ===============================\n",
    "def _bs_layer_loop(rep_sub: np.ndarray, M: int, n_reps: int, compute_once: Callable[[np.ndarray], float]):\n",
    "    \"\"\"Bootstrap: sample M with replacement and apply compute_once(X_layer) -> scalar for each layer.\"\"\"\n",
    "    L, N, D = rep_sub.shape\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    A = np.full((n_reps, L), np.nan, np.float32)\n",
    "    for r in range(n_reps):\n",
    "        idx = rng.integers(0, N, size=M)\n",
    "        for l in range(L):\n",
    "            X = rep_sub[l, idx].astype(np.float32, copy=False)\n",
    "            try:    A[r, l] = float(compute_once(X))\n",
    "            except Exception: A[r, l] = np.nan\n",
    "    mu = np.nanmean(A, axis=0).astype(np.float32)\n",
    "    lo = np.nanpercentile(A, 2.5, axis=0).astype(np.float32)\n",
    "    hi = np.nanpercentile(A, 97.5, axis=0).astype(np.float32)\n",
    "    return mu, lo, hi\n",
    "\n",
    "# ---- Metric registries ----\n",
    "FAST_ONCE: Dict[str, Callable[[np.ndarray], float]] = {\n",
    "    \"iso\": _iso_once,     # IsoScore\n",
    "    \"pca99\": _pca99_once  # lPCA @ 0.99 explained variance\n",
    "}\n",
    "\n",
    "HEAVY_ONCE: Dict[str, Callable[[np.ndarray], float] | None] = {\n",
    "    \"gride\": _dadapy_gride_once\n",
    "}\n",
    "\n",
    "LABELS = {\n",
    "    \"pca99\":\"lPCA 0.99\",\n",
    "    \"gride\":\"GRIDE\"\n",
    "}\n",
    "\n",
    "# Keep runtime reasonable (add more if you want)\n",
    "ALL_METRICS = [\"pca99\"]\n",
    "\n",
    "# =============================== SAVE / PLOT ===============================\n",
    "def save_metric_csv_all_classes(metric: str,\n",
    "                                class_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                                layers: np.ndarray,\n",
    "                                baseline: str,\n",
    "                                subset_name: str = \"raw\",\n",
    "                                rep_mode_used: str = \"\"):\n",
    "    rows = []\n",
    "    for c, stats in class_to_stats.items():\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        for l, val in enumerate(mu):\n",
    "            rows.append({\n",
    "                \"subset\": subset_name, \"model\": baseline, \"feature\": \"arity\",\n",
    "                \"class\": c, \"metric\": metric, \"layer\": int(layers[l]),\n",
    "                \"mean\": float(val) if np.isfinite(val) else np.nan,\n",
    "                \"ci_low\": float(lo[l]) if isinstance(lo, np.ndarray) and np.isfinite(lo[l]) else np.nan,\n",
    "                \"ci_high\": float(hi[l]) if isinstance(hi, np.ndarray) and np.isfinite(hi[l]) else np.nan,\n",
    "                \"n_tokens\": int(stats.get(\"n\", 0)),\n",
    "                \"word_rep_mode\": rep_mode_used or WORD_REP_MODE,\n",
    "                \"source_csv\": Path(CSV_PATH).name,\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    out = CSV_DIR / f\"arity_{subset_name}_{metric}_{baseline}.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "\n",
    "def plot_metric_with_ci(class_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                        layers: np.ndarray, metric: str, title: str, out_path: Path,\n",
    "                        palette: Dict[str, Tuple[float, float, float]] | None = None):\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    for c in sorted(class_to_stats.keys(), key=lambda s: int(s)):\n",
    "        stats = class_to_stats[c]\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        if mu is None or np.all(np.isnan(mu)): continue\n",
    "        color = palette.get(c) if isinstance(palette, dict) else None\n",
    "        plt.plot(layers, mu, label=c, lw=1.8, color=color)\n",
    "        if isinstance(lo, np.ndarray) and isinstance(hi, np.ndarray) and not np.all(np.isnan(lo)):\n",
    "            plt.fill_between(layers, lo, hi, alpha=0.15, color=color)\n",
    "    plt.xlabel(\"Layer\"); plt.ylabel(LABELS.get(metric, metric.upper())); plt.title(title)\n",
    "    plt.legend(ncol=3, fontsize=\"small\", title=\"Arity (4 = 4+)\", frameon=False)\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=220); plt.close()\n",
    "\n",
    "# =============================== DRIVER ===============================\n",
    "def run_arity_pipeline():\n",
    "    # 1) Load tokens + arity classes (with first-word exclusion)\n",
    "    df_sent, ar_df = load_arity_df(CSV_PATH)\n",
    "    classes = sorted(ar_df.arity_class.unique(), key=lambda s: int(s))\n",
    "    palette = make_class_palette(classes)\n",
    "    print(f\"✓ corpus ready — {len(ar_df):,} tokens across arity classes {classes}\")\n",
    "\n",
    "    # 2) Optional per-class cap (cap arity \"0\" to 30k via PER_CLASS_CAPS)\n",
    "    raw_df = sample_raw(ar_df, RAW_MAX_PER_CLASS, PER_CLASS_CAPS)\n",
    "    print(\"Sample sizes per arity (raw cap):\")\n",
    "    print(raw_df.arity_class.value_counts().sort_index().to_dict())\n",
    "\n",
    "    # 3) Embed once (encoder or decoder; rep mode auto-validated)\n",
    "    reps, filled, rep_mode_used = embed_subset(df_sent, raw_df, BASELINE, WORD_REP_MODE, BATCH_SIZE)\n",
    "    raw_df = raw_df.reset_index(drop=True).loc[filled].reset_index(drop=True)\n",
    "    cls_arr = raw_df.arity_class.values\n",
    "    L = reps.shape[0]; layers = np.arange(L)\n",
    "    print(f\"✓ embedded {len(raw_df):,} tokens  • layers={L}  • rep_mode={rep_mode_used}\")\n",
    "\n",
    "    # 4) Metric loop\n",
    "    for metric in ALL_METRICS:\n",
    "        print(f\"\\n→ Computing metric: {metric} …\")\n",
    "        compute_once = FAST_ONCE.get(metric) or HEAVY_ONCE.get(metric)\n",
    "        if compute_once is None:\n",
    "            print(f\"  (skipping {metric}: estimator unavailable)\")\n",
    "            continue\n",
    "\n",
    "        # choose bootstrap budget\n",
    "        n_bs = N_BOOTSTRAP_FAST if metric in FAST_ONCE else N_BOOTSTRAP_HEAVY\n",
    "        Mcap = FAST_BS_MAX_SAMP_PER_CLASS if metric in FAST_ONCE else HEAVY_BS_MAX_SAMP_PER_CLASS\n",
    "\n",
    "        class_results: Dict[str, Dict[str, np.ndarray]] = {}\n",
    "        for c in classes:\n",
    "            idx = np.where(cls_arr == c)[0]\n",
    "            if idx.size < 3:\n",
    "                continue\n",
    "            sub = reps[:, idx]  # (L, n_c, D)\n",
    "            Nc = sub.shape[1]\n",
    "            M = min(Mcap, Nc)\n",
    "            mu, lo, hi = _bs_layer_loop(sub, M, n_bs, compute_once)\n",
    "            class_results[c] = {\"mean\": mu, \"lo\": lo, \"hi\": hi, \"n\": int(Nc)}\n",
    "\n",
    "        save_metric_csv_all_classes(metric, class_results, layers, BASELINE,\n",
    "                                    subset_name=\"raw\", rep_mode_used=rep_mode_used)\n",
    "        plot_metric_with_ci(class_results, layers, metric,\n",
    "                            title=f\"{LABELS.get(metric, metric.upper())} • {BASELINE} • rep={rep_mode_used}\",\n",
    "                            out_path=PLOT_DIR / f\"arity_raw_{metric}_{BASELINE}_rep-{rep_mode_used}.png\",\n",
    "                            palette=palette)\n",
    "        print(f\"  ✓ saved: CSV= {CSV_DIR}/arity_raw_{metric}_{BASELINE}.csv  \"\n",
    "              f\"plot= {PLOT_DIR}/arity_raw_{metric}_{BASELINE}_rep-{rep_mode_used}.png\")\n",
    "\n",
    "        del class_results; gc.collect()\n",
    "        if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    del reps; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    print(\"\\n✓ done (incremental outputs produced per metric).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_arity_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d02d8c-577f-4a3e-a181-95cc6dc986fe",
   "metadata": {},
   "source": [
    "## Fine Grained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d89fd02f-39bb-4d0d-bf3e-f865dd383218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ corpus ready — 21,495 tokens across arity classes ['0', '1', '2', '3', '4', '5', '6']\n",
      "Sample sizes per arity (raw cap):\n",
      "{'0': 822, '1': 2011, '2': 5006, '3': 5978, '4': 4281, '5': 2201, '6': 1196}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-base-uncased (embed subset): 100%|████| 8607/8607 [01:02<00:00, 137.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ embedded 21,495 tokens  • layers=13  • rep_mode=first\n",
      "\n",
      "→ Computing metric: iso …\n",
      "  ✓ saved: CSV= tables_ARITY_no_index/arity_bootstrap/arity_raw_iso_bert-base-uncased.csv  plot= results_ARITY_no_index/arity_raw_iso_bert-base-uncased_rep-first.png\n",
      "\n",
      "→ Computing metric: gride …\n",
      "  ✓ saved: CSV= tables_ARITY_no_index/arity_bootstrap/arity_raw_gride_bert-base-uncased.csv  plot= results_ARITY_no_index/arity_raw_gride_bert-base-uncased_rep-first.png\n",
      "\n",
      "→ Computing metric: pca99 …\n",
      "  ✓ saved: CSV= tables_ARITY_no_index/arity_bootstrap/arity_raw_pca99_bert-base-uncased.csv  plot= results_ARITY_no_index/arity_raw_pca99_bert-base-uncased_rep-first.png\n",
      "\n",
      "✓ done (incremental outputs produced per metric).\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, gc, ast, random, inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# =========================== Optional deps ===========================\n",
    "HAS_DADAPY = False\n",
    "try:\n",
    "    from dadapy import Data  # DADApy ID estimators (TwoNN, GRIDE)\n",
    "    HAS_DADAPY = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# IsoScore: library if available, else a monotone fallback\n",
    "try:\n",
    "    from isoscore import IsoScore\n",
    "    _HAS_ISOSCORE = True\n",
    "except Exception:\n",
    "    _HAS_ISOSCORE = False\n",
    "    class _IsoScoreFallback:\n",
    "        @staticmethod\n",
    "        def IsoScore(X: np.ndarray) -> float:\n",
    "            C = np.cov(X.T, ddof=0)\n",
    "            ev = np.linalg.eigvalsh(C)\n",
    "            if ev.mean() <= 0 or ev[-1] <= 0:\n",
    "                return 0.0\n",
    "            # mean / max eigenvalue in [0,1] (↑ ~ more isotropic)\n",
    "            return float(np.clip(ev.mean() / ev[-1], 0.0, 1.0))\n",
    "    IsoScore = _IsoScoreFallback()\n",
    "\n",
    "# =============================== CONFIG ===============================\n",
    "# =============================== CONFIG ===============================\n",
    "CSV_PATH   = \"en_ewt-ud-train_sentences.csv\"\n",
    "\n",
    "BASELINE   = \"bert-base-uncased\"\n",
    "WORD_REP_MODE = \"first\"  # GPT-2: {\"last\",\"mean\"}\n",
    "\n",
    "EXCLUDE_FIRST_WORD = True\n",
    "\n",
    "POS_COL      = \"pos\"     # column in your CSV with POS tags\n",
    "KEEP_POS_TAG = \"VERB\"    # only keep tokens whose POS == \"VERB\"\n",
    "\n",
    "\n",
    "# No per-class cap for the fast metrics\n",
    "RAW_MAX_PER_CLASS = int(1e12)\n",
    "\n",
    "# Cap overrides: cap only arity \"0\" to 30_000; others use RAW_MAX_PER_CLASS\n",
    "PER_CLASS_CAPS: Dict[str, int] = {\"0\": 30_000}\n",
    "\n",
    "# Bootstrap replicates (tune down if slow)\n",
    "N_BOOTSTRAP_FAST   = 50\n",
    "N_BOOTSTRAP_HEAVY  = 20\n",
    "\n",
    "# Per-replicate sample size (M = min(cap, N_class))\n",
    "FAST_BS_MAX_SAMP_PER_CLASS  = int(1e12)\n",
    "HEAVY_BS_MAX_SAMP_PER_CLASS = 5000\n",
    "\n",
    "RAND_SEED=42\n",
    "PLOT_DIR     = Path(\"results_ARITY_no_index\"); PLOT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "CSV_DIR      = Path(\"tables_ARITY_no_index\") / \"arity_bootstrap\"; CSV_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Throughput (raise if you have more GPU memory)\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Reproducibility & device\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "random.seed(RAND_SEED); np.random.seed(RAND_SEED); torch.manual_seed(RAND_SEED)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\": torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Seaborn style\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "EPS = 1e-12\n",
    "\n",
    "# =============================== HELPERS ===============================\n",
    "def _to_list(x):\n",
    "    return ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else x\n",
    "\n",
    "def _center(X: np.ndarray) -> np.ndarray:\n",
    "    return X - X.mean(0, keepdims=True)\n",
    "\n",
    "def _eigvals_from_X(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Eigenvalues of covariance up to a constant via SVD of centered X (descending).\"\"\"\n",
    "    Xc = _center(X.astype(np.float32, copy=False))\n",
    "    try:\n",
    "        _, S, _ = np.linalg.svd(Xc, full_matrices=False)\n",
    "        lam = (S**2).astype(np.float64)\n",
    "        lam.sort()\n",
    "        return lam[::-1]\n",
    "    except Exception:\n",
    "        return np.array([], dtype=np.float64)\n",
    "\n",
    "def _jitter_unique(X: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"Add tiny noise if there are duplicate rows (helps NN-based estimators).\"\"\"\n",
    "    try:\n",
    "        if np.unique(X, axis=0).shape[0] < X.shape[0]:\n",
    "            X = X + np.random.normal(scale=eps, size=X.shape).astype(X.dtype)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return X\n",
    "\n",
    "def _num_hidden_layers(model) -> int:\n",
    "    n = getattr(model.config, \"num_hidden_layers\", None)\n",
    "    if n is None: n = getattr(model.config, \"n_layer\", None)\n",
    "    if n is None: raise ValueError(\"Cannot determine num_hidden_layers from model.config\")\n",
    "    return int(n)\n",
    "\n",
    "def _hidden_size(model) -> int:\n",
    "    d = getattr(model.config, \"hidden_size\", None)\n",
    "    if d is None: d = getattr(model.config, \"n_embd\", None)\n",
    "    if d is None: raise ValueError(\"Cannot determine hidden_size from model.config\")\n",
    "    return int(d)\n",
    "\n",
    "def _is_gpt_like(model) -> bool:\n",
    "    mt = str(getattr(model.config, \"model_type\", \"\")).lower()\n",
    "    name = str(getattr(getattr(model, \"name_or_path\", \"\"), \"lower\", lambda: \"\")())\n",
    "    return (\"gpt2\" in mt) or (\"gpt2\" in name)\n",
    "\n",
    "def _pick_arity_col(df: pd.DataFrame) -> str:\n",
    "    for cand in [\"arity\", \"ariety\", \"ARITY\", \"Arity\"]:\n",
    "        if cand in df.columns: return cand\n",
    "    raise ValueError(\"No arity column found. Expected one of: arity, ariety, ARITY, Arity\")\n",
    "\n",
    "# -------- Safe tokenizer+model loader (fixes GPT-2 NoneType path error) --------\n",
    "def _safe_load_tok_and_model(baseline: str):\n",
    "    \"\"\"\n",
    "    Robust tokenizer+model loader for GPT-2 and others.\n",
    "    - Patches os.path.isfile during tokenizer load to avoid NoneType path errors.\n",
    "    - Falls back between 'gpt2' and 'openai-community/gpt2'.\n",
    "    Returns: (tokzr, model, model_id_used)\n",
    "    \"\"\"\n",
    "    import os as _os\n",
    "\n",
    "    bl = baseline.lower()\n",
    "    if \"gpt2\" in bl:\n",
    "        candidates = []\n",
    "        for mid in (baseline, \"openai-community/gpt2\", \"gpt2\"):\n",
    "            if mid not in candidates:\n",
    "                candidates.append(mid)\n",
    "    else:\n",
    "        candidates = [baseline]\n",
    "\n",
    "    last_err = None\n",
    "    for mid in candidates:\n",
    "        try:\n",
    "            # Patch during tokenizer load only\n",
    "            _orig_isfile = _os.path.isfile\n",
    "            _os.path.isfile = (lambda p: False if p is None else _orig_isfile(p))\n",
    "            try:\n",
    "                tokzr = AutoTokenizer.from_pretrained(mid, use_fast=True, add_prefix_space=True)\n",
    "            finally:\n",
    "                _os.path.isfile = _orig_isfile\n",
    "\n",
    "            # Right padding + pad token (for GPT-2)\n",
    "            if getattr(tokzr, \"padding_side\", None) != \"right\":\n",
    "                tokzr.padding_side = \"right\"\n",
    "            if tokzr.pad_token is None and getattr(tokzr, \"eos_token\", None) is not None:\n",
    "                tokzr.pad_token = tokzr.eos_token\n",
    "\n",
    "            # Pass add_prefix_space=True at call time if supported\n",
    "            tok_call_kwargs = {}\n",
    "            try:\n",
    "                if \"add_prefix_space\" in inspect.signature(tokzr.__call__).parameters:\n",
    "                    tok_call_kwargs[\"add_prefix_space\"] = True\n",
    "            except Exception:\n",
    "                pass\n",
    "            tokzr._safe_call_kwargs = tok_call_kwargs\n",
    "\n",
    "            model = AutoModel.from_pretrained(mid, output_hidden_states=True)\n",
    "            if getattr(model.config, \"pad_token_id\", None) is None and tokzr.pad_token_id is not None:\n",
    "                model.config.pad_token_id = tokzr.pad_token_id\n",
    "\n",
    "            return tokzr, model, mid\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "\n",
    "    raise RuntimeError(f\"Failed to load tokenizer/model for {baseline} \"\n",
    "                       f\"(tried {candidates}). Last error: {last_err}\")\n",
    "\n",
    "# ========= Per-subsample single-value compute functions (used inside bootstrap) =========\n",
    "def _iso_once(X: np.ndarray) -> float:\n",
    "    return float(IsoScore.IsoScore(X))\n",
    "\n",
    "def _pcaXX_once(X: np.ndarray, var_ratio: float) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    c = np.cumsum(lam); thr = c[-1] * var_ratio\n",
    "    return float(np.searchsorted(c, thr) + 1)\n",
    "\n",
    "def _pca99_once(X: np.ndarray) -> float:\n",
    "    return _pcaXX_once(X, 0.99)\n",
    "\n",
    "def _erank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    p = lam / (lam.sum() + EPS)\n",
    "    H = -(p * np.log(p + EPS)).sum()\n",
    "    return float(np.exp(H))\n",
    "\n",
    "def _pr_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    s1 = lam.sum(); s2 = (lam**2).sum()\n",
    "    return float((s1**2) / (s2 + EPS))\n",
    "\n",
    "def _stable_rank_once(X: np.ndarray) -> float:\n",
    "    lam = _eigvals_from_X(X)\n",
    "    if lam.size == 0: return np.nan\n",
    "    return float(lam.sum() / (lam.max() + EPS))\n",
    "\n",
    "def _dadapy_twonn_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    id_est, _, _ = d.compute_id_2NN()\n",
    "    return float(id_est)\n",
    "\n",
    "def _dadapy_gride_once(X: np.ndarray) -> float:\n",
    "    if not HAS_DADAPY: return np.nan\n",
    "    d = Data(coordinates=_jitter_unique(X))\n",
    "    d.compute_distances(maxk=64)\n",
    "    ids, _, _ = d.return_id_scaling_gride(range_max=64)\n",
    "    return float(ids[-1])\n",
    "\n",
    "# =============================== DATA (arity already in CSV) ===============================\n",
    "def load_arity_df(csv_path: str):\n",
    "    \"\"\"\n",
    "    Reads sentence-level rows with list columns: tokens, <arity>, pos.\n",
    "    Expands to one row per token with an arity class in {0,1,2,3,4} (4 = 4+).\n",
    "\n",
    "    - EXCLUDES the first token (word_id == 0) of each sentence if EXCLUDE_FIRST_WORD=True.\n",
    "    - KEEPS ONLY tokens whose POS == KEEP_POS_TAG (e.g. \"VERB\").\n",
    "    Returns:\n",
    "      df_sent (sent-level with tokens),\n",
    "      df_tok  (token-level: sentence_id, word_id, arity_class, word[, pos])\n",
    "    \"\"\"\n",
    "    if csv_path is None:\n",
    "        raise TypeError(\"CSV_PATH is None — set CSV_PATH to your dataset filename.\")\n",
    "\n",
    "    # Read full CSV once\n",
    "    df_all = pd.read_csv(csv_path)\n",
    "\n",
    "    # Find the correct arity column name (\"arity\" / \"ariety\" / etc.)\n",
    "    ar_col = _pick_arity_col(df_all)\n",
    "\n",
    "    # We need sentence_id, tokens, arity and POS\n",
    "    needed_cols = [\"sentence_id\", \"tokens\", ar_col, POS_COL]\n",
    "    for c in needed_cols:\n",
    "        if c not in df_all.columns:\n",
    "            raise ValueError(f\"Expected column {c!r} in CSV, but it is missing.\")\n",
    "    df = df_all[needed_cols].copy()\n",
    "\n",
    "    df[\"sentence_id\"] = df[\"sentence_id\"].astype(str)\n",
    "    df[\"tokens\"]      = df[\"tokens\"].apply(_to_list)\n",
    "    df[ar_col]        = df[ar_col].apply(_to_list)\n",
    "    df[POS_COL]       = df[POS_COL].apply(_to_list)\n",
    "\n",
    "    rows = []\n",
    "    for sid, toks, A, poss in df[[\"sentence_id\", \"tokens\", ar_col, POS_COL]].itertuples(index=False):\n",
    "        # be safe if lengths differ\n",
    "        L = min(len(toks), len(A), len(poss))\n",
    "        for wid in range(L):\n",
    "            # ---- EXCLUDE FIRST WORD ----\n",
    "            if EXCLUDE_FIRST_WORD and wid == 0:\n",
    "                continue\n",
    "\n",
    "            pos_tag = poss[wid]\n",
    "            # ---- POS FILTER: keep only VERBs (or KEEP_POS_TAG) ----\n",
    "            if pos_tag != KEEP_POS_TAG:\n",
    "                continue\n",
    "\n",
    "            tok = toks[wid]\n",
    "            a   = A[wid]\n",
    "\n",
    "            try:\n",
    "                ai = int(a)\n",
    "            except Exception:\n",
    "                ai = 0\n",
    "\n",
    "            # Arity class: 0,1,2,3,4 (where 4 = 4+)\n",
    "            cl = str(min(max(ai, 0), 6))\n",
    "            rows.append((sid, wid, cl, tok, pos_tag))\n",
    "\n",
    "    df_tok = pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\"sentence_id\", \"word_id\", \"arity_class\", \"word\", \"pos\"]\n",
    "    )\n",
    "\n",
    "    # Safety: enforce again (should be redundant)\n",
    "    if EXCLUDE_FIRST_WORD and not df_tok.empty:\n",
    "        df_tok = df_tok[df_tok.word_id != 0].reset_index(drop=True)\n",
    "\n",
    "    df_sent = df[[\"sentence_id\", \"tokens\"]].drop_duplicates(\"sentence_id\")\n",
    "    return df_sent, df_tok\n",
    "\n",
    "\n",
    "def sample_raw(df_tok: pd.DataFrame,\n",
    "               per_class_cap: int = RAW_MAX_PER_CLASS,\n",
    "               per_class_caps: Dict[str, int] | None = None) -> pd.DataFrame:\n",
    "    \"\"\"Per-class cap without frequency matching (with optional overrides per class).\"\"\"\n",
    "    picks = []\n",
    "    caps = per_class_caps or {}\n",
    "    for c, sub in df_tok.groupby(\"arity_class\", sort=False):\n",
    "        cap = caps.get(str(c), per_class_cap)\n",
    "        n = min(len(sub), cap)\n",
    "        picks.append(sub.sample(n, random_state=RAND_SEED, replace=False))\n",
    "    return pd.concat(picks, ignore_index=True)\n",
    "\n",
    "def make_class_palette(classes: List[str]) -> Dict[str, Tuple[float, float, float]]:\n",
    "    base_colors: List[Tuple[float, float, float]] = []\n",
    "    for name in (\"tab20\", \"tab20b\", \"tab20c\"):\n",
    "        try: base_colors.extend(sns.color_palette(name, 20))\n",
    "        except Exception: pass\n",
    "    if len(base_colors) < len(classes):\n",
    "        base_colors = list(sns.color_palette(\"husl\", len(classes)))\n",
    "    ordered = list(sorted(classes, key=lambda s: int(s) if s.isdigit() else 99))\n",
    "    return {cls: base_colors[i % len(base_colors)] for i, cls in enumerate(ordered)}\n",
    "\n",
    "# =============================== TOKENIZER/MODEL (robust) ===============================\n",
    "def _load_tokenizer_and_model(baseline: str):\n",
    "    \"\"\"Backward-compatible alias using the safe loader above.\"\"\"\n",
    "    return _safe_load_tok_and_model(baseline)\n",
    "\n",
    "# =============================== EMBEDDING (BERT & GPT‑2) ===============================\n",
    "def embed_subset(df_sent: pd.DataFrame,\n",
    "                 subset_df: pd.DataFrame,\n",
    "                 baseline: str = BASELINE,\n",
    "                 word_rep_mode: str = WORD_REP_MODE,\n",
    "                 batch_size: int = BATCH_SIZE) -> Tuple[np.ndarray, np.ndarray, str]:\n",
    "    \"\"\"\n",
    "    Return (reps, filled, rep_mode_used).\n",
    "    reps shape: (L, N, D) where L includes the embedding layer (layer 0).\n",
    "    \"\"\"\n",
    "    df_sent[\"sentence_id\"]  = df_sent[\"sentence_id\"].astype(str)\n",
    "    subset_df[\"sentence_id\"] = subset_df[\"sentence_id\"].astype(str)\n",
    "\n",
    "    # sid -> list[(global_idx, word_id)]\n",
    "    by_sid: Dict[str, List[Tuple[int,int]]] = {}\n",
    "    for gidx, (sid, wid) in enumerate(subset_df[[\"sentence_id\",\"word_id\"]].itertuples(index=False)):\n",
    "        by_sid.setdefault(str(sid), []).append((gidx, int(wid)))\n",
    "\n",
    "    sids = list(by_sid.keys())\n",
    "    df_sel = (df_sent[df_sent.sentence_id.isin(sids)]\n",
    "              .drop_duplicates(\"sentence_id\")\n",
    "              .set_index(\"sentence_id\")\n",
    "              .loc[sids])\n",
    "\n",
    "    # Robust tokenizer/model load (fixes GPT-2 \"NoneType path\" crashes)\n",
    "    tokzr, model, model_id_used = _load_tokenizer_and_model(baseline)\n",
    "\n",
    "    # Pre-tokenized input with word mapping\n",
    "    enc_kwargs = dict(is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
    "    # If tokenizer supports add_prefix_space, supply it\n",
    "    enc_kwargs.update(getattr(tokzr, \"_safe_call_kwargs\", {}))\n",
    "\n",
    "    # GPT‑2 specifics: ensure pad token is set\n",
    "    if tokzr.pad_token is None and getattr(tokzr, \"eos_token\", None) is not None:\n",
    "        tokzr.pad_token = tokzr.eos_token\n",
    "    if getattr(model.config, \"pad_token_id\", None) is None and tokzr.pad_token_id is not None:\n",
    "        model.config.pad_token_id = tokzr.pad_token_id\n",
    "\n",
    "    if device == \"cuda\": model.half()\n",
    "    model = model.eval().to(device)\n",
    "\n",
    "    L = _num_hidden_layers(model) + 1   # include embeddings\n",
    "    D = _hidden_size(model)\n",
    "    N = len(subset_df)\n",
    "\n",
    "    reps   = np.zeros((L, N, D), np.float32)  # float32 for stable SVD/stats\n",
    "    filled = np.zeros(N, dtype=bool)\n",
    "\n",
    "    # Choose/validate rep mode depending on model family\n",
    "    gpt_like = _is_gpt_like(model)\n",
    "    if gpt_like:\n",
    "        rep_mode = word_rep_mode if word_rep_mode in {\"last\",\"mean\"} else \"last\"\n",
    "    else:\n",
    "        rep_mode = word_rep_mode if word_rep_mode in {\"first\",\"mean\",\"last\"} else \"first\"\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(device == \"cuda\"):\n",
    "        for start in tqdm(range(0, len(sids), batch_size), desc=f\"{model_id_used} (embed subset)\"):\n",
    "            batch_ids    = sids[start : start + batch_size]\n",
    "            batch_tokens = df_sel.loc[batch_ids, \"tokens\"].tolist()\n",
    "\n",
    "            enc_be = tokzr(batch_tokens, **enc_kwargs)\n",
    "            enc_t  = {k: v.to(device) for k, v in enc_be.items()}\n",
    "            out = model(**enc_t)\n",
    "            h = torch.stack(out.hidden_states).detach().cpu().numpy().astype(np.float32)  # (L,B,T,D)\n",
    "\n",
    "            for b, sid in enumerate(batch_ids):\n",
    "                # Map word_id -> token positions for this item\n",
    "                word_map = {}\n",
    "                wids = enc_be.word_ids(b)  # fast tokenizer needed\n",
    "                if wids is None:\n",
    "                    raise RuntimeError(\"Fast tokenizer required (word_ids unavailable).\")\n",
    "                for tidx, wid in enumerate(wids):\n",
    "                    if wid is not None:\n",
    "                        word_map.setdefault(int(wid), []).append(int(tidx))\n",
    "\n",
    "                for gidx, wid in by_sid.get(sid, []):\n",
    "                    toks = word_map.get(wid)\n",
    "                    if not toks: continue\n",
    "                    if rep_mode == \"first\":\n",
    "                        vec = h[:, b, toks[0], :]\n",
    "                    elif rep_mode == \"last\":\n",
    "                        vec = h[:, b, toks[-1], :]\n",
    "                    else:  # \"mean\"\n",
    "                        vec = h[:, b, toks, :].mean(axis=1)\n",
    "                    reps[:, gidx, :] = vec.astype(np.float32, copy=False)\n",
    "                    filled[gidx] = True\n",
    "\n",
    "            del enc_be, enc_t, out, h\n",
    "            if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    missing = int((~filled).sum())\n",
    "    if missing: print(f\"⚠ Missing vectors for {missing} of {N} sampled words\")\n",
    "    del model; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    return reps, filled, rep_mode\n",
    "\n",
    "# =============================== BOOTSTRAP CORE ===============================\n",
    "def _bs_layer_loop(rep_sub: np.ndarray, M: int, n_reps: int, compute_once: Callable[[np.ndarray], float]):\n",
    "    \"\"\"Bootstrap: sample M with replacement and apply compute_once(X_layer) -> scalar for each layer.\"\"\"\n",
    "    L, N, D = rep_sub.shape\n",
    "    rng = np.random.default_rng(RAND_SEED)\n",
    "    A = np.full((n_reps, L), np.nan, np.float32)\n",
    "    for r in range(n_reps):\n",
    "        idx = rng.integers(0, N, size=M)\n",
    "        for l in range(L):\n",
    "            X = rep_sub[l, idx].astype(np.float32, copy=False)\n",
    "            try:    A[r, l] = float(compute_once(X))\n",
    "            except Exception: A[r, l] = np.nan\n",
    "    mu = np.nanmean(A, axis=0).astype(np.float32)\n",
    "    lo = np.nanpercentile(A, 2.5, axis=0).astype(np.float32)\n",
    "    hi = np.nanpercentile(A, 97.5, axis=0).astype(np.float32)\n",
    "    return mu, lo, hi\n",
    "\n",
    "# ---- Metric registries ----\n",
    "FAST_ONCE: Dict[str, Callable[[np.ndarray], float]] = {\n",
    "    \"iso\": _iso_once,     # IsoScore\n",
    "    \"pca99\": _pca99_once  # lPCA @ 0.99 explained variance\n",
    "}\n",
    "\n",
    "HEAVY_ONCE: Dict[str, Callable[[np.ndarray], float] | None] = {\n",
    "    \"gride\": _dadapy_gride_once\n",
    "}\n",
    "\n",
    "LABELS = {\n",
    "    \"pca99\":\"lPCA 0.99\",\n",
    "    \"gride\":\"GRIDE\"\n",
    "}\n",
    "\n",
    "# Keep runtime reasonable (add more if you want)\n",
    "ALL_METRICS = [\"iso\",\"gride\",\"pca99\"]\n",
    "\n",
    "# =============================== SAVE / PLOT ===============================\n",
    "def save_metric_csv_all_classes(metric: str,\n",
    "                                class_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                                layers: np.ndarray,\n",
    "                                baseline: str,\n",
    "                                subset_name: str = \"raw\",\n",
    "                                rep_mode_used: str = \"\"):\n",
    "    rows = []\n",
    "    for c, stats in class_to_stats.items():\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        for l, val in enumerate(mu):\n",
    "            rows.append({\n",
    "                \"subset\": subset_name, \"model\": baseline, \"feature\": \"arity\",\n",
    "                \"class\": c, \"metric\": metric, \"layer\": int(layers[l]),\n",
    "                \"mean\": float(val) if np.isfinite(val) else np.nan,\n",
    "                \"ci_low\": float(lo[l]) if isinstance(lo, np.ndarray) and np.isfinite(lo[l]) else np.nan,\n",
    "                \"ci_high\": float(hi[l]) if isinstance(hi, np.ndarray) and np.isfinite(hi[l]) else np.nan,\n",
    "                \"n_tokens\": int(stats.get(\"n\", 0)),\n",
    "                \"word_rep_mode\": rep_mode_used or WORD_REP_MODE,\n",
    "                \"source_csv\": Path(CSV_PATH).name,\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    out = CSV_DIR / f\"arity_{subset_name}_{metric}_{baseline}.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "\n",
    "def plot_metric_with_ci(class_to_stats: Dict[str, Dict[str, np.ndarray]],\n",
    "                        layers: np.ndarray, metric: str, title: str, out_path: Path,\n",
    "                        palette: Dict[str, Tuple[float, float, float]] | None = None):\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    for c in sorted(class_to_stats.keys(), key=lambda s: int(s)):\n",
    "        stats = class_to_stats[c]\n",
    "        mu, lo, hi = stats[\"mean\"], stats.get(\"lo\"), stats.get(\"hi\")\n",
    "        if mu is None or np.all(np.isnan(mu)): continue\n",
    "        color = palette.get(c) if isinstance(palette, dict) else None\n",
    "        plt.plot(layers, mu, label=c, lw=1.8, color=color)\n",
    "        if isinstance(lo, np.ndarray) and isinstance(hi, np.ndarray) and not np.all(np.isnan(lo)):\n",
    "            plt.fill_between(layers, lo, hi, alpha=0.15, color=color)\n",
    "    plt.xlabel(\"Layer\"); plt.ylabel(LABELS.get(metric, metric.upper())); plt.title(title)\n",
    "    plt.legend(ncol=3, fontsize=\"small\", title=\"Arity (4 = 4+)\", frameon=False)\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=220); plt.close()\n",
    "\n",
    "# =============================== DRIVER ===============================\n",
    "def run_arity_pipeline():\n",
    "    # 1) Load tokens + arity classes (with first-word exclusion)\n",
    "    df_sent, ar_df = load_arity_df(CSV_PATH)\n",
    "    classes = sorted(ar_df.arity_class.unique(), key=lambda s: int(s))\n",
    "    palette = make_class_palette(classes)\n",
    "    print(f\"✓ corpus ready — {len(ar_df):,} tokens across arity classes {classes}\")\n",
    "\n",
    "    # 2) Optional per-class cap (cap arity \"0\" to 30k via PER_CLASS_CAPS)\n",
    "    raw_df = sample_raw(ar_df, RAW_MAX_PER_CLASS, PER_CLASS_CAPS)\n",
    "    print(\"Sample sizes per arity (raw cap):\")\n",
    "    print(raw_df.arity_class.value_counts().sort_index().to_dict())\n",
    "\n",
    "    # 3) Embed once (encoder or decoder; rep mode auto-validated)\n",
    "    reps, filled, rep_mode_used = embed_subset(df_sent, raw_df, BASELINE, WORD_REP_MODE, BATCH_SIZE)\n",
    "    raw_df = raw_df.reset_index(drop=True).loc[filled].reset_index(drop=True)\n",
    "    cls_arr = raw_df.arity_class.values\n",
    "    L = reps.shape[0]; layers = np.arange(L)\n",
    "    print(f\"✓ embedded {len(raw_df):,} tokens  • layers={L}  • rep_mode={rep_mode_used}\")\n",
    "\n",
    "    # 4) Metric loop\n",
    "    for metric in ALL_METRICS:\n",
    "        print(f\"\\n→ Computing metric: {metric} …\")\n",
    "        compute_once = FAST_ONCE.get(metric) or HEAVY_ONCE.get(metric)\n",
    "        if compute_once is None:\n",
    "            print(f\"  (skipping {metric}: estimator unavailable)\")\n",
    "            continue\n",
    "\n",
    "        # choose bootstrap budget\n",
    "        n_bs = N_BOOTSTRAP_FAST if metric in FAST_ONCE else N_BOOTSTRAP_HEAVY\n",
    "        Mcap = FAST_BS_MAX_SAMP_PER_CLASS if metric in FAST_ONCE else HEAVY_BS_MAX_SAMP_PER_CLASS\n",
    "\n",
    "        class_results: Dict[str, Dict[str, np.ndarray]] = {}\n",
    "        for c in classes:\n",
    "            idx = np.where(cls_arr == c)[0]\n",
    "            if idx.size < 3:\n",
    "                continue\n",
    "            sub = reps[:, idx]  # (L, n_c, D)\n",
    "            Nc = sub.shape[1]\n",
    "            M = min(Mcap, Nc)\n",
    "            mu, lo, hi = _bs_layer_loop(sub, M, n_bs, compute_once)\n",
    "            class_results[c] = {\"mean\": mu, \"lo\": lo, \"hi\": hi, \"n\": int(Nc)}\n",
    "\n",
    "        save_metric_csv_all_classes(metric, class_results, layers, BASELINE,\n",
    "                                    subset_name=\"raw\", rep_mode_used=rep_mode_used)\n",
    "        plot_metric_with_ci(class_results, layers, metric,\n",
    "                            title=f\"{LABELS.get(metric, metric.upper())} • {BASELINE} • rep={rep_mode_used}\",\n",
    "                            out_path=PLOT_DIR / f\"arity_raw_{metric}_{BASELINE}_rep-{rep_mode_used}.png\",\n",
    "                            palette=palette)\n",
    "        print(f\"  ✓ saved: CSV= {CSV_DIR}/arity_raw_{metric}_{BASELINE}.csv  \"\n",
    "              f\"plot= {PLOT_DIR}/arity_raw_{metric}_{BASELINE}_rep-{rep_mode_used}.png\")\n",
    "\n",
    "        del class_results; gc.collect()\n",
    "        if device == \"cuda\": torch.cuda.empty_cache()\n",
    "\n",
    "    del reps; gc.collect()\n",
    "    if device == \"cuda\": torch.cuda.empty_cache()\n",
    "    print(\"\\n✓ done (incremental outputs produced per metric).\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_arity_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f92bad-f336-491c-b7d3-d9dab590ef6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
